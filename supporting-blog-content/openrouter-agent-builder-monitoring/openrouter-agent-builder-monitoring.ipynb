{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring AI Connectors and Agent Builder with OpenRouter\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/elastic/elasticsearch-labs/blob/main/supporting-blog-content/openrouter-agent-builder-monitoring/openrouter-agent-builder-monitoring.ipynb)\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "- Create an AI Connector for Agent Builder using OpenRouter\n",
    "- Set up an inference endpoint for data enrichment\n",
    "- Build an ingest pipeline that extracts structured fields from product descriptions\n",
    "- Create an Agent Builder agent that queries enriched data\n",
    "- Monitor LLM usage with OpenRouter Broadcast and Elastic APM\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "We'll build an AI-enriched audio products catalog where:\n",
    "1. An **ingest pipeline** uses OpenRouter to extract structured fields (category, features, use_case) from product descriptions\n",
    "2. An **Agent Builder** agent answers questions about the products using semantic search and ES|QL\n",
    "3. **OpenRouter Broadcast** sends traces to Elastic APM for monitoring costs, latency, and token usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "First, let's install the required Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU elasticsearch requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Credentials\n",
    "\n",
    "You'll need the following credentials:\n",
    "\n",
    "- **ELASTIC_URL**: Your Elasticsearch endpoint URL\n",
    "- **KIBANA_URL**: Your Kibana endpoint URL\n",
    "- **ELASTIC_API_KEY**: An API key with permissions to create indices, pipelines, inference endpoints, and connectors\n",
    "- **OPENROUTER_AGENT_KEY**: Your OpenRouter API key for Agent Builder interactions\n",
    "- **OPENROUTER_INGESTION_KEY**: (Optional) A separate OpenRouter API key for ingestion. If not provided, uses the agent key.\n",
    "\n",
    "To get started with Elastic Cloud, [sign up for a free trial](https://cloud.elastic.co/registration?utm_source=github&utm_content=elasticsearch-labs-notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "ELASTIC_URL = getpass(\"Elastic URL: \")\n",
    "KIBANA_URL = getpass(\"Kibana URL: \")\n",
    "ELASTIC_API_KEY = getpass(\"Elastic API Key: \")\n",
    "OPENROUTER_AGENT_KEY = getpass(\"OpenRouter Agent API Key: \")\n",
    "\n",
    "# Using separate keys allows differentiating costs between ingestion and agent chat in monitoring dashboards\n",
    "OPENROUTER_INGESTION_KEY = (\n",
    "    getpass(\"OpenRouter Ingestion API Key (press Enter to use Agent key): \")\n",
    "    or OPENROUTER_AGENT_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Elasticsearch Client\n",
    "\n",
    "Create the Elasticsearch client with a higher timeout for inference operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import requests\n",
    "\n",
    "es = Elasticsearch(hosts=[ELASTIC_URL], api_key=ELASTIC_API_KEY, request_timeout=60)\n",
    "\n",
    "# Verify connection\n",
    "info = es.info()\n",
    "print(f\"Connected to Elasticsearch {info['version']['number']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AI Connector for Agent Builder\n",
    "\n",
    "The AI Connector allows Agent Builder to communicate with LLMs through OpenRouter. We use a reasoning-capable model like GPT-5.2 for the agent since it needs to handle complex queries and tool orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector_payload = {\n",
    "    \"name\": \"OpenRouter Agent Connector\",\n",
    "    \"connector_type_id\": \".gen-ai\",\n",
    "    \"config\": {\n",
    "        \"apiProvider\": \"Other\",\n",
    "        \"apiUrl\": \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "        \"defaultModel\": \"openai/gpt-5.2\",\n",
    "        \"enableNativeFunctionCalling\": True,\n",
    "    },\n",
    "    \"secrets\": {\"apiKey\": OPENROUTER_AGENT_KEY},\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{KIBANA_URL}/api/actions/connector\",\n",
    "    headers={\n",
    "        \"kbn-xsrf\": \"true\",\n",
    "        \"Authorization\": f\"ApiKey {ELASTIC_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    },\n",
    "    json=connector_payload,\n",
    ")\n",
    "\n",
    "connector = response.json()\n",
    "if \"id\" in connector:\n",
    "    print(f\"Connector created: {connector['id']}\")\n",
    "else:\n",
    "    print(f\"Response: {connector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Inference Endpoint\n",
    "\n",
    "The inference endpoint allows Elasticsearch to call LLMs during data processing. We use a fast, cheaper model like GPT-4.1-mini for bulk ingestion tasks that don't require advanced reasoning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_config = {\n",
    "    \"service\": \"openai\",\n",
    "    \"service_settings\": {\n",
    "        \"model_id\": \"openai/gpt-4.1-mini\",\n",
    "        \"api_key\": OPENROUTER_INGESTION_KEY,\n",
    "        \"url\": \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "    },\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = es.inference.put(\n",
    "        inference_id=\"openrouter-inference-endpoint\",\n",
    "        task_type=\"completion\",\n",
    "        body=inference_config,\n",
    "    )\n",
    "    print(f\"Inference endpoint created: {response['inference_id']}\")\n",
    "except Exception as e:\n",
    "    if \"resource_already_exists\" in str(e).lower():\n",
    "        print(\"Inference endpoint already exists\")\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Ingest Pipeline\n",
    "\n",
    "The ingest pipeline extracts structured fields from product descriptions using the LLM. The key is providing possible values as enums so the LLM groups consistently. Otherwise, we might get variations like \"Noise Cancellation\", \"ANC\", and \"noise-cancelling\" that are harder to aggregate.\n",
    "\n",
    "The pipeline has four processors:\n",
    "1. **Script**: Builds the extraction prompt with the product description\n",
    "2. **Inference**: Calls OpenRouter to extract structured data\n",
    "3. **JSON**: Parses the response and adds fields to the document\n",
    "4. **Remove**: Cleans up temporary fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the extraction prompt\n",
    "EXTRACTION_PROMPT = (\n",
    "    \"Extract audio product information from this description. \"\n",
    "    \"Return raw JSON only, no markdown, no explanation. Fields: \"\n",
    "    \"category (string, one of: Headphones/Earbuds/Speakers/Microphones/Accessories), \"\n",
    "    \"features (array of strings from: wireless/noise_cancellation/long_battery/waterproof/voice_assistant/fast_charging/portable/surround_sound), \"\n",
    "    \"use_case (string, one of: Travel/Office/Home/Fitness/Gaming/Studio). \"\n",
    "    \"Description: \"\n",
    ")\n",
    "\n",
    "# Create the enrichment pipeline\n",
    "pipeline_config = {\n",
    "    \"processors\": [\n",
    "        {\"script\": {\"source\": f\"ctx.prompt = '{EXTRACTION_PROMPT}' + ctx.description\"}},\n",
    "        {\n",
    "            \"inference\": {\n",
    "                \"model_id\": \"openrouter-inference-endpoint\",\n",
    "                \"input_output\": {\n",
    "                    \"input_field\": \"prompt\",\n",
    "                    \"output_field\": \"ai_response\",\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        {\"json\": {\"field\": \"ai_response\", \"add_to_root\": True}},\n",
    "        {\"remove\": {\"field\": [\"prompt\", \"ai_response\"]}},\n",
    "    ]\n",
    "}\n",
    "\n",
    "es.ingest.put_pipeline(id=\"product-enrichment-pipeline\", body=pipeline_config)\n",
    "\n",
    "print(\"Pipeline created: product-enrichment-pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Index with Mapping\n",
    "\n",
    "Create the index with the appropriate mappings. The enriched fields (category, features, use_case) are mapped as keywords for efficient filtering and aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"text\"},\n",
    "            \"description\": {\"type\": \"text\"},\n",
    "            \"price\": {\"type\": \"float\"},\n",
    "            \"category\": {\"type\": \"keyword\"},\n",
    "            \"features\": {\"type\": \"keyword\"},\n",
    "            \"use_case\": {\"type\": \"keyword\"},\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "es.indices.create(index=\"products-enriched\", body=index_mapping)\n",
    "\n",
    "print(\"Index created: products-enriched\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Index Sample Products\n\nIndex sample audio products using the enrichment pipeline. Each document will be processed by the LLM to extract structured fields.\n\n> **Note:** For production use with larger datasets, use the [Bulk API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-bulk) for better performance. We use individual indexing here to clearly show each document being processed."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = [\n",
    "    {\n",
    "        \"name\": \"Wireless Noise-Canceling Headphones\",\n",
    "        \"description\": \"Premium wireless Bluetooth headphones with active noise cancellation, 30-hour battery life, and premium leather ear cushions. Perfect for travel and office use.\",\n",
    "        \"price\": 299.99,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Portable Bluetooth Speaker\",\n",
    "        \"description\": \"Compact waterproof speaker with 360-degree surround sound. 20-hour battery life, perfect for outdoor adventures and pool parties.\",\n",
    "        \"price\": 149.99,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Studio Condenser Microphone\",\n",
    "        \"description\": \"Professional USB microphone with noise cancellation and voice assistant compatibility. Ideal for podcasting, streaming, and home studio recording.\",\n",
    "        \"price\": 199.99,\n",
    "    },\n",
    "]\n",
    "\n",
    "for i, product in enumerate(products):\n",
    "    es.index(\n",
    "        index=\"products-enriched\",\n",
    "        id=i,\n",
    "        body=product,\n",
    "        pipeline=\"product-enrichment-pipeline\",\n",
    "    )\n",
    "    print(f\"Indexed: {product['name']}\")\n",
    "\n",
    "# Refresh to make documents searchable\n",
    "es.indices.refresh(index=\"products-enriched\")\n",
    "print(\"\\nAll products indexed and refreshed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Enrichment\n",
    "\n",
    "Let's verify that the pipeline correctly extracted structured fields from the product descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = es.search(\n",
    "    index=\"products-enriched\", body={\"query\": {\"match_all\": {}}, \"size\": 10}\n",
    ")\n",
    "\n",
    "print(\"Enriched Products:\\n\")\n",
    "for hit in results[\"hits\"][\"hits\"]:\n",
    "    source = hit[\"_source\"]\n",
    "    print(f\"Name: {source['name']}\")\n",
    "    print(f\"  Category: {source.get('category', 'N/A')}\")\n",
    "    print(f\"  Features: {source.get('features', 'N/A')}\")\n",
    "    print(f\"  Use Case: {source.get('use_case', 'N/A')}\")\n",
    "    print(f\"  Price: ${source['price']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agent Builder Agent\n",
    "\n",
    "Create an Agent Builder agent that can answer questions about the product catalog using semantic search and ES|QL for analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_payload = {\n",
    "    \"id\": \"audio-product-assistant\",\n",
    "    \"name\": \"Audio Product Assistant\",\n",
    "    \"description\": \"Answers questions about audio product catalog using semantic search and analytics\",\n",
    "    \"labels\": [\"audio\"],\n",
    "    \"avatar_color\": \"#BFDBFF\",\n",
    "    \"avatar_symbol\": \"AU\",\n",
    "    \"configuration\": {\n",
    "        \"tools\": [\n",
    "            {\n",
    "                \"tool_ids\": [\n",
    "                    \"platform.core.search\",\n",
    "                    \"platform.core.list_indices\",\n",
    "                    \"platform.core.get_index_mapping\",\n",
    "                    \"platform.core.execute_esql\",\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"instructions\": \"\"\"You are an audio product assistant that helps users find and analyze audio equipment.\n",
    "\n",
    "Use the products-enriched index for all queries. The extracted fields are:\n",
    "- category: Headphones, Earbuds, Speakers, Microphones, or Accessories\n",
    "- features: array of product features like wireless, noise_cancellation, long_battery\n",
    "- use_case: Travel, Office, Home, Fitness, Gaming, or Studio\n",
    "\n",
    "For analytical questions, use ESQL to aggregate data.\n",
    "For product searches, use semantic search on the description field.\"\"\",\n",
    "    },\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{KIBANA_URL}/api/agent_builder/agents\",\n",
    "    headers={\n",
    "        \"kbn-xsrf\": \"true\",\n",
    "        \"Authorization\": f\"ApiKey {ELASTIC_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    },\n",
    "    json=agent_payload,\n",
    ")\n",
    "\n",
    "agent = response.json()\n",
    "if \"id\" in agent:\n",
    "    print(f\"Agent created: {agent['id']}\")\n",
    "else:\n",
    "    print(f\"Response: {agent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with ES|QL Query\n",
    "\n",
    "Let's run a simple ES|QL query to verify the data is accessible and the enriched fields work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esql_query = \"\"\"\n",
    "FROM products-enriched\n",
    "| STATS avg_price = AVG(price), count = COUNT(*) BY category\n",
    "| SORT avg_price DESC\n",
    "\"\"\"\n",
    "\n",
    "result = es.esql.query(query=esql_query)\n",
    "print(\"Average Price by Category:\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure OpenRouter Broadcast\n",
    "\n",
    "To monitor LLM usage, costs, and performance, configure OpenRouter Broadcast to send traces to Elastic APM.\n",
    "\n",
    "### Step 1: Get OpenTelemetry Endpoint\n",
    "\n",
    "Navigate to the APM tutorial in Kibana:\n",
    "```\n",
    "https://<your_kibana_url>/app/observabilityOnboarding/otel-apm/?category=application\n",
    "```\n",
    "\n",
    "Collect the URL and authentication token from the **OpenTelemetry** tab.\n",
    "\n",
    "### Step 2: Configure Broadcast in OpenRouter\n",
    "\n",
    "1. Go to [OpenRouter Broadcast settings](https://openrouter.ai/settings/broadcast)\n",
    "2. Add a new destination for \"OpenTelemetry Collector\"\n",
    "3. Configure the endpoint with the `/v1/traces` path:\n",
    "\n",
    "```\n",
    "Endpoint: https://xxxxx.ingest.us-east-2.aws.elastic-cloud.com:443/v1/traces\n",
    "Headers: {\"Authorization\": \"Bearer YOUR_APM_SECRET_TOKEN\"}\n",
    "```\n",
    "\n",
    "**Important:** Your Kibana server needs to be reachable via the public internet to receive data from OpenRouter.\n",
    "\n",
    "### Step 3: Test Connection\n",
    "\n",
    "Press **Test connection** in OpenRouter and verify you see a success message.\n",
    "\n",
    "### Monitoring Data\n",
    "\n",
    "After configuration, you'll see documents in Kibana under:\n",
    "- Data stream: `traces-generic.otel-default`\n",
    "- Service name: `openrouter`\n",
    "\n",
    "The traces include:\n",
    "- Token usage (prompt, completion, total)\n",
    "- Cost (in USD)\n",
    "- Latency (time to first token, total)\n",
    "- Model information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "Uncomment and run the following code to clean up the resources created in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Uncomment to clean up resources\n\n# Delete index\n# es.indices.delete(index=\"products-enriched\")\n# print(\"Deleted index: products-enriched\")\n\n# Delete pipeline\n# es.ingest.delete_pipeline(id=\"product-enrichment-pipeline\")\n# print(\"Deleted pipeline: product-enrichment-pipeline\")\n\n# Delete inference endpoint\n# es.inference.delete(inference_id=\"openrouter-inference-endpoint\")\n# print(\"Deleted inference endpoint: openrouter-inference-endpoint\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}