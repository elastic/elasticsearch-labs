{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fetch surronding chucks (N-1, N+1)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://ela.st/fetch-surrounding-chunks)\n",
    "\n",
    "This notebook is designed to handle the ingestion of book text (Harry Potter and the Sorcerer's Stone) into an Elasticsearch Cloud instance. It includes partitioning the book text into chapters and chunking the chapter text, which are then ingested into Elasticsearch. The setup utilizes a nested structure, and for each chunk, it stores dense and sparse (ELSER) vector representations along with the text representation.\n",
    "\n",
    "Searches are performed using dense vector comparisons, sparse vector comparisons, and text search in parallel to demonstrate the power of hybrid search strategies. Additionally, the notebook is configured to retrieve adjacent chunks (n-1 and n+1), allowing for a more contextual understanding of the search results.\n",
    "\n"
   ],
   "metadata": {
    "id": "aAUkwshINwV7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install required python libraries\n"
   ],
   "metadata": {
    "id": "MUEpppV7SeLu"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXuL8wsQNq8G",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8d79cd0b-1603-4efe-a039-0b494f3dae5a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: elasticsearch==8.13.2 in /usr/local/lib/python3.10/dist-packages (8.13.2)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.13 in /usr/local/lib/python3.10/dist-packages (from elasticsearch==8.13.2) (8.13.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /usr/local/lib/python3.10/dist-packages (from elastic-transport<9,>=8.13->elasticsearch==8.13.2) (2.0.7)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from elastic-transport<9,>=8.13->elasticsearch==8.13.2) (2024.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: eland in /usr/local/lib/python3.10/dist-packages (8.13.1)\n",
      "Requirement already satisfied: elasticsearch<9,>=8.3 in /usr/local/lib/python3.10/dist-packages (from eland) (8.13.2)\n",
      "Requirement already satisfied: pandas<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from eland) (1.5.3)\n",
      "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.10/dist-packages (from eland) (3.7.1)\n",
      "Requirement already satisfied: numpy<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from eland) (1.23.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from eland) (23.2)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.13 in /usr/local/lib/python3.10/dist-packages (from elasticsearch<9,>=8.3->eland) (8.13.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->eland) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->eland) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->eland) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->eland) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->eland) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->eland) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->eland) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2,>=1.5->eland) (2023.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /usr/local/lib/python3.10/dist-packages (from elastic-transport<9,>=8.13->elasticsearch<9,>=8.3->eland) (2.0.7)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from elastic-transport<9,>=8.13->elasticsearch<9,>=8.3->eland) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->eland) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install elasticsearch==8.13.2\n",
    "!pip install pandas\n",
    "!python -m pip install eland\n",
    "\n",
    "import json\n",
    "import time\n",
    "import urllib.request\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, BertTokenizer\n",
    "from elasticsearch import Elasticsearch, helpers, exceptions\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Elasticsearch and Tokenizer Configuration\n",
    "\n",
    "This section sets up the necessary configurations for connecting to Elasticsearch and initializing the tokenizers used for text processing.\n",
    "\n",
    "### Configuration Details:\n",
    "1. **Elasticsearch Credentials**:\n",
    "   - `ELASTIC_CLOUD_ID`: The Cloud ID for the Elasticsearch cluster, securely fetched using the `getpass` function.\n",
    "   - `ELASTIC_API_KEY`: The API key for Elasticsearch authentication, securely fetched using the `getpass` function.\n",
    "\n",
    "2. **Index Settings**:\n",
    "   - `raw_source_index`: The name of the index for the raw dataset (`harry_potter_dataset-raw`).\n",
    "   - `index_name`: The name of the enriched dataset index (`harry_potter_dataset_enriched`).\n",
    "\n",
    "3. **Embedding Models**:\n",
    "   - `dense_embedding_model_id`: Specifies the model used for generating dense embeddings (`sentence-transformers__all-minilm-l6-v2`).\n",
    "   - `dense_huggingface_model_id`: The Hugging Face model ID for the dense embeddings (`sentence-transformers/all-MiniLM-L6-v2`).\n",
    "   - `dense_model_number_of_allocators`: The number of allocators for the dense embedding model (2).\n",
    "  \n",
    "\n",
    "   - `elser_model_id`: Specifies the ELSER model ID (`.elser_model_2_linux-x86_64`).\n",
    "   - `elser_model_number_of_allocators`: The number of allocators for the ELSER model (2).\n",
    "\n",
    "4. **Tokenizer Initialization**:\n",
    "   - `bert_tokenizer`: Initializes the BERT tokenizer (`bert-base-uncased`) for English text processing.\n",
    "\n",
    "5. **Chunking Parameters**:\n",
    "   - `SEMANTIC_SEARCH_TOKEN_LIMIT`: Sets the token limit for each chunk (500 tokens per chunk, considering space for special tokens).\n",
    "   - `ELSER_TOKEN_OVERLAP`: Defines the overlap ratio between chunks (default is 0%, customizable for context continuity).\n",
    "\n",
    "These configurations ensure that the necessary components are properly set up for efficient text processing, indexing, and search operations in Elasticsearch.\n"
   ],
   "metadata": {
    "id": "2w7uTCYdQ0m6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from getpass import getpass\n",
    "\n",
    "# https://www.elastic.co/search-labs/tutorials/install-elasticsearch/elastic-cloud#finding-your-cloud-id\n",
    "ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID: \")\n",
    "\n",
    "# https://www.elastic.co/search-labs/tutorials/install-elasticsearch/elastic-cloud#creating-an-api-key\n",
    "ELASTIC_API_KEY = getpass(\"Elastic Api Key: \")\n",
    "\n",
    "raw_source_index = \"harry_potter_dataset-raw\"\n",
    "index_name = \"harry_potter_dataset_enriched\"\n",
    "\n",
    "dense_embedding_model_id = \"sentence-transformers__all-minilm-l6-v2\"\n",
    "dense_huggingface_model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "dense_model_number_of_allocators = 2\n",
    "\n",
    "elser_model_id = \".elser_model_2_linux-x86_64\"\n",
    "elser_model_number_of_allocators = 2\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "SEMANTIC_SEARCH_TOKEN_LIMIT = 500\n",
    "ELSER_TOKEN_OVERLAP = 0.0\n",
    "\n",
    "\n",
    "# Create the client instance\n",
    "esclient = Elasticsearch(\n",
    "    cloud_id=ELASTIC_CLOUD_ID,\n",
    "    api_key=ELASTIC_API_KEY,\n",
    ")\n",
    "print(esclient.info())"
   ],
   "metadata": {
    "id": "LGQAjG6PERfx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6389acbb-f13e-4850-8e66-cd814d39bb83"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Elastic Cloud ID: ··········\n",
      "Elastic Api Key: ··········\n",
      "{'name': 'instance-0000000001', 'cluster_name': '951b9d7d79064735b681a5a2d7921825', 'cluster_uuid': 'ITHi4ramTZq6OIR5dEB9Eg', 'version': {'number': '8.14.0', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '8d96bbe3bf5fed931f3119733895458eab75dca9', 'build_date': '2024-06-03T10:05:49.073003402Z', 'build_snapshot': False, 'lucene_version': '9.10.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Import model\n",
    "Using the eland_import_hub_model script, download and install all-MiniLM-L6-v2 transformer model. Setting the NLP --task-type as text_embedding.\n",
    "\n",
    "To get the Cloud ID, go to Elastic cloud and on the deployment overview page, copy down the Cloud ID.\n",
    "\n",
    "To authenticate your request, you could use API key. Alternatively, you can use your cloud deployment username and password."
   ],
   "metadata": {
    "id": "rOWheQ-uJE2C"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!eland_import_hub_model --cloud-id $ELASTIC_CLOUD_ID --es-model-id {dense_embedding_model_id} --hub-model-id {dense_huggingface_model_id} --task-type text_embedding --es-api-key $ELASTIC_API_KEY --start --clear-previous\n",
    "resp = esclient.ml.update_trained_model_deployment(\n",
    "    model_id=dense_embedding_model_id,\n",
    "    body={\"number_of_allocations\": dense_model_number_of_allocators},\n",
    ")\n",
    "print(resp)"
   ],
   "metadata": {
    "id": "4NH8JJkQJDit",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "35415b4a-6ccd-4820-fedc-5f0a63ba1c83"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/eland/ml/_optional.py:116: UserWarning: Eland requires version '1.3' or newer of 'sklearn' (version '1.2.2' currently installed). Use pip or conda to update sklearn.\n",
      "  warnings.warn(msg, UserWarning)\n",
      "2024-06-06 03:41:36,972 INFO : Establishing connection to Elasticsearch\n",
      "2024-06-06 03:41:37,026 INFO : Connected to cluster named '951b9d7d79064735b681a5a2d7921825' (version: 8.14.0)\n",
      "2024-06-06 03:41:37,026 INFO : Loading HuggingFace transformer tokenizer and model 'sentence-transformers/all-MiniLM-L6-v2'\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "STAGE:2024-06-06 03:41:37 636871:636871 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2024-06-06 03:41:37 636871:636871 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2024-06-06 03:41:37 636871:636871 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4481: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "2024-06-06 03:41:39,201 INFO : Stopping deployment for model with id 'sentence-transformers__all-minilm-l6-v2'\n",
      "2024-06-06 03:41:39,339 INFO : Deleting model with id 'sentence-transformers__all-minilm-l6-v2'\n",
      "2024-06-06 03:41:39,493 INFO : Creating model with id 'sentence-transformers__all-minilm-l6-v2'\n",
      "2024-06-06 03:41:39,520 INFO : Uploading model definition\n",
      "100% 87/87 [00:14<00:00,  5.80 parts/s]\n",
      "2024-06-06 03:41:54,517 INFO : Uploading model vocabulary\n",
      "2024-06-06 03:41:54,697 INFO : Starting model deployment\n",
      "2024-06-06 03:41:57,028 INFO : Model successfully imported with id 'sentence-transformers__all-minilm-l6-v2'\n",
      "{'assignment': {'task_parameters': {'model_id': 'sentence-transformers__all-minilm-l6-v2', 'deployment_id': 'sentence-transformers__all-minilm-l6-v2', 'model_bytes': 90303458, 'threads_per_allocation': 1, 'number_of_allocations': 2, 'queue_capacity': 1024, 'cache_size': '90303458b', 'priority': 'normal', 'per_deployment_memory_bytes': 90269696, 'per_allocation_memory_bytes': 236040288}, 'routing_table': {'hO-ZlcEkQW2sb3rdra9ERg': {'current_allocations': 1, 'target_allocations': 2, 'routing_state': 'started', 'reason': ''}}, 'assignment_state': 'started', 'start_time': '2024-06-06T03:41:54.73081128Z', 'max_assigned_allocations': 1}}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Download and Deploy ELSER Model\n",
    "\n",
    "In this example, we are going to download and deploy the ELSER model in our ML node. Make sure you have an ML node in order to run the ELSER model."
   ],
   "metadata": {
    "id": "f1SXd1uhhhhe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# delete model if already downloaded and deployed\n",
    "try:\n",
    "    esclient.ml.delete_trained_model(model_id=elser_model_id, force=True)\n",
    "    print(\"Model deleted successfully, we will proceed with creating one\")\n",
    "except exceptions.NotFoundError:\n",
    "    print(\"Model doesn't exist, but we will proceed with creating one\")\n",
    "\n",
    "# Creates the ELSER model configuration. Automatically downloads the model if it doesn't exist.\n",
    "esclient.ml.put_trained_model(\n",
    "    model_id=elser_model_id, input={\"field_names\": [\"text_field\"]}\n",
    ")"
   ],
   "metadata": {
    "id": "vL68fse9hhAN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above command will download the ELSER model. This will take a few minutes to complete. Use the following command to check the status of the model download."
   ],
   "metadata": {
    "id": "2R54LYIqwC-f"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "while True:\n",
    "    status = esclient.ml.get_trained_models(\n",
    "        model_id=elser_model_id, include=\"definition_status\"\n",
    "    )\n",
    "\n",
    "    if status[\"trained_model_configs\"][0][\"fully_defined\"]:\n",
    "        print(\"ELSER model is downloaded and ready to be deployed.\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"ELSER model is downloaded but not ready to be deployed.\")\n",
    "    time.sleep(5)"
   ],
   "metadata": {
    "id": "wE3KHB3BwCVk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once the model is downloaded, we can deploy the model in our ML node. Use the following command to deploy the model.  This also will take a few minutes to complete.\n"
   ],
   "metadata": {
    "id": "_8-mvOj5wanm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Start ELSER model deployment if not already deployed\n",
    "esclient.ml.start_trained_model_deployment(\n",
    "    model_id=elser_model_id,\n",
    "    number_of_allocations=elser_model_number_of_allocators,\n",
    "    wait_for=\"starting\",\n",
    ")\n",
    "\n",
    "while True:\n",
    "    status = esclient.ml.get_trained_models_stats(\n",
    "        model_id=elser_model_id,\n",
    "    )\n",
    "    if status[\"trained_model_stats\"][0][\"deployment_stats\"][\"state\"] == \"started\":\n",
    "        print(\"ELSER model has been successfully deployed.\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"ELSER model is currently being deployed.\")\n",
    "    time.sleep(5)"
   ],
   "metadata": {
    "id": "xzdANHzxwaSf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Helper Methods/Functions"
   ],
   "metadata": {
    "id": "3LlGP3aJP1ce"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def whitespace_tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def manage_index(es, index_name, settings, mappings, delete_index=False):\n",
    "    if es.indices.exists(index=index_name):\n",
    "        if delete_index:\n",
    "            print(f\"Index {index_name} exists. Deleting it...\")\n",
    "            es.indices.delete(index=index_name)\n",
    "            print(f\"Index {index_name} deleted!\")\n",
    "        else:\n",
    "            print(f\"Index {index_name} already exists. Skipping creation.\")\n",
    "            return\n",
    "    es.indices.create(index=index_name, settings=settings, mappings=mappings)\n",
    "    print(f\"Index {index_name} created successfully!\")\n",
    "\n",
    "\n",
    "def generate_actions(df, index_name):\n",
    "    for _, row in df.iterrows():\n",
    "        chunks = chunk(row[\"chapter_full_text\"])\n",
    "        passages = [\n",
    "            {\"text\": ch[\"text\"], \"chunk_number\": ch[\"chunk_number\"]} for ch in chunks\n",
    "        ]\n",
    "        doc = {\n",
    "            \"_index\": index_name,\n",
    "            \"_source\": {\n",
    "                \"book_title\": row[\"book_title\"],\n",
    "                \"chapter\": row[\"chapter\"],\n",
    "                \"chapter_full_text\": row[\"chapter_full_text\"],\n",
    "                \"passages\": passages,\n",
    "            },\n",
    "        }\n",
    "        yield doc\n",
    "\n",
    "\n",
    "def index_dataframe(es, index_name, df, thread_count=1, chunk_size=200):\n",
    "    print(f\"Indexing documents to {index_name}...\")\n",
    "    success_count = 0\n",
    "    failed_count = 0\n",
    "    try:\n",
    "        for success, _ in helpers.parallel_bulk(\n",
    "            es,\n",
    "            generate_actions(df, index_name),\n",
    "            thread_count=thread_count,\n",
    "            chunk_size=chunk_size,\n",
    "        ):\n",
    "            if success:\n",
    "                success_count += 1\n",
    "            else:\n",
    "                failed_count += 1\n",
    "    except helpers.BulkIndexError as e:\n",
    "        print(\"Bulk indexing error:\", e)\n",
    "        for error_detail in e.errors:\n",
    "            print(error_detail)\n",
    "    print(f\"Successfully indexed {success_count} documents.\")\n",
    "    print(f\"Failed to index {failed_count} documents.\")\n",
    "\n",
    "\n",
    "def build_vector(text):\n",
    "    docs = [{\"text_field\": text}]\n",
    "    response = esclient.ml.infer_trained_model(\n",
    "        model_id=dense_embedding_model_id, docs=docs\n",
    "    )\n",
    "    return response.get(\"inference_results\", [{}])[0].get(\"predicted_value\", [])\n",
    "\n",
    "\n",
    "def build_rrf_query(\n",
    "    embeddings, user_query, rrf_rank_constant, rrf_window_size, debug=False\n",
    "):\n",
    "    query = {\n",
    "        \"_source\": False,\n",
    "        \"sub_searches\": [\n",
    "            {\n",
    "                \"query\": {\n",
    "                    \"nested\": {\n",
    "                        \"path\": \"passages\",\n",
    "                        \"query\": {\"match\": {\"passages.text\": user_query}},\n",
    "                        \"inner_hits\": {\n",
    "                            \"name\": \"text_hits\",\n",
    "                            \"size\": 1,\n",
    "                            \"_source\": [\"passages.text\", \"passages.chunk_number\"],\n",
    "                        },\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"query\": {\n",
    "                    \"nested\": {\n",
    "                        \"path\": \"passages\",\n",
    "                        \"query\": {\n",
    "                            \"knn\": {\n",
    "                                \"query_vector\": embeddings,\n",
    "                                \"field\": \"passages.vector.predicted_value\",\n",
    "                                \"num_candidates\": 50,\n",
    "                            }\n",
    "                        },\n",
    "                        \"inner_hits\": {\n",
    "                            \"name\": \"dense_hit\",\n",
    "                            \"size\": 1,\n",
    "                            \"_source\": [\"passages.text\", \"passages.chunk_number\"],\n",
    "                        },\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"query\": {\n",
    "                    \"nested\": {\n",
    "                        \"path\": \"passages\",\n",
    "                        \"query\": {\n",
    "                            \"bool\": {\n",
    "                                \"should\": [\n",
    "                                    {\n",
    "                                        \"text_expansion\": {\n",
    "                                            \"passages.content_embedding.predicted_value\": {\n",
    "                                                \"model_id\": elser_model_id,\n",
    "                                                \"model_text\": user_query,\n",
    "                                            }\n",
    "                                        }\n",
    "                                    }\n",
    "                                ]\n",
    "                            }\n",
    "                        },\n",
    "                        \"inner_hits\": {\n",
    "                            \"name\": \"sparse_hits\",\n",
    "                            \"size\": 1,\n",
    "                            \"_source\": [\"passages.text\", \"passages.chunk_number\"],\n",
    "                        },\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        ],\n",
    "        \"rank\": {\n",
    "            \"rrf\": {\"window_size\": rrf_window_size, \"rank_constant\": rrf_rank_constant}\n",
    "        },\n",
    "    }\n",
    "    if debug:\n",
    "        print(json.dumps(query, indent=4))\n",
    "    return query\n",
    "\n",
    "\n",
    "def build_custom_query(\n",
    "    query_vector, user_query, knn_boost_factor, text_expansion_boost, debug=False\n",
    "):\n",
    "    query = {\n",
    "        \"_source\": False,\n",
    "        \"fields\": [\"chapter\"],\n",
    "        \"query\": {\n",
    "            \"function_score\": {\n",
    "                \"query\": {\n",
    "                    \"bool\": {\n",
    "                        \"should\": [\n",
    "                            {\n",
    "                                \"nested\": {\n",
    "                                    \"path\": \"passages\",\n",
    "                                    \"query\": {\"match\": {\"passages.text\": user_query}},\n",
    "                                    \"inner_hits\": {\n",
    "                                        \"name\": \"text_hits\",\n",
    "                                        \"size\": 1,\n",
    "                                        \"_source\": [\n",
    "                                            \"passages.text\",\n",
    "                                            \"passages.chunk_number\",\n",
    "                                        ],\n",
    "                                    },\n",
    "                                }\n",
    "                            },\n",
    "                            {\n",
    "                                \"nested\": {\n",
    "                                    \"path\": \"passages\",\n",
    "                                    \"query\": {\n",
    "                                        \"script_score\": {\n",
    "                                            \"query\": {\n",
    "                                                \"knn\": {\n",
    "                                                    \"field\": \"passages.vector.predicted_value\",\n",
    "                                                    \"query_vector\": query_vector,\n",
    "                                                    \"num_candidates\": 50,\n",
    "                                                }\n",
    "                                            },\n",
    "                                            \"script\": {\n",
    "                                                \"source\": \"Math.log(1 + _score * params.boost_factor)\",\n",
    "                                                \"params\": {\n",
    "                                                    \"boost_factor\": knn_boost_factor\n",
    "                                                },\n",
    "                                            },\n",
    "                                        }\n",
    "                                    },\n",
    "                                    \"inner_hits\": {\n",
    "                                        \"name\": \"dense_hit\",\n",
    "                                        \"size\": 1,\n",
    "                                        \"_source\": [\n",
    "                                            \"passages.text\",\n",
    "                                            \"passages.chunk_number\",\n",
    "                                        ],\n",
    "                                    },\n",
    "                                }\n",
    "                            },\n",
    "                            {\n",
    "                                \"nested\": {\n",
    "                                    \"path\": \"passages\",\n",
    "                                    \"query\": {\n",
    "                                        \"script_score\": {\n",
    "                                            \"query\": {\n",
    "                                                \"bool\": {\n",
    "                                                    \"should\": [\n",
    "                                                        {\n",
    "                                                            \"text_expansion\": {\n",
    "                                                                \"passages.content_embedding.predicted_value\": {\n",
    "                                                                    \"model_id\": \".elser_model_2_linux-x86_64\",\n",
    "                                                                    \"model_text\": user_query,\n",
    "                                                                }\n",
    "                                                            }\n",
    "                                                        }\n",
    "                                                    ]\n",
    "                                                }\n",
    "                                            },\n",
    "                                            \"script\": {\n",
    "                                                \"source\": \"_score * params.boost_factor\",\n",
    "                                                \"params\": {\n",
    "                                                    \"boost_factor\": text_expansion_boost\n",
    "                                                },\n",
    "                                            },\n",
    "                                        }\n",
    "                                    },\n",
    "                                    \"inner_hits\": {\n",
    "                                        \"name\": \"sparse_hits\",\n",
    "                                        \"size\": 1,\n",
    "                                        \"_source\": [\n",
    "                                            \"passages.text\",\n",
    "                                            \"passages.chunk_number\",\n",
    "                                        ],\n",
    "                                    },\n",
    "                                }\n",
    "                            },\n",
    "                        ]\n",
    "                    }\n",
    "                },\n",
    "                \"score_mode\": \"sum\",\n",
    "                \"boost_mode\": \"sum\",\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "    if debug:\n",
    "        print(json.dumps(query, indent=4))\n",
    "    return query\n",
    "\n",
    "\n",
    "def get_adjacent_chunks_query(doc_id, base_chunk_number, max_chunk_number, debug=False):\n",
    "    # Determine the chunk numbers to query based on the base_chunk_number\n",
    "    if base_chunk_number == 1:\n",
    "        chunk_numbers = [\n",
    "            base_chunk_number,\n",
    "            base_chunk_number + 1,\n",
    "            base_chunk_number + 2,\n",
    "        ]\n",
    "    elif base_chunk_number == max_chunk_number:\n",
    "        chunk_numbers = [\n",
    "            base_chunk_number,\n",
    "            base_chunk_number - 1,\n",
    "            base_chunk_number - 2,\n",
    "        ]\n",
    "    else:\n",
    "        chunk_numbers = [\n",
    "            base_chunk_number - 1,\n",
    "            base_chunk_number,\n",
    "            base_chunk_number + 1,\n",
    "        ]\n",
    "\n",
    "    # Construct the query\n",
    "    query = {\n",
    "        \"_source\": False,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\"term\": {\"_id\": doc_id}},\n",
    "                    {\n",
    "                        \"nested\": {\n",
    "                            \"path\": \"passages\",\n",
    "                            \"query\": {\n",
    "                                \"bool\": {\n",
    "                                    \"should\": [\n",
    "                                        {\"term\": {\"passages.chunk_number\": num}}\n",
    "                                        for num in chunk_numbers\n",
    "                                    ]\n",
    "                                }\n",
    "                            },\n",
    "                            \"inner_hits\": {\n",
    "                                \"_source\": [\"passages.text\", \"passages.chunk_number\"]\n",
    "                            },\n",
    "                        }\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if debug:\n",
    "        print(json.dumps(query, indent=4))\n",
    "\n",
    "    return query\n",
    "\n",
    "\n",
    "def get_max_chunk_number_query(chapter_number, debug=False):\n",
    "    # Construct the query\n",
    "    query = {\n",
    "        \"size\": 0,\n",
    "        \"query\": {\"term\": {\"chapter\": chapter_number}},\n",
    "        \"aggs\": {\n",
    "            \"max_chunk_number\": {\n",
    "                \"nested\": {\"path\": \"passages\"},\n",
    "                \"aggs\": {\"max_chunk\": {\"max\": {\"field\": \"passages.chunk_number\"}}},\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if debug:\n",
    "        print(json.dumps(query, indent=4))\n",
    "\n",
    "    return query\n",
    "\n",
    "\n",
    "def print_text_from_results(results):\n",
    "    if results[\"hits\"][\"hits\"]:\n",
    "        for hit in results[\"hits\"][\"hits\"]:\n",
    "            if \"inner_hits\" in hit and \"passages\" in hit[\"inner_hits\"]:\n",
    "                nested_hits = hit[\"inner_hits\"][\"passages\"][\"hits\"][\"hits\"]\n",
    "                for nested_hit in nested_hits:\n",
    "                    chunk_number = nested_hit[\"_source\"][\"chunk_number\"]\n",
    "                    text = nested_hit[\"_source\"][\"text\"]\n",
    "                    # print(f\"Text from chunk {chunk_number}: {text}\")\n",
    "                    print(\n",
    "                        f\"\\n\\nText from chunk {chunk_number}: {textwrap.fill(first_passage_text, width=200)}\"\n",
    "                    )\n",
    "    else:\n",
    "        print(\"No hits found.\")\n",
    "\n",
    "\n",
    "def chunk(\n",
    "    text, chunk_size=SEMANTIC_SEARCH_TOKEN_LIMIT, overlap_ratio=ELSER_TOKEN_OVERLAP\n",
    "):\n",
    "    step_size = round(chunk_size * (1 - overlap_ratio))\n",
    "    tokens = bert_tokenizer.encode(text)\n",
    "    tokens = tokens[1:-1]  # remove special beginning and end tokens\n",
    "    result = []\n",
    "    chunk_number = 1\n",
    "    for i in range(0, len(tokens), step_size):\n",
    "        end = i + chunk_size\n",
    "        chunk_text = bert_tokenizer.decode(tokens[i:end])\n",
    "        result.append({\"text\": chunk_text, \"chunk_number\": chunk_number})\n",
    "        chunk_number += 1\n",
    "        if end >= len(tokens):\n",
    "            break\n",
    "    return result\n",
    "\n",
    "\n",
    "def check_task_status(es, task_id):\n",
    "    while True:\n",
    "        task_response = es.tasks.get(task_id=task_id)\n",
    "        if task_response[\"completed\"]:\n",
    "            print(\"Reindexing complete.\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Indexing...\")\n",
    "            time.sleep(10)"
   ],
   "metadata": {
    "id": "xB2a9-qtONbQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Ingest Pipelines"
   ],
   "metadata": {
    "id": "izMU8HqqP7ld"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the ingest pipeline configuration\n",
    "pipeline_body = {\n",
    "    \"description\": \"Pipeline for processing book passages\",\n",
    "    \"processors\": [\n",
    "        {\n",
    "            \"foreach\": {\n",
    "                \"field\": \"passages\",\n",
    "                \"processor\": {\n",
    "                    \"inference\": {\n",
    "                        \"field_map\": {\"_ingest._value.text\": \"text_field\"},\n",
    "                        \"model_id\": dense_embedding_model_id,\n",
    "                        \"target_field\": \"_ingest._value.vector\",\n",
    "                        \"on_failure\": [\n",
    "                            {\n",
    "                                \"append\": {\n",
    "                                    \"field\": \"_source._ingest.inference_errors\",\n",
    "                                    \"value\": [\n",
    "                                        {\n",
    "                                            \"message\": \"Processor 'inference' in pipeline 'ml-inference-title-vector' failed with message '{{ _ingest.on_failure_message }}'\",\n",
    "                                            \"pipeline\": \"ml-inference-title-vector\",\n",
    "                                            \"timestamp\": \"{{{ _ingest.timestamp }}}\",\n",
    "                                        }\n",
    "                                    ],\n",
    "                                }\n",
    "                            }\n",
    "                        ],\n",
    "                    }\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"foreach\": {\n",
    "                \"field\": \"passages\",\n",
    "                \"processor\": {\n",
    "                    \"inference\": {\n",
    "                        \"field_map\": {\"_ingest._value.text\": \"text_field\"},\n",
    "                        \"model_id\": elser_model_id,\n",
    "                        \"target_field\": \"_ingest._value.content_embedding\",\n",
    "                        \"on_failure\": [\n",
    "                            {\n",
    "                                \"append\": {\n",
    "                                    \"field\": \"_source._ingest.inference_errors\",\n",
    "                                    \"value\": [\n",
    "                                        {\n",
    "                                            \"message\": \"Processor 'inference' in pipeline 'ml-inference-title-vector' failed with message '{{ _ingest.on_failure_message }}'\",\n",
    "                                            \"pipeline\": \"ml-inference-title-vector\",\n",
    "                                            \"timestamp\": \"{{{ _ingest.timestamp }}}\",\n",
    "                                        }\n",
    "                                    ],\n",
    "                                }\n",
    "                            }\n",
    "                        ],\n",
    "                    }\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Create or update the pipeline\n",
    "pipeline_id = \"books_dataset_chunker\"\n",
    "esclient.ingest.put_pipeline(id=pipeline_id, body=pipeline_body)\n",
    "print(f\"Ingest pipeline '{pipeline_id}' created/updated successfully.\")"
   ],
   "metadata": {
    "id": "iUOFJK48OamP",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b7feb26f-a084-4d48-dbba-4a53cc0b0255"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ingest pipeline 'books_dataset_chunker' created/updated successfully.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Index Settings"
   ],
   "metadata": {
    "id": "6ZkRwEGdQBRP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 2,\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"default_pipeline\": \"books_dataset_chunker\",\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"dynamic\": \"false\",\n",
    "        \"properties\": {\n",
    "            \"book_title\": {\"type\": \"keyword\"},\n",
    "            \"chapter\": {\"type\": \"keyword\"},\n",
    "            \"chapter_full_text\": {\"type\": \"text\", \"index\": False},\n",
    "            \"passages\": {\n",
    "                \"type\": \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"content_embedding\": {\n",
    "                        \"properties\": {\n",
    "                            \"is_truncated\": {\"type\": \"boolean\"},\n",
    "                            \"model_id\": {\n",
    "                                \"type\": \"text\",\n",
    "                                \"fields\": {\n",
    "                                    \"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}\n",
    "                                },\n",
    "                            },\n",
    "                            \"predicted_value\": {\"type\": \"sparse_vector\"},\n",
    "                        }\n",
    "                    },\n",
    "                    \"text\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}},\n",
    "                    },\n",
    "                    \"vector\": {\n",
    "                        \"properties\": {\n",
    "                            \"is_truncated\": {\"type\": \"boolean\"},\n",
    "                            \"model_id\": {\n",
    "                                \"type\": \"text\",\n",
    "                                \"fields\": {\n",
    "                                    \"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}\n",
    "                                },\n",
    "                            },\n",
    "                            \"predicted_value\": {\n",
    "                                \"type\": \"dense_vector\",\n",
    "                                \"dims\": 384,\n",
    "                                \"index\": True,\n",
    "                                \"similarity\": \"dot_product\",\n",
    "                            },\n",
    "                        }\n",
    "                    },\n",
    "                    \"chunk_number\": {\"type\": \"integer\"},\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "raw_source_index_settings = {\n",
    "    \"settings\": {\"number_of_shards\": 2, \"number_of_replicas\": 0},\n",
    "    \"mappings\": {\n",
    "        \"dynamic\": \"false\",\n",
    "        \"properties\": {\n",
    "            \"book_title\": {\"type\": \"keyword\"},\n",
    "            \"chapter\": {\"type\": \"keyword\"},\n",
    "            \"chapter_full_text\": {\"type\": \"text\", \"index\": False},\n",
    "            \"passages\": {\n",
    "                \"type\": \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"text\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}},\n",
    "                    },\n",
    "                    \"chunk_number\": {\"type\": \"integer\"},\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# Manage indices\n",
    "manage_index(\n",
    "    esclient,\n",
    "    index_name,\n",
    "    index_settings[\"settings\"],\n",
    "    index_settings[\"mappings\"],\n",
    "    delete_index=True,\n",
    ")\n",
    "manage_index(\n",
    "    esclient,\n",
    "    raw_source_index,\n",
    "    raw_source_index_settings[\"settings\"],\n",
    "    raw_source_index_settings[\"mappings\"],\n",
    "    delete_index=True,\n",
    ")"
   ],
   "metadata": {
    "id": "vZ3Z5gZbOgjF",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5a1ed103-d9be-42ae-daac-bab2daca51be"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index harry_potter_dataset_enriched exists. Deleting it...\n",
      "Index harry_potter_dataset_enriched deleted!\n",
      "Index harry_potter_dataset_enriched created successfully!\n",
      "Index harry_potter_dataset-raw exists. Deleting it...\n",
      "Index harry_potter_dataset-raw deleted!\n",
      "Index harry_potter_dataset-raw created successfully!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fetch and Process the Book Text\n",
    "\n",
    "This section downloads the full text of \"Harry Potter and the Sorcerer's Stone\" from a specified URL and processes it to extract chapters and their titles. The text is then structured into a Pandas DataFrame for further analysis and indexing.\n",
    "\n",
    "### Key Steps:\n",
    "1. **Download Text**: The book is fetched using `urllib.request` from the provided URL.\n",
    "2. **Extract Chapters**: The text is split into chapters based on predefined patterns, omitting the text before the first chapter.\n",
    "3. **Capture Chapter Titles**: Chapter titles are extracted and paired with their respective texts.\n",
    "4. **Data Structuring**:\n",
    "   - Convert the list of chapter titles and texts into a DataFrame.\n",
    "   - Assign sequential numbers to chapters.\n",
    "   - Add the book title as metadata.\n",
    "   - Apply a text chunking function to split each chapter into manageable passages.\n",
    "\n",
    "This prepares the text data for efficient indexing and advanced search operations in Elasticsearch.\n"
   ],
   "metadata": {
    "id": "NPtbLhVOQUF3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Fetch and process the book text\n",
    "potter_book_url = \"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt\"\n",
    "response = urllib.request.urlopen(potter_book_url)\n",
    "harry_potter_book_text = response.read().decode(\"utf-8\")\n",
    "chapter_pattern = re.compile(r\"CHAPTER [A-Z]+\", re.IGNORECASE)\n",
    "chapters = chapter_pattern.split(harry_potter_book_text)[1:]\n",
    "chapter_titles = re.findall(chapter_pattern, harry_potter_book_text)\n",
    "chapters_with_titles = list(zip(chapter_titles, chapters))\n",
    "\n",
    "print(\"Total chapters found:\", len(chapters))\n",
    "if chapters_with_titles:\n",
    "    print(\"First chapter title:\", chapters_with_titles[0][0])\n",
    "    print(\"Text sample from first chapter:\", chapters_with_titles[0][1][:500])\n",
    "\n",
    "\n",
    "# Structuring chapters into a DataFrame\n",
    "df = pd.DataFrame(chapters_with_titles, columns=[\"chapter_title\", \"chapter_full_text\"])\n",
    "df[\"chapter\"] = df.index + 1\n",
    "df[\"book_title\"] = \"Harry Potter and the Sorcerer’s Stone\"\n",
    "df[\"passages\"] = df[\"chapter_full_text\"].apply(lambda text: chunk(text))"
   ],
   "metadata": {
    "id": "0L4YI96xOuKn",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "68318a23-b10f-49ab-a329-ec32b1d49993"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6535 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total chapters found: 17\n",
      "First chapter title: CHAPTER ONE\n",
      "Text sample from first chapter: \n",
      "\n",
      "THE BOY WHO LIVED\n",
      "\n",
      "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say\n",
      "that they were perfectly normal, thank you very much. They were the last\n",
      "people you'd expect to be involved in anything strange or mysterious,\n",
      "because they just didn't hold with such nonsense.\n",
      "\n",
      "Mr. Dursley was the director of a firm called Grunnings, which made\n",
      "drills. He was a big, beefy man with hardly any neck, although he did\n",
      "have a very large mustache. Mrs. Dursley was thin and blonde and had\n",
      "nearly t\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Indexing DataFrame into Elasticsearch\n",
    "\n",
    "This section uploads the structured data from a Pandas DataFrame into a specified Elasticsearch index. The DataFrame contains chapter information from \"Harry Potter and the Sorcerer's Stone\", including chapter titles, full texts, and additional metadata.\n",
    "\n",
    "### Key Operation:\n",
    "- **Index Data**: The `index_dataframe` function is called with the Elasticsearch client, the raw source index name, and the DataFrame as arguments. This operation effectively uploads the data into Elasticsearch, making it searchable and ready for further processing.\n"
   ],
   "metadata": {
    "id": "DKK4574EQaTl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "index_dataframe(esclient, raw_source_index, df)"
   ],
   "metadata": {
    "id": "7ReLAtz1O1HF",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e07cace3-8c74-4a72-b2a9-10a7f22d99fd"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Indexing documents to harry_potter_dataset-raw...\n",
      "Successfully indexed 17 documents.\n",
      "Failed to index 0 documents.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Asynchronous Reindexing in Elasticsearch\n",
    "\n",
    "This section initiates an asynchronous reindex operation to transfer data from the raw source index to the enriched index in Elasticsearch. This process runs in the background, allowing other operations to continue without waiting for completion.\n",
    "\n",
    "### Key Steps:\n",
    "1. **Start Reindex**: The reindex operation is triggered from the `raw_source_index` to the `index_name`, with `wait_for_completion` set to `False` to allow asynchronous execution.\n",
    "2. **Retrieve Task ID**: The task ID of the reindex operation is captured and printed for monitoring purposes.\n",
    "3. **Monitor Progress**: The `check_task_status` function continuously checks the status of the reindex task, providing updates every 10 seconds until the operation is complete.\n"
   ],
   "metadata": {
    "id": "pA5QroYdQgcM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Start the reindex operation asynchronously\n",
    "response = esclient.reindex(\n",
    "    body={\"source\": {\"index\": raw_source_index}, \"dest\": {\"index\": index_name}},\n",
    "    wait_for_completion=False,\n",
    ")\n",
    "task_id = response[\"task\"]\n",
    "print(\"Task ID:\", task_id)\n",
    "check_task_status(esclient, task_id)"
   ],
   "metadata": {
    "id": "HOCX_lbmO3zl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4e8a2859-6c28-42ff-b956-7183c80ede9e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Task ID: _m32HYljRgqsVl7G-4wPtw:23883\n",
      "Indexing...\n",
      "Indexing...\n",
      "Indexing...\n",
      "Reindexing complete.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom Search Query Construction and Execution\n",
    "\n",
    "This section constructs and executes a custom search query in Elasticsearch, utilizing a hybrid approach combining vector and text-based search methods to enhance search accuracy and relevance. The specific example used is a user query about the \"Nimbus 2000\".\n",
    "\n",
    "### Key Steps:\n",
    "1. **Define User Query**: The user query is specified as \"what is a nimbus 2000\".\n",
    "2. **Set Boost Factors**:\n",
    "   - `knn_boost_factor`: A value to amplify the importance of the vector-based search component.\n",
    "   - `text_expansion_boost`: A value to modify the weight of the text-based search component.\n",
    "3. **Build Query**: The `build_custom_query` function constructs the search query, incorporating both dense vector and text expansion components.\n",
    "4. **Execute Search**: The query is executed against the specified Elasticsearch index.\n",
    "5. **Identify Relevant Passages**:\n",
    "   - The search results are analyzed to find the passage with the highest relevance score.\n",
    "   - The ID and chunk number of the best matching passage are captured and printed.\n",
    "6. **Fetch Surrounding Chunks**: Constructs and executes a query to retrieve chunks adjacent to the identified passage for broader context. If the matched chunk is the first chunk, fetches n, n+1, and n+2. If the chunk is the last chunk in the chapter, fetches n, n-1, and n-2. For other chunks, fetches n-1, n, and n+1.\n",
    "7. **Display Results**: Outputs text from the relevant and adjacent passages."
   ],
   "metadata": {
    "id": "xJBDwRmDQq4n"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Custom Search Query Construction\n",
    "user_query = \"what is a nimbus 2000\"\n",
    "\n",
    "\n",
    "knn_boost_factor = 20\n",
    "text_expansion_boost = 1\n",
    "query = build_custom_query(\n",
    "    build_vector(user_query),\n",
    "    user_query,\n",
    "    knn_boost_factor,\n",
    "    text_expansion_boost,\n",
    "    debug=False,\n",
    ")\n",
    "\n",
    "# Searching and identifying relevant passages\n",
    "results = esclient.search(index=index_name, body=query, _source=False)\n",
    "\n",
    "hit_id = None\n",
    "chunk_number = None\n",
    "chapter_number = None\n",
    "max_chunk_number = None\n",
    "max_chapter_chunk_result = None\n",
    "max_chunk_query = None\n",
    "\n",
    "\n",
    "if results and results.get(\"hits\") and results[\"hits\"].get(\"hits\"):\n",
    "    highest_score = -1\n",
    "    best_hit = None\n",
    "    hit_id = results[\"hits\"][\"hits\"][0][\"_id\"]\n",
    "    chapter_number = results[\"hits\"][\"hits\"][0][\"fields\"][\"chapter\"][0]\n",
    "    if \"inner_hits\" in results[\"hits\"][\"hits\"][0]:\n",
    "        for hit_type in [\"text_hits\", \"dense_hit\", \"sparse_hits\"]:\n",
    "            if hit_type in results[\"hits\"][\"hits\"][0][\"inner_hits\"]:\n",
    "                inner_hit = results[\"hits\"][\"hits\"][0][\"inner_hits\"][hit_type][\"hits\"]\n",
    "                if inner_hit[\"hits\"]:\n",
    "                    max_score = inner_hit[\"max_score\"]\n",
    "                    if max_score and max_score > highest_score:\n",
    "                        highest_score = max_score\n",
    "                        best_hit = inner_hit[\"hits\"][0]\n",
    "\n",
    "    if best_hit:\n",
    "        first_passage_text = best_hit[\"_source\"][\"text\"]\n",
    "        chunk_number = best_hit[\"_source\"][\"chunk_number\"]\n",
    "        # print(f\"Matched Chunk ID: {hit_id}, Chunk Number: {chunk_number}, Text: {first_passage_text}\")\n",
    "        print(\n",
    "            f\"Matched Chunk ID: {hit_id}, Chunk Number: {chunk_number}, Text:\\n{textwrap.fill(first_passage_text, width=200)}\"\n",
    "        )\n",
    "        print(f\"\\n\")\n",
    "    else:\n",
    "        print(f\"ID: {hit_id}, No relevant passages found.\")\n",
    "else:\n",
    "    print(\"No results found.\")\n",
    "\n",
    "# Fetch Surrounding Chunks if chapter_number is not None\n",
    "if chapter_number is not None:\n",
    "    print(f\"Fetch Surrounding Chunks\")\n",
    "    print(f\"------------------------\")\n",
    "\n",
    "    # max_chunk_query = get_max_chunk_number_query(chapter_number, debug=False)\n",
    "    # max_chapter_chunk_result = esclient.search(index=index_name, body=max_chunk_query, _source=False)\n",
    "    max_chapter_chunk_result = esclient.search(\n",
    "        index=index_name,\n",
    "        body=get_max_chunk_number_query(chapter_number, debug=False),\n",
    "        _source=False,\n",
    "    )\n",
    "    max_chunk_number = max_chapter_chunk_result[\"aggregations\"][\"max_chunk_number\"][\n",
    "        \"max_chunk\"\n",
    "    ][\"value\"]\n",
    "\n",
    "    adjacent_chunks_query = get_adjacent_chunks_query(\n",
    "        hit_id, chunk_number, max_chunk_number, debug=False\n",
    "    )\n",
    "    results = esclient.search(\n",
    "        index=index_name, body=adjacent_chunks_query, _source=False\n",
    "    )\n",
    "    print_text_from_results(results)\n",
    "else:\n",
    "    print(\"Skipping fetch of surrounding chunks due to no initial results.\")\n",
    "\n",
    "\n",
    "# max_chapter_chunk_result = esclient.search(index=index_name, body=get_max_chunk_number_query(chapter_number, debug=False), _source=False)"
   ],
   "metadata": {
    "id": "u7NFZBRJO3t7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "69b06dea-5189-40e7-83c4-bcc8baac5b91"
   },
   "execution_count": 77,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Matched Chunk ID: 6rWk648BZDaSvPZ6OJE_, Chunk Number: 3, Text:\n",
      "t speaking to us? \" said harry. \" yes, don't stop now, \" said ron, \" it's doing us so much good. \" hermione marched away with her nose in the air. harry had a lot of trouble keeping his mind on his\n",
      "lessons that day. it kept wandering up to the dormitory where his new broomstick was lying under his bed, or straying off to the quidditch field where he'd be learning to play that night. he bolted\n",
      "his dinner that evening without noticing what he was eating, and then rushed upstairs with ron to unwrap the nimbus two thousand at last. \" wow, \" ron sighed, as the broomstick rolled onto harry's\n",
      "bedspread. even harry, who knew nothing about the different brooms, thought it looked wonderful. sleek and shiny, with a mahogany handle, it had a long tail of neat, straight twigs and nimbus two\n",
      "thousand written in gold near the top. as seven o'clock drew nearer, harry left the castle and set off in the dusk toward the quidditch field. held never been inside the stadium before. hundreds of\n",
      "seats were raised in stands around the field so that the spectators were high enough to see what was going on. at either end of the field were three golden poles with hoops on the end. they reminded\n",
      "harry of the little plastic sticks muggle children blew bubbles through, except that they were fifty feet high. too eager to fly again to wait for wood, harry mounted his broomstick and kicked off\n",
      "from the ground. what a feeling - - he swooped in and out of the goal posts and then sped up and down the field. the nimbus two thousand turned wherever he wanted at his lightest touch. \" hey, potter,\n",
      "come down!'oliver wood had arrived. fie was carrying a large wooden crate under his arm. harry landed next to him. \" very nice, \" said wood, his eyes glinting. \" i see what mcgonagall meant... you\n",
      "really are a natural. i'm just going to teach you the rules this evening, then you'll be joining team practice three times a week. \" he opened the crate. inside were four different - sized balls. \"\n",
      "right, \" said wood. \" now, quidditch is easy enough to understand, even if it's not too easy to play. there are seven players on each side.\n",
      "\n",
      "\n",
      "Fetch Surrounding Chunks\n",
      "------------------------\n",
      "\n",
      "\n",
      "Text from Chunk 2: t speaking to us? \" said harry. \" yes, don't stop now, \" said ron, \" it's doing us so much good. \" hermione marched away with her nose in the air. harry had a lot of trouble keeping his mind on his\n",
      "lessons that day. it kept wandering up to the dormitory where his new broomstick was lying under his bed, or straying off to the quidditch field where he'd be learning to play that night. he bolted\n",
      "his dinner that evening without noticing what he was eating, and then rushed upstairs with ron to unwrap the nimbus two thousand at last. \" wow, \" ron sighed, as the broomstick rolled onto harry's\n",
      "bedspread. even harry, who knew nothing about the different brooms, thought it looked wonderful. sleek and shiny, with a mahogany handle, it had a long tail of neat, straight twigs and nimbus two\n",
      "thousand written in gold near the top. as seven o'clock drew nearer, harry left the castle and set off in the dusk toward the quidditch field. held never been inside the stadium before. hundreds of\n",
      "seats were raised in stands around the field so that the spectators were high enough to see what was going on. at either end of the field were three golden poles with hoops on the end. they reminded\n",
      "harry of the little plastic sticks muggle children blew bubbles through, except that they were fifty feet high. too eager to fly again to wait for wood, harry mounted his broomstick and kicked off\n",
      "from the ground. what a feeling - - he swooped in and out of the goal posts and then sped up and down the field. the nimbus two thousand turned wherever he wanted at his lightest touch. \" hey, potter,\n",
      "come down!'oliver wood had arrived. fie was carrying a large wooden crate under his arm. harry landed next to him. \" very nice, \" said wood, his eyes glinting. \" i see what mcgonagall meant... you\n",
      "really are a natural. i'm just going to teach you the rules this evening, then you'll be joining team practice three times a week. \" he opened the crate. inside were four different - sized balls. \"\n",
      "right, \" said wood. \" now, quidditch is easy enough to understand, even if it's not too easy to play. there are seven players on each side.\n",
      "\n",
      "\n",
      "Text from Chunk 3: t speaking to us? \" said harry. \" yes, don't stop now, \" said ron, \" it's doing us so much good. \" hermione marched away with her nose in the air. harry had a lot of trouble keeping his mind on his\n",
      "lessons that day. it kept wandering up to the dormitory where his new broomstick was lying under his bed, or straying off to the quidditch field where he'd be learning to play that night. he bolted\n",
      "his dinner that evening without noticing what he was eating, and then rushed upstairs with ron to unwrap the nimbus two thousand at last. \" wow, \" ron sighed, as the broomstick rolled onto harry's\n",
      "bedspread. even harry, who knew nothing about the different brooms, thought it looked wonderful. sleek and shiny, with a mahogany handle, it had a long tail of neat, straight twigs and nimbus two\n",
      "thousand written in gold near the top. as seven o'clock drew nearer, harry left the castle and set off in the dusk toward the quidditch field. held never been inside the stadium before. hundreds of\n",
      "seats were raised in stands around the field so that the spectators were high enough to see what was going on. at either end of the field were three golden poles with hoops on the end. they reminded\n",
      "harry of the little plastic sticks muggle children blew bubbles through, except that they were fifty feet high. too eager to fly again to wait for wood, harry mounted his broomstick and kicked off\n",
      "from the ground. what a feeling - - he swooped in and out of the goal posts and then sped up and down the field. the nimbus two thousand turned wherever he wanted at his lightest touch. \" hey, potter,\n",
      "come down!'oliver wood had arrived. fie was carrying a large wooden crate under his arm. harry landed next to him. \" very nice, \" said wood, his eyes glinting. \" i see what mcgonagall meant... you\n",
      "really are a natural. i'm just going to teach you the rules this evening, then you'll be joining team practice three times a week. \" he opened the crate. inside were four different - sized balls. \"\n",
      "right, \" said wood. \" now, quidditch is easy enough to understand, even if it's not too easy to play. there are seven players on each side.\n",
      "\n",
      "\n",
      "Text from Chunk 4: t speaking to us? \" said harry. \" yes, don't stop now, \" said ron, \" it's doing us so much good. \" hermione marched away with her nose in the air. harry had a lot of trouble keeping his mind on his\n",
      "lessons that day. it kept wandering up to the dormitory where his new broomstick was lying under his bed, or straying off to the quidditch field where he'd be learning to play that night. he bolted\n",
      "his dinner that evening without noticing what he was eating, and then rushed upstairs with ron to unwrap the nimbus two thousand at last. \" wow, \" ron sighed, as the broomstick rolled onto harry's\n",
      "bedspread. even harry, who knew nothing about the different brooms, thought it looked wonderful. sleek and shiny, with a mahogany handle, it had a long tail of neat, straight twigs and nimbus two\n",
      "thousand written in gold near the top. as seven o'clock drew nearer, harry left the castle and set off in the dusk toward the quidditch field. held never been inside the stadium before. hundreds of\n",
      "seats were raised in stands around the field so that the spectators were high enough to see what was going on. at either end of the field were three golden poles with hoops on the end. they reminded\n",
      "harry of the little plastic sticks muggle children blew bubbles through, except that they were fifty feet high. too eager to fly again to wait for wood, harry mounted his broomstick and kicked off\n",
      "from the ground. what a feeling - - he swooped in and out of the goal posts and then sped up and down the field. the nimbus two thousand turned wherever he wanted at his lightest touch. \" hey, potter,\n",
      "come down!'oliver wood had arrived. fie was carrying a large wooden crate under his arm. harry landed next to him. \" very nice, \" said wood, his eyes glinting. \" i see what mcgonagall meant... you\n",
      "really are a natural. i'm just going to teach you the rules this evening, then you'll be joining team practice three times a week. \" he opened the crate. inside were four different - sized balls. \"\n",
      "right, \" said wood. \" now, quidditch is easy enough to understand, even if it's not too easy to play. there are seven players on each side.\n"
     ]
    }
   ]
  }
 ]
}
