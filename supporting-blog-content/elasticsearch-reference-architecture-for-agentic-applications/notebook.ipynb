{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c279f348",
   "metadata": {},
   "source": [
    "# Elasticsearch reference architecture for Agentic applications\n",
    "\n",
    "This notebook in a guide application for building agentic applications using Elasticsearch Agentic Builder as tool provider and LangChain with LangGraph as workflow engine. This notebook is based on the Elastic Labs Blog post [Elasticsearch reference architecture for Agentic applications](https://www.elastic.co/search-labs/blog/elasticsearch-reference-architecture-for-agentic-applications).\n",
    "\n",
    "![Architecture diagram](./arch_diagram.png)\n",
    "\n",
    "## Use Case: Security Vulnerability Agent\n",
    "\n",
    "The Security Vulnerability Agent identifies potential risks based on a user's question by combining three complementary layers:\n",
    "\n",
    "1. **Semantic search** with embeddings over an internal knowledge base of past incidents, configurations, and known vulnerabilities to retrieve relevant historical evidence.\n",
    "2. **Internet search** for newly published advisories or threat intelligence that may not yet exist internally.\n",
    "3. **LLM correlation** that correlates and prioritizes both internal and external findings, evaluates their relevance to the user's specific environment, and produces a clear explanation along with potential mitigation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9600527",
   "metadata": {},
   "source": [
    "# Install dependencies and importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8851c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-mcp-adapters langchain-openai langgraph elasticsearch dotenv rich -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c87fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from typing import TypedDict\n",
    "\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "from rich.panel import Panel\n",
    "\n",
    "console = Console()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7e7d9",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Throughout the application we use common items such as the Elasticsearch client and environment variables. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d2a465",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "ELASTICSEARCH_ENDPOINT = os.getenv(\"ELASTICSEARCH_ENDPOINT\")\n",
    "ELASTICSEARCH_API_KEY = os.getenv(\"ELASTICSEARCH_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\")\n",
    "KIBANA_URL = os.getenv(\"KIBANA_URL\")\n",
    "\n",
    "INDEX_NAME = \"security-vulnerabilities\"\n",
    "KIBANA_HEADERS = {\n",
    "    \"kbn-xsrf\": \"true\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"ApiKey {ELASTICSEARCH_API_KEY}\",\n",
    "}  # Useful for Agent Builder setup API calls\n",
    "\n",
    "MCP_ENDPOINT = f\"{KIBANA_URL}/api/agent_builder/mcp\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa333ed1",
   "metadata": {},
   "source": [
    "## Elasticsearch Client\n",
    "\n",
    "Initialize the Elasticsearch client to interact with your Elasticsearch cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5571144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client = Elasticsearch(ELASTICSEARCH_ENDPOINT, api_key=ELASTICSEARCH_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd866d8",
   "metadata": {},
   "source": [
    "## Agent Builder Tool Creation\n",
    "\n",
    "Create a tool specialized in security queries that will perform semantic search. This tool will be available through the Agent Builder MCP server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d197f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "security_search_tool = {\n",
    "    \"id\": \"security-semantic-search\",\n",
    "    \"type\": \"index_search\",\n",
    "    \"description\": \"Search internal security documents including incident reports, pentests, internal CVEs, security guidelines, and architecture decisions. Uses semantic search powered by ELSER to find relevant security information even without exact keyword matches. Returns documents with severity assessment and affected systems.\",\n",
    "    \"tags\": [\"security\", \"semantic\", \"vulnerabilities\"],\n",
    "    \"configuration\": {\n",
    "        \"pattern\": INDEX_NAME,\n",
    "    },\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(\n",
    "        f\"{KIBANA_URL}/api/agent_builder/tools\",\n",
    "        headers=KIBANA_HEADERS,\n",
    "        json=security_search_tool,\n",
    "    )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(\"‚úÖ Security semantic search tool created successfully\")\n",
    "    else:\n",
    "        print(f\"Response: {response.text}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating tool: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a7ca18",
   "metadata": {},
   "source": [
    "## Index Mapping\n",
    "\n",
    "To define the data structure, we need to create an index with appropriate mappings. We are creating a `semantic_text` field to perform semantic search using the information from the fields marked with the `copy_to` property. This enables the ELSER model to generate embeddings for semantic search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae2b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"title\": {\"type\": \"text\", \"copy_to\": \"semantic_field\"},\n",
    "            \"content\": {\"type\": \"text\", \"copy_to\": \"semantic_field\"},\n",
    "            \"doc_type\": {\"type\": \"keyword\", \"copy_to\": \"semantic_field\"},\n",
    "            \"severity\": {\"type\": \"keyword\", \"copy_to\": \"semantic_field\"},\n",
    "            \"affected_systems\": {\"type\": \"keyword\", \"copy_to\": \"semantic_field\"},\n",
    "            \"date\": {\"type\": \"date\"},\n",
    "            \"semantic_field\": {\"type\": \"semantic_text\"},\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "if es_client.indices.exists(index=INDEX_NAME) is False:\n",
    "    es_client.indices.create(index=INDEX_NAME, body=index_mapping)\n",
    "    print(f\"‚úÖ Index '{INDEX_NAME}' created with semantic_text field for ELSER\")\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è  Index '{INDEX_NAME}' already exists, skipping creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb950a3",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "With the mapping definition, we can ingest the data using the bulk API. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4754e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bulk_actions(documents, index_name):\n",
    "    for doc in documents:\n",
    "        yield {\"_index\": index_name, \"_source\": doc}\n",
    "\n",
    "\n",
    "try:\n",
    "    with open(\"dataset.json\", \"r\") as f:\n",
    "        security_documents = json.load(f)\n",
    "\n",
    "    success, failed = helpers.bulk(\n",
    "        es_client,\n",
    "        build_bulk_actions(security_documents, INDEX_NAME),\n",
    "        refresh=True,\n",
    "    )\n",
    "    print(f\"üì• {success} documents indexed successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during bulk indexing: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd90c447",
   "metadata": {},
   "source": [
    "## LangChain MCP Client\n",
    "\n",
    "Here we create an MCP client using LangChain to consume the Agent Builder tools. The Agent Builder MCP server is available at `{KIBANA_URL}/api/agent_builder/mcp` and exposes Elasticsearch data and Agent Builder tools, acting strictly as a tools provider.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0b1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"agent-builder\": {\n",
    "            \"transport\": \"streamable_http\",\n",
    "            \"url\": MCP_ENDPOINT,\n",
    "            \"headers\": {\"Authorization\": f\"ApiKey {ELASTICSEARCH_API_KEY}\"},\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()\n",
    "\n",
    "print(f\"üìã MCP Tools available: {[t.name for t in tools]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db6956c",
   "metadata": {},
   "source": [
    "## Agent Creation\n",
    "\n",
    "Create an agent that selects the appropriate tool based on the user input. The agent is configured with a system prompt that defines it as a cybersecurity expert specializing in infrastructure security.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffd69d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning = {\"effort\": \"low\"}\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5.2-2025-12-11\", reasoning=reasoning, openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "system_prompt = \"\"\"You are a cybersecurity expert specializing in infrastructure security.\n",
    "\n",
    "        Your role is to:\n",
    "        1. Analyze security queries from users\n",
    "        2. Search internal security documents (incidents, pentests, CVEs, guidelines)\n",
    "        3. Provide actionable security recommendations\n",
    "        4. Assess vulnerability severity and impact\n",
    "\n",
    "        When responding:\n",
    "        - Always search internal documents first using the agent builder tools\n",
    "        - Provide specific, technical, and actionable advice\n",
    "        - Cite relevant internal incidents and documentation\n",
    "        - Assess severity (critical, high, medium, low)\n",
    "        - Recommend immediate mitigation steps\n",
    "\n",
    "        Be concise but comprehensive. Focus on practical security guidance.\"\"\"\n",
    "\n",
    "agent = create_agent(llm, tools, system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56433b60",
   "metadata": {},
   "source": [
    "## Agent State Definition\n",
    "\n",
    "We define the application state. This state will be passed through the LangGraph workflow nodes, allowing each node to read and update the state as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc4a288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent state\n",
    "class AgentState(TypedDict):\n",
    "    query: str\n",
    "    agent_builder_response: dict\n",
    "    internet_results: list\n",
    "    final_response: str\n",
    "    needs_internet_search: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd10c3f",
   "metadata": {},
   "source": [
    "## Internet Search Tool\n",
    "\n",
    "Create a tool that searches the internet for newly published advisories or threat intelligence that may not yet exist internally. This tool uses the Serper API to search external sources for CVE, advisories, and security intelligence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08ce712",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"internet_search_tool\")\n",
    "def internet_search_tool(query: str, top_k: int = 3) -> list:\n",
    "    \"\"\"\n",
    "    Search external sources using Serper API for CVE, advisories, and security intelligence.\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"https://google.serper.dev/search\"\n",
    "    payload = {\"q\": query + \" CVE security vulnerability\", \"num\": top_k}\n",
    "    headers_serper = {\"X-API-KEY\": SERPER_API_KEY, \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, headers=headers_serper)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        results = []\n",
    "        for item in data.get(\"organic\", [])[:top_k]:\n",
    "            results.append(\n",
    "                {\n",
    "                    \"title\": item.get(\"title\"),\n",
    "                    \"snippet\": item.get(\"snippet\"),\n",
    "                    \"link\": item.get(\"link\"),\n",
    "                    \"source\": \"Web Search\",\n",
    "                }\n",
    "            )\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error in Serper API: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e49f63b",
   "metadata": {},
   "source": [
    "## LangGraph Workflow Nodes\n",
    "\n",
    "We use LangGraph to define a workflow capable of making decisions, running tool calls, and summarizing results. The workflow consists of four main nodes:\n",
    "\n",
    "- **call_agent_builder_semantic_search**: Queries internal documentation using the Agent Builder MCP server and stores the retrieved messages in the state.\n",
    "- **decide_internet_search**: Analyzes the internal results and determines whether an external search is required.\n",
    "- **perform_internet_search**: Runs an external search using the Serper API when needed.\n",
    "- **generate_final_response**: Correlates internal and external findings and produces a final, actionable cybersecurity analysis for the user.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518ea99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 1: Call Agent Builder semantic search tool via MCP\n",
    "async def call_agent_builder_semantic_search(state: AgentState) -> AgentState:\n",
    "    query = state[\"query\"]\n",
    "    console.print(f\"üîç [cyan]Searching internal docs for:[/cyan] {query}\")\n",
    "\n",
    "    response = await agent.ainvoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": query},\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    messages = response.get(\"messages\", [])\n",
    "    final_message = messages[-1] if messages else None\n",
    "\n",
    "    if final_message:\n",
    "        agent_response_text = final_message.content\n",
    "        console.print(\n",
    "            f\"‚úÖ [green]Agent response:[/green] {agent_response_text[:200]}...\"\n",
    "        )\n",
    "\n",
    "        state[\"agent_builder_response\"] = {\n",
    "            \"response\": {\"message\": agent_response_text},\n",
    "            \"raw_messages\": messages,\n",
    "        }\n",
    "    else:\n",
    "        console.print(\"‚ö†Ô∏è  [yellow]No response from agent[/yellow]\")\n",
    "        state[\"agent_builder_response\"] = {}\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "# Node 2: Decide if internet search needed\n",
    "async def decide_internet_search(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Decide if we need additional internet search based on Agent Builder results. If yes, the state will have a new key called \"needs_internet_search\" with a boolean value.\n",
    "    \"\"\"\n",
    "    # Extract content from Agent Builder response\n",
    "    ab_response = state[\"agent_builder_response\"]\n",
    "    agent_response = (\n",
    "        ab_response.get(\"response\", {}).get(\"message\", \"\") if ab_response else \"\"\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "      User query: \"{state['query']}\"\n",
    "      \n",
    "      Elasticsearch MCP server found the following internal information:\n",
    "      {agent_response[:500] if agent_response else 'No response'}...\n",
    "      \n",
    "      Should we search external sources (CVE databases, security advisories) for additional context?\n",
    "      Consider:\n",
    "      - Search external if query asks about specific CVE numbers or public vulnerabilities\n",
    "      - Search external if internal findings mention vulnerabilities that might have public advisories\n",
    "      - Don't search external if internal findings are comprehensive and specific to company systems\n",
    "    \"\"\"\n",
    "\n",
    "    # Structured output for decision making\n",
    "    class SearchDecision(BaseModel):\n",
    "        \"\"\"Decision on whether additional internet search is needed.\"\"\"\n",
    "\n",
    "        needs_internet_search: bool = Field(\n",
    "            description=\"Whether we need to search external CVE databases and security advisories to complement internal findings\"\n",
    "        )\n",
    "        reasoning: str = Field(description=\"Brief explanation of the decision\")\n",
    "\n",
    "    llm_with_structure = llm.with_structured_output(SearchDecision)\n",
    "\n",
    "    decision: SearchDecision = llm_with_structure.invoke(prompt)\n",
    "    state[\"needs_internet_search\"] = decision.needs_internet_search\n",
    "\n",
    "    status = (\n",
    "        \"‚úÖ [green]Yes[/green]\"\n",
    "        if state[\"needs_internet_search\"]\n",
    "        else \"‚ùå [red]No[/red]\"\n",
    "    )\n",
    "    console.print(f\"ü§î [bold]Internet search needed:[/bold] {status}\")\n",
    "    console.print(f\"   [dim]{decision.reasoning}[/dim]\")\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "# Node 3: Search internet for additional information\n",
    "async def perform_internet_search(state: AgentState) -> AgentState:\n",
    "    if not state[\"needs_internet_search\"]:\n",
    "        console.print(\"‚ÑπÔ∏è  [blue]Internet search not needed, skipping.[/blue]\")\n",
    "        state[\"internet_results\"] = []\n",
    "        return state\n",
    "\n",
    "    query = state[\"query\"]\n",
    "    console.print(f\"üåê [cyan]Performing internet search for:[/cyan] {query}\")\n",
    "\n",
    "    results = internet_search_tool.invoke(query)\n",
    "    state[\"internet_results\"] = results\n",
    "\n",
    "    console.print(f\"üîé [green]Internet search returned {len(results)} results.[/green]\")\n",
    "    for i, res in enumerate(results):\n",
    "        console.print(f\"  [{i}] [link]{res['link']}[/link] - {res['title']}\")\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "# Node 4: Generate final response\n",
    "async def generate_final_response(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Correlate Agent Builder findings with internet search and generate comprehensive response.\n",
    "    \"\"\"\n",
    "    console.print(\"üß† [magenta]Generating final correlated response...[/magenta]\")\n",
    "\n",
    "    ab_response = state[\"agent_builder_response\"]\n",
    "    internal_context = (\n",
    "        ab_response.get(\"response\", {}).get(\"message\", \"No internal findings\")\n",
    "        if ab_response\n",
    "        else \"No internal findings\"\n",
    "    )\n",
    "\n",
    "    # Format internet results with links\n",
    "    internet_results_formatted = []\n",
    "    if state[\"internet_results\"]:\n",
    "        for i, r in enumerate(state[\"internet_results\"], 1):\n",
    "            internet_results_formatted.append(\n",
    "                f\"[{i}] {r['title']}\\n    Snippet: {r['snippet']}\\n    Link: {r['link']}\"\n",
    "            )\n",
    "\n",
    "    external_context = (\n",
    "        \"\\n\\n\".join(internet_results_formatted)\n",
    "        if internet_results_formatted\n",
    "        else \"No internet information searched.\"\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a cybersecurity expert. Provide a comprehensive security analysis.\n",
    "\n",
    "    **User query:** {state['query']}\n",
    "\n",
    "    **Internal company findings (from internal Elasticsearch knowledge base):**\n",
    "    {internal_context}\n",
    "\n",
    "    **External security intelligence (from web sources):**\n",
    "    {external_context}\n",
    "\n",
    "    **Your task:**\n",
    "    1. Correlate internal and external information\n",
    "    2. Assess vulnerability severity and impact on company systems\n",
    "    3. Provide specific, actionable mitigation steps\n",
    "    4. Highlight any gaps in coverage or additional concerns\n",
    "\n",
    "    **IMPORTANT - Citation Format:**\n",
    "    - When referencing information from internal Elasticsearch, use: [internal knowledge]\n",
    "    - When referencing information from external web sources, cite the link number like: [1], [2], etc.\n",
    "    - Example: \"Express 4.17 has a prototype pollution vulnerability [internal knowledge] which is documented as CVE-2022-24999 [1]\"\n",
    "\n",
    "    Be concise, technical, and actionable.\n",
    "    \"\"\"\n",
    "\n",
    "    response_content = llm.invoke(prompt).content\n",
    "\n",
    "    text_parts = []\n",
    "    for item in response_content:\n",
    "        if isinstance(item, dict):\n",
    "            # Extract text from 'text' field if it exists\n",
    "            if \"text\" in item:\n",
    "                text_parts.append(item[\"text\"])\n",
    "            # If it's a string directly, use it\n",
    "            elif isinstance(item.get(\"content\"), str):\n",
    "                text_parts.append(item[\"content\"])\n",
    "        elif isinstance(item, str):\n",
    "            text_parts.append(item)\n",
    "    response = \"\".join(text_parts) if text_parts else str(response_content)\n",
    "\n",
    "    # Append external links section at the end if there are external results\n",
    "    if state[\"internet_results\"]:\n",
    "        response += \"\\n\\n---\\n## Internet References\\n\"\n",
    "        for i, r in enumerate(state[\"internet_results\"], 1):\n",
    "            response += f\"[{i}] {r['title']}\\n    URL: {r['link']}\\n\\n\"\n",
    "\n",
    "    state[\"final_response\"] = response\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377f7fee",
   "metadata": {},
   "source": [
    "## Workflow Definition\n",
    "\n",
    "With the workflow nodes defined, we can now build the LangGraph workflow. The workflow connects the nodes with edges and conditional routing logic. The workflow starts with an internal search, then decides whether external search is needed, and finally generates a comprehensive response that correlates both internal and external findings.\n",
    "\n",
    "![Workflow diagram](./svia_workflow.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b985d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\n",
    "    \"call_agent_builder_semantic_search\", call_agent_builder_semantic_search\n",
    ")\n",
    "workflow.add_node(\"decide_internet_search\", decide_internet_search)\n",
    "workflow.add_node(\"perform_internet_search\", perform_internet_search)\n",
    "workflow.add_node(\"generate_response\", generate_final_response)\n",
    "\n",
    "workflow.add_edge(START, \"call_agent_builder_semantic_search\")\n",
    "workflow.add_edge(\"call_agent_builder_semantic_search\", \"decide_internet_search\")\n",
    "\n",
    "\n",
    "def should_search_in_internet(state: AgentState) -> str:\n",
    "    \"\"\"Route to internet search or directly to generate response\"\"\"\n",
    "    if state[\"needs_internet_search\"]:\n",
    "        return \"perform_internet_search\"\n",
    "\n",
    "    return \"generate_response\"\n",
    "\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"decide_internet_search\",\n",
    "    should_search_in_internet,\n",
    "    {\n",
    "        \"perform_internet_search\": \"perform_internet_search\",\n",
    "        \"generate_response\": \"generate_response\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"perform_internet_search\", \"generate_response\")\n",
    "workflow.add_edge(\"generate_response\", END)\n",
    "\n",
    "compiled_workflow = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4384766a",
   "metadata": {},
   "source": [
    "## Generating the workflow diagram image (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc2bbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Generate the graph visualization\n",
    "    png_data = compiled_workflow.get_graph().draw_mermaid_png()\n",
    "    output_path = \"svia_workflow.png\"\n",
    "\n",
    "    # Save to file\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(png_data)\n",
    "\n",
    "    print(f\"üìä Workflow diagram saved to: {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not generate workflow diagram: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e336b95",
   "metadata": {},
   "source": [
    "# Query execution\n",
    "\n",
    "In this section we execute the workflow with a sample query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc923b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"We are using Node.js with Express 4.17 for our API gateway. Are there known prototype pollution or remote code execution vulnerabilities?\"\n",
    "\n",
    "console.print(\n",
    "    Panel.fit(f\"üîê [bold cyan]QUERY:[/bold cyan] {query}\", border_style=\"cyan\")\n",
    ")\n",
    "\n",
    "initial_state = AgentState(\n",
    "    {\n",
    "        \"query\": query,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Usar ainvoke ya que search_internal_docs es async\n",
    "result = await compiled_workflow.ainvoke(initial_state)\n",
    "\n",
    "console.print(\"\\n\")\n",
    "console.print(\n",
    "    Panel(\n",
    "        Markdown(result[\"final_response\"]),\n",
    "        title=\"üìã FINAL RESPONSE\",\n",
    "        border_style=\"green\",\n",
    "        padding=(1, 2),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f23db7d",
   "metadata": {},
   "source": [
    "# Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b168aa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the index\n",
    "es_client.indices.delete(index=INDEX_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
