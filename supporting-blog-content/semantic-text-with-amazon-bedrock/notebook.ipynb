{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNm1fxC187O92TX+zoVW0AP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# semantic_text with Amazon Bedrock\n","\n","This notebook demonstrates how to work with `semantic_text` with Amazon Bedrock. This demonstration is related to the article [semantic_text with Amazon Bedrock](https://www.elastic.co/search-labs/blog/semantic-text-with-amazon-bedrock)."],"metadata":{"id":"8u767cQZGY8S"}},{"cell_type":"markdown","source":["## Install Packages and Import Necessary Modules"],"metadata":{"id":"Hb7NL7JZG8JW"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TzstFbQI5xDB","executionInfo":{"status":"ok","timestamp":1720891247779,"user_tz":300,"elapsed":15822,"user":{"displayName":"jeff rengifo","userId":"13415845823741737327"}},"outputId":"2587b351-8e02-43da-8d62-bd5954e5487f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting elasticsearch==8.14\n","  Downloading elasticsearch-8.14.0-py3-none-any.whl (480 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.2/480.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting elastic-transport<9,>=8.13 (from elasticsearch==8.14)\n","  Downloading elastic_transport-8.13.1-py3-none-any.whl (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.5/64.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3<3,>=1.26.2 in /usr/local/lib/python3.10/dist-packages (from elastic-transport<9,>=8.13->elasticsearch==8.14) (2.0.7)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from elastic-transport<9,>=8.13->elasticsearch==8.14) (2024.7.4)\n","Installing collected packages: elastic-transport, elasticsearch\n","Successfully installed elastic-transport-8.13.1 elasticsearch-8.14.0\n"]}],"source":["# install packages\n","!python3 -m pip install elasticsearch==8.14\n","\n","\n","# import modules\n","from elasticsearch import Elasticsearch, exceptions\n","from getpass import getpass\n","import json"]},{"cell_type":"markdown","source":["## Declaring Variables\n","\n","This code will create inputs where you can enter your credentials.\n","Here you can learn how to retrieve your Elasticsearch credentials: [Finding Your Cloud ID](https://www.elastic.co/search-labs/tutorials/install-elasticsearch/elastic-cloud#finding-your-cloud-id)."],"metadata":{"id":"KsuDxaUPqmgF"}},{"cell_type":"code","source":["ELASTIC_CLUSTER_ID = getpass(\"Elastic Cloud ID: \")\n","ELASTIC_API_KEY = getpass(\"Elastic Api Key: \")\n","\n","AWS_ACCESS_KEY = getpass(\"AWS acess key: \")\n","AWS_SECRET_KEY = getpass(\"AWS secret key: \")\n","\n","# AWS region\n","region = \"us-east-1\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z90zha3EqaVR","executionInfo":{"status":"ok","timestamp":1720891343570,"user_tz":300,"elapsed":19706,"user":{"displayName":"jeff rengifo","userId":"13415845823741737327"}},"outputId":"2a0b6294-86be-4536-a7d7-5fef2ffd5a93"},"execution_count":7,"outputs":[{"name":"stdout","output_type":"stream","text":["Elastic Cloud ID: ··········\n","Elastic Api Key: ··········\n","AWS acess key: ··········\n","AWS secret key: ··········\n"]}]},{"cell_type":"markdown","source":["## Instance a Elasticsearch client"],"metadata":{"id":"3O2HclcYHEsS"}},{"cell_type":"code","source":["# Create the client instance\n","es_client = Elasticsearch(\n","    cloud_id=ELASTIC_CLUSTER_ID,\n","    api_key=ELASTIC_API_KEY,\n",")"],"metadata":{"id":"MZYzFA8k6YJg","executionInfo":{"status":"ok","timestamp":1720901991494,"user_tz":300,"elapsed":287,"user":{"displayName":"jeff rengifo","userId":"13415845823741737327"}}},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":["## Create Embeddings task\n","\n","Let's create the inference endpoint using the [Create inference API](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-inference-api.html)."],"metadata":{"id":"Ec6TgewRJpSo"}},{"cell_type":"code","source":["try:\n","    es_client.inference.delete_model(inference_id=\"bedrock-embeddings\")\n","except exceptions.NotFoundError:\n","    # Inference endpoint does not exist\n","    pass\n","\n","try:\n","    es_client.options(\n","        request_timeout=60, max_retries=3, retry_on_timeout=True\n","    ).inference.put_model(\n","        task_type=\"text_embedding\",\n","        inference_id=\"bedrock-embeddings\", # The name of your inference endpoint\n","        body={\n","            \"service\": \"amazonbedrock\",\n","            \"service_settings\": {\n","                \"access_key\": AWS_ACCESS_KEY,\n","                \"secret_key\": AWS_SECRET_KEY,\n","                \"region\": region,\n","                \"provider\": \"amazontitan\",\n","                \"model\": \"amazon.titan-embed-text-v1\",\n","            },\n","        },\n","    )\n","    print(\"Inference endpoint created successfully\")\n","except exceptions.BadRequestError as e:\n","    if e.error == \"resource_already_exists_exception\":\n","        print(\"Inference is already created.\")\n","    else:\n","        raise e"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OeltM9WnDB4D","executionInfo":{"status":"ok","timestamp":1720901994948,"user_tz":300,"elapsed":1507,"user":{"displayName":"jeff rengifo","userId":"13415845823741737327"}},"outputId":"6791d1d8-32ba-40fa-e4c0-962e311396b5"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["Inference endpoint created successfully\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-65-27461d581ecc>:8: ElasticsearchWarning: [tracing.apm.agent.global_labels.deploymentId] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  es_client.options(\n","<ipython-input-65-27461d581ecc>:8: ElasticsearchWarning: [tracing.apm.agent.global_labels.deploymentName] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  es_client.options(\n","<ipython-input-65-27461d581ecc>:8: ElasticsearchWarning: [tracing.apm.agent.global_labels.isElasticUser] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  es_client.options(\n","<ipython-input-65-27461d581ecc>:8: ElasticsearchWarning: [tracing.apm.secret_token] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  es_client.options(\n","<ipython-input-65-27461d581ecc>:8: ElasticsearchWarning: [tracing.apm.agent.service_node_name] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  es_client.options(\n","<ipython-input-65-27461d581ecc>:8: ElasticsearchWarning: [tracing.apm.agent.global_labels.subscriptionLevel] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  es_client.options(\n","<ipython-input-65-27461d581ecc>:8: ElasticsearchWarning: [tracing.apm.agent.global_labels.organizationId] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  es_client.options(\n","<ipython-input-65-27461d581ecc>:8: ElasticsearchWarning: [tracing.apm.agent.environment] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  es_client.options(\n","<ipython-input-65-27461d581ecc>:8: ElasticsearchWarning: [tracing.apm.agent.server_url] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  es_client.options(\n"]}]},{"cell_type":"markdown","source":["## Completion Task"],"metadata":{"id":"gd1VbtYz6Y0m"}},{"cell_type":"code","source":["try:\n","    es_client.options(\n","        request_timeout=60, max_retries=3, retry_on_timeout=True\n","    ).inference.put_model(\n","        task_type=\"completion\",\n","        inference_id=\"bedrock-completion\",\n","        body={\n","            \"service\": \"amazonbedrock\",\n","            \"service_settings\": {\n","              \"access_key\": AWS_ACCESS_KEY,\n","              \"secret_key\": AWS_SECRET_KEY,\n","              \"region\": region,\n","              \"model\": \"anthropic.claude-3-haiku-20240307-v1:0\",\n","              \"provider\": \"anthropic\",\n","        }}\n","      )\n","    print(\"Completion task created successfully\")\n","except exceptions.BadRequestError as e:\n","    if e.error == \"resource_already_exists_exception\":\n","        print(\"Completion already created.\")\n","    else:\n","        raise e"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k6brtW01sQjL","executionInfo":{"status":"ok","timestamp":1720901999589,"user_tz":300,"elapsed":552,"user":{"displayName":"jeff rengifo","userId":"13415845823741737327"}},"outputId":"c9411efa-6ac9-4508-df5b-9fec7c620cb2"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["Completion already created.\n"]}]},{"cell_type":"markdown","source":["## Creating Mappings"],"metadata":{"id":"CrrLwgauONDS"}},{"cell_type":"code","source":["try:\n","    es_client.indices.create(\n","        index=\"semantic-text-bedrock\",\n","        body={\n","            \"mappings\": {\n","                \"properties\": {\n","                    \"super_body\": {\n","                        \"type\": \"semantic_text\",\n","                        \"inference_id\": \"bedrock-embeddings\",\n","                    }\n","                }\n","            }\n","        }\n","    )\n","except exceptions.RequestError as e:\n","    if e.error == \"resource_already_exists_exception\":\n","        print(\"Index already exists.\")\n","    else:\n","        raise e"],"metadata":{"id":"0ytI1PAV7hNo","executionInfo":{"status":"ok","timestamp":1720902002657,"user_tz":300,"elapsed":336,"user":{"displayName":"jeff rengifo","userId":"13415845823741737327"}}},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":["## Indexing data"],"metadata":{"id":"YgFmmn_rOm2S"}},{"cell_type":"code","source":["document_content = \"Answer the question _what's the cat thing about?_ , based on the following context \\n ---\\n---\\ntitle: \\\"semantic_text with Amazon Bedrock\\\"\\nslug: \\\"semantic-text-with-amazon-bedrock\\\"\\ndate: \\\"2024-07-11\\\"\\ndescription: \\\"Using semantic_text new feature, and AWS Bedrock as inference endpoint service\\\"\\nauthor:\\n  - slug: gustavo-llermaly\\nimage: \\\"semantic-text-with-amazon-bedrock/cover.png\\\"\\ncategory:\\n  - slug: integrations\\n  - slug: how-to\\n  - slug: generative-ai\\n  - slug: vector-database\\ntags:\\n  - slug: rag\\n  - slug: search\\n---\\n\\n## Introduction\\n\\nSome of the biggest challenges on RAG systems are chunking text, generating embeddings, and then retrieving them.\\nDeciding which settings to use, and how to actually generate the chunks requires developing additional code or using frameworks like [LangChain](https://www.elastic.co/search-labs/integrations/langchain) or [LlamaIndex](https://www.elastic.co/search-labs/integrations/llama-index).\\n\\nFew months ago, we provided a way to [chunk documents using ingest pipelines](https://www.elastic.co/search-labs/blog/chunking-via-ingest-pipelines) , leveraging the recent addition of [nested vector fields](https://www.elastic.co/search-labs/blog/multi-vector-relevance).\\n\\nWith the addition of the [semantic_text mapping type](https://www.elastic.co/search-labs/blog/semantic-search-simplified-semantic-text) the process of chunking text, generating embeddings, and then retrieving them comes to a single place.\\n\\nIn this article, we are going to create an end-to-end RAG application without leaving Elastic, using Bedrock as our inference service.\\n\\n![Diagram](/assets/images/semantic-text-with-amazon-bedrock/diagram.png)\\n\\n### Steps\\n\\n1. [Creating Endpoints](#creating-endpoints)\\n2. [Creating mappings](#creating-mappings)\\n3. [Indexing data](#indexing-data)\\n4. [Asking questions](#asking-questions)\\n\\n## Creating Endpoints\\n\\nBefore creating our index, we must create the endpoints we are going to use for our inference process. The endpoints will be named:\\n\\n1. Embeddings Task\\n2. Completion Task\\n\\nWe will use Bedrock as our provider for both of them. With these two endpoints we can create a full [RAG](https://www.elastic.co/search-labs/blog/retrieval-augmented-generation-rag) application only using Elastic tools!\\n\\nIf you want to read more about how to configure Bedrock, I recommend you read [this article](https://www.elastic.co/search-labs/blog/elasticsearch-amazon-bedrock-support) first.\\n\\n### Embeddings Task\\n\\nThis task will help us create [vector embeddings](https://www.elastic.co/search-labs/tutorials/search-tutorial/vector-search/embeddings-intro) for our documents content and for the questions the user will ask.\\n\\nWith these vectors we can find the chunks that are more relevant to the question and retrieve the documents that contain the answer.\\n\\nGo ahead and run in [Kibana DevTools Console](https://www.elastic.co/guide/en/kibana/current/console-kibana.html) to create the endpoint:\\n\\n```json\\nPUT _inference/text_embedding/bedrock-embeddings\\n {\\n    \\\"service\\\": \\\"amazonbedrock\\\",\\n    \\\"service_settings\\\": {\\n        \\\"access_key\\\": \\\"{AWS_ACCESS_KEY}\\\",\\n        \\\"secret_key\\\": \\\"{AWS_SECRET_KEY}\\\",\\n        \\\"region\\\": \\\"{AWS_REGION}\\\",\\n        \\\"provider\\\": \\\"amazontitan\\\",\\n        \\\"model\\\": \\\"amazon.titan-embed-text-v1\\\"\\n    }\\n}\\n```\\n\\n- _`provider` must be one of `amazontitan, cohere`_\\n- _`model` must be one \\\\_model_id_ [you have access](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html) to in Bedrock\\\\_\\n\\nOptional additional settings\\n\\n- `dimensions`: The output dimensions to use for the inference\\n- `max_input_tokens`:The maximum number of input tokens\\n- `similarity`: The similarity measure to use\\n\\n### Completion Task\\n\\nAfter we find the best chunks, we must send them to the LLM model so it can generate an answer for us.\\n\\nRun the following to add the completion endpoint:\\n\\n```json\\nPUT _inference/completion/bedrock-completion\\n{\\n    \\\"service\\\": \\\"amazonbedrock\\\",\\n    \\\"service_settings\\\": {\\n        \\\"access_key\\\": \\\"{AWS_ACCESS_KEY}\\\",\\n        \\\"secret_key\\\": \\\"{AWS_SECRET_KEY}\\\",\\n        \\\"region\\\": \\\"{AWS_REGION}\\\",\\n        \\\"model\\\": \\\"anthropic.claude-3-haiku-20240307-v1:0\\\",\\n        \\\"provider\\\": \\\"anthropic\\\",\\n    }\\n}\\n```\\n\\n- _`provider` must be one of `amazontitan, anthropic, ai21labs, cohere, meta, mistral`_\\n- _`model` must be one \\\\_model_id_ or ARN [you have access](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html) to in Bedrock\\\\_\\n\\n## Creating Mappings\\n\\nThe new [semantic_text](https://www.elastic.co/search-labs/blog/semantic-search-simplified-semantic-text) mapping type will make things super easy. It will take care of inferring the embedding mappings and configurations, and doing the passage chunking for you! If you want to read more you can go to this great [article](https://www.elastic.co/search-labs/blog/semantic-search-simplified-semantic-text).\\n\\n```json\\nPUT semantic-text-bedrock\\n{\\n  \\\"mappings\\\": {\\n    \\\"properties\\\": {\\n      \\\"super_body\\\": {\\n        \\\"type\\\": \\\"semantic_text\\\",\\n        \\\"inference_id\\\": \\\"bedrock-embeddings\\\"\\n      }\\n    }\\n  }\\n}\\n```\\n\\nYES. That\\'s it. `super_body` is ready to be searched with vectors, and to handle chunking.\\n\\n## Indexing data\\n\\nFor data indexing we have many [methods available](https://www.elastic.co/search-labs/blog/ES-data-ingestion), you can pick the one of your preference.\\n\\nFor simplicity, and _recursivity_, I will just copy this whole article as rich text and store it as a document.\\n\\n![a cat looking into a screen that displays a live feed of the same cat, creating an infinite loop effect.](/assets/images/semantic-text-with-amazon-bedrock/recursive_cat.gif)\\n\\n```json\\nPOST semantic-text-bedrock/_doc\\n{\\n  \\\"super_body\\\": \\\"<The content of this article>\\\"\\n}\\n```\\n\\nWe have it. Time to test.\\n\\n## Asking questions\\n\\nThe question and answer is a two steps process. First we must retrieve the text chunks relevant to the question, and then we must send the chunks to the LLM to generate the answer.\\n\\nWe will explore two strategies to do that, as promised, without any additional code or framework.\\n\\n### Strategy 1: API Calls\\n\\nWe can run two API calls: one to the `_search` endpoint to retrieve the chunk, and another one to the `inference` endpoint to do the LLM completion step.\\n\\n#### Retrieving chunks\\n\\nWe are going to try a sort of \\\"needle in a haystack\\\" query, to make sure the answer from the LLM is obtained from this article, and not from the LLM base knowledge. We are going to ask about the cat gif referring to the recursivity of this article.\\n\\nWe could run the nice and short default query for semantic-text:\\n\\n```json\\nGET semantic-text-bedrock/_search\\n{\\n  \\\"query\\\": {\\n    \\\"semantic\\\": {\\n      \\\"field\\\": \\\"super_body\\\",\\n      \\\"query\\\": \\\"what\\'s the cat thing about?\\\"\\n    }\\n  }\\n}\\n```\\n\\nThe problem is this query will not sort the inner hits (chunks) by relevance, which is what we need if we don\\'t want to send the entire document to the LLM as context. It will sort the document´s relevance _per document_, and not _per chunk_.\\n\\nThis longer query will sort inner hits (chunks) by relevance, so we can grab the juicy ones.\\n\\n```json\\nGET semantic-text-bedrock/_search\\n{\\n  \\\"_source\\\": false,\\n  \\\"retriever\\\": {\\n    \\\"standard\\\": {\\n      \\\"query\\\": {\\n        \\\"nested\\\": {\\n          \\\"path\\\": \\\"super_body.inference.chunks\\\",\\n          \\\"query\\\": {\\n            \\\"knn\\\": {\\n              \\\"field\\\": \\\"super_body.inference.chunks.embeddings\\\",\\n              \\\"query_vector_builder\\\": {\\n                \\\"text_embedding\\\": {\\n                  \\\"model_id\\\": \\\"bedrock-embeddings\\\",\\n                  \\\"model_text\\\": \\\"what\\'s the cat thing about?\\\"\\n                }\\n              }\\n            }\\n          },\\n          \\\"inner_hits\\\": {\\n            \\\"size\\\": 1,\\n            \\\"name\\\": \\\"semantic-text-bedrock.super_body\\\",\\n            \\\"_source\\\": \\\"*.text\\\"\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n\\n_we set root level `_source` to false, because we are interested on the relevant chunks only_\\n\\nAs you can see, we are using [retrievers](https://www.elastic.co/search-labs/blog/elasticsearch-retrievers) for this query, and the response looks like this:\\n\\nNow from the response we can copy the top chunk and combine the text in one big string. What some frameworks do, is to add metadata to each of the chunks.\\n\\n#### Answering the question\\n\\nNow we can use the bedrock completion endpoint we created previously to send this question along with the relevant chunks and get the answer.\\n\\n```json\\nPOST _inference/completion/bedrock-completion\\n{\\n    \\\"input\\\": \\\"\\\"\\\"Answer the question:\\\\n\\n\\n    _what\\'s the cat thing about?_ ,\\n    based on the following context \\\\n\\n\\n    <paste the relevant chunks here>\\\"\\\"\\\"\\n}\\n```\\n\\nLet\\'s take a look at the answer!\\n\\n### Strategy 2: Playground\\n\\nNow you learned how things work internally, let me show you how you can do this nice and easy, and with a nice UI on top. Using [Elastic Playground](https://www.elastic.co/search-labs/blog/rag-playground-introduction).\\n\\nGo to Playground, configure the Bedrock connector, and then select the index we just created and you are ready to go.\\n\\n![Configuring Playground](/assets/images/semantic-text-with-amazon-bedrock/playground_config.gif)\\n\\nFrom here you can start asking questions to your brand new index.\\n\\n![Playground chat](/assets/images/semantic-text-with-amazon-bedrock/playgroud_chat.png)\\n\\n## Conclusion\\n\\nThe new `semantic_text` mapping type makes creating a RAG setup extremely easy, without having to leave the Elastic ecosystem. Things like chunking and mapping settings are not a challenge anymore (at least not initially!), and there are various alternatives to ask questions to the data.\\n\\nAWS Bedrock is fully integrated by providing both embeddings and completion endpoints, and also being included as a Playground connector!.\\n\\n_If you are interested on reproducing the examples of this article, you can find the [Postman collection](https://www.postman.com/collection/) with the requests [here](https://github.com/elastic/elasticsearch-labs/blob/main/supporting-blog-content/semantic-text-with-amazon-bedrock/postman_collection.json)_\\n\\n---\\n\""],"metadata":{"id":"r1csQatEEAGB","executionInfo":{"status":"ok","timestamp":1720902005276,"user_tz":300,"elapsed":294,"user":{"displayName":"jeff rengifo","userId":"13415845823741737327"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["es_client.index(index='semantic-text-bedrock', document={\n","    \"super_body\": document_content\n","})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uhQNJeUX7hR_","executionInfo":{"status":"ok","timestamp":1720902020758,"user_tz":300,"elapsed":3536,"user":{"displayName":"jeff rengifo","userId":"13415845823741737327"}},"outputId":"c91e7540-8d4f-49cf-df46-7d2ea52b5538"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(https://76375649f86c42418232a62cd615875f.us-west2.gcp.elastic-cloud.com:443)> has been marked alive after a successful request\n","<ipython-input-70-dad547fab455>:1: ElasticsearchWarning: [tracing.apm.agent.service_node_name] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  es_client.index(index='semantic-text-bedrock', document={\n","<ipython-input-70-dad547fab455>:1: ElasticsearchWarning: [tracing.apm.secret_token] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  es_client.index(index='semantic-text-bedrock', document={\n","<ipython-input-70-dad547fab455>:1: ElasticsearchWarning: [tracing.apm.agent.global_labels.isElasticUser] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  es_client.index(index='semantic-text-bedrock', document={\n","<ipython-input-70-dad547fab455>:1: ElasticsearchWarning: [tracing.apm.agent.global_labels.deploymentName] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  es_client.index(index='semantic-text-bedrock', document={\n","<ipython-input-70-dad547fab455>:1: ElasticsearchWarning: [tracing.apm.agent.global_labels.deploymentId] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  es_client.index(index='semantic-text-bedrock', document={\n","<ipython-input-70-dad547fab455>:1: ElasticsearchWarning: [tracing.apm.agent.server_url] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  es_client.index(index='semantic-text-bedrock', document={\n","<ipython-input-70-dad547fab455>:1: ElasticsearchWarning: [tracing.apm.agent.environment] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  es_client.index(index='semantic-text-bedrock', document={\n","<ipython-input-70-dad547fab455>:1: ElasticsearchWarning: [tracing.apm.agent.global_labels.organizationId] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  es_client.index(index='semantic-text-bedrock', document={\n","<ipython-input-70-dad547fab455>:1: ElasticsearchWarning: [tracing.apm.agent.global_labels.subscriptionLevel] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  es_client.index(index='semantic-text-bedrock', document={\n"]},{"output_type":"execute_result","data":{"text/plain":["ObjectApiResponse({'_index': 'semantic-text-bedrock', '_id': 'eNG-rZABYjkNAUNO6B0A', '_version': 1, 'result': 'created', '_shards': {'total': 2, 'successful': 2, 'failed': 0}, '_seq_no': 0, '_primary_term': 1})"]},"metadata":{},"execution_count":70}]},{"cell_type":"markdown","source":["## Asking question"],"metadata":{"id":"E9Sbb-O6VsAL"}},{"cell_type":"code","source":["response = es_client.search(\n","    index=\"semantic-text-bedrock\",\n","    body={\n","        \"_source\": False,\n","        \"retriever\": {\n","            \"standard\": {\n","                \"query\": {\n","                    \"nested\": {\n","                        \"path\": \"super_body.inference.chunks\",\n","                        \"query\": {\n","                            \"knn\": {\n","                                \"field\": \"super_body.inference.chunks.embeddings\",\n","                                \"query_vector_builder\": {\n","                                    \"text_embedding\": {\n","                                        \"model_id\": \"bedrock-embeddings\",\n","                                        \"model_text\": \"what's the cat thing about?\",\n","                                    }\n","                                },\n","                            }\n","                        },\n","                        \"inner_hits\": {\n","                            \"size\": 5,\n","                            \"name\": \"semantic-text-bedrock.super_body\",\n","                            \"_source\": \"*.text\",\n","                        },\n","                    }\n","                }\n","            }\n","        },\n","    },\n",")\n","\n","# Print results\n","formatted_json = json.dumps(response.body, indent=4)\n","\n","print(formatted_json)"],"metadata":{"id":"RytL6yugA77h","executionInfo":{"status":"ok","timestamp":1720902026932,"user_tz":300,"elapsed":1339,"user":{"displayName":"jeff rengifo","userId":"13415845823741737327"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2e3f0b9f-8d96-430a-924e-79e92085342d"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","    \"took\": 551,\n","    \"timed_out\": false,\n","    \"_shards\": {\n","        \"total\": 1,\n","        \"successful\": 1,\n","        \"skipped\": 0,\n","        \"failed\": 0\n","    },\n","    \"hits\": {\n","        \"total\": {\n","            \"value\": 1,\n","            \"relation\": \"eq\"\n","        },\n","        \"max_score\": 0.7386911,\n","        \"hits\": [\n","            {\n","                \"_index\": \"semantic-text-bedrock\",\n","                \"_id\": \"eNG-rZABYjkNAUNO6B0A\",\n","                \"_score\": 0.7386911,\n","                \"inner_hits\": {\n","                    \"semantic-text-bedrock.super_body\": {\n","                        \"hits\": {\n","                            \"total\": {\n","                                \"value\": 9,\n","                                \"relation\": \"eq\"\n","                            },\n","                            \"max_score\": 0.7386911,\n","                            \"hits\": [\n","                                {\n","                                    \"_index\": \"semantic-text-bedrock\",\n","                                    \"_id\": \"eNG-rZABYjkNAUNO6B0A\",\n","                                    \"_nested\": {\n","                                        \"field\": \"super_body.inference.chunks\",\n","                                        \"offset\": 0\n","                                    },\n","                                    \"_score\": 0.7386911,\n","                                    \"_source\": {\n","                                        \"text\": \"Answer the question _what's the cat thing about?_ , based on the following context \\n ---\\n---\\ntitle: \\\"semantic_text with Amazon Bedrock\\\"\\nslug: \\\"semantic-text-with-amazon-bedrock\\\"\\ndate: \\\"2024-07-11\\\"\\ndescription: \\\"Using semantic_text new feature, and AWS Bedrock as inference endpoint service\\\"\\nauthor:\\n  - slug: gustavo-llermaly\\nimage: \\\"semantic-text-with-amazon-bedrock/cover.png\\\"\\ncategory:\\n  - slug: integrations\\n  - slug: how-to\\n  - slug: generative-ai\\n  - slug: vector-database\\ntags:\\n  - slug: rag\\n  - slug: search\\n---\\n\\n## Introduction\\n\\nSome of the biggest challenges on RAG systems are chunking text, generating embeddings, and then retrieving them.\\nDeciding which settings to use, and how to actually generate the chunks requires developing additional code or using frameworks like [LangChain](https://www.elastic.co/search-labs/integrations/langchain) or [LlamaIndex](https://www.elastic.co/search-labs/integrations/llama-index).\\n\\nFew months ago, we provided a way to [chunk documents using ingest pipelines](https://www.elastic.co/search-labs/blog/chunking-via-ingest-pipelines) , leveraging the recent addition of [nested vector fields](https://www.elastic.co/search-labs/blog/multi-vector-relevance).\\n\\nWith the addition of the [semantic_text mapping type](https://www.elastic.co/search-labs/blog/semantic-search-simplified-semantic-text) the process of chunking text, generating embeddings, and then retrieving them comes to a single place.\\n\\nIn this article, we are going to create an end-to-end RAG application without leaving Elastic, using Bedrock as our inference service.\\n\\n![Diagram](/assets/images/semantic-text-with-amazon-bedrock/diagram.png)\\n\\n### Steps\\n\\n1. [Creating Endpoints](#creating-endpoints)\\n2. [Creating mappings](#creating-mappings)\\n3. [Indexing data](#indexing-data)\\n4. [Asking questions](#asking-questions)\\n\\n## Creating Endpoints\\n\\nBefore\"\n","                                    }\n","                                },\n","                                {\n","                                    \"_index\": \"semantic-text-bedrock\",\n","                                    \"_id\": \"eNG-rZABYjkNAUNO6B0A\",\n","                                    \"_nested\": {\n","                                        \"field\": \"super_body.inference.chunks\",\n","                                        \"offset\": 7\n","                                    },\n","                                    \"_score\": 0.6258662,\n","                                    \"_source\": {\n","                                        \"text\": \"/blog/elasticsearch-retrievers) for this query, and the response looks like this:\\n\\nNow from the response we can copy the top chunk and combine the text in one big string. What some frameworks do, is to add metadata to each of the chunks.\\n\\n#### Answering the question\\n\\nNow we can use the bedrock completion endpoint we created previously to send this question along with the relevant chunks and get the answer.\\n\\n```json\\nPOST _inference/completion/bedrock-completion\\n{\\n    \\\"input\\\": \\\"\\\"\\\"Answer the question:\\\\n\\n\\n    _what's the cat thing about?_ ,\\n    based on the following context \\\\n\\n\\n    <paste the relevant chunks here>\\\"\\\"\\\"\\n}\\n```\\n\\nLet's take a look at the answer!\\n\\n### Strategy 2: Playground\\n\\nNow you learned how things work internally, let me show you how you can do this nice and easy, and with a nice UI on top. Using [Elastic Playground](https://www.elastic.co/search-labs/blog/rag-playground-introduction).\\n\\nGo to Playground, configure the Bedrock connector, and then select the index we just created and you are ready to go.\\n\\n![Configuring Playground](/assets/images/semantic-text-with-amazon-bedrock/playground_config.gif)\\n\\nFrom here you can start asking questions to your brand new index.\\n\\n![Playground chat](/assets/images/semantic-text-with-amazon-bedrock/playgroud_chat.png)\\n\\n## Conclusion\\n\\nThe new `semantic_text` mapping type makes creating a RAG setup extremely easy, without having to leave the Elastic ecosystem. Things like chunking and mapping settings are not a challenge anymore (at least not initially!), and there are various alternatives to ask questions to the data.\\n\\nAWS Bedrock is fully integrated by providing\"\n","                                    }\n","                                },\n","                                {\n","                                    \"_index\": \"semantic-text-bedrock\",\n","                                    \"_id\": \"eNG-rZABYjkNAUNO6B0A\",\n","                                    \"_nested\": {\n","                                        \"field\": \"super_body.inference.chunks\",\n","                                        \"offset\": 5\n","                                    },\n","                                    \"_score\": 0.6074438,\n","                                    \"_source\": {\n","                                        \"text\": \" this article>\\\"\\n}\\n```\\n\\nWe have it. Time to test.\\n\\n## Asking questions\\n\\nThe question and answer is a two steps process. First we must retrieve the text chunks relevant to the question, and then we must send the chunks to the LLM to generate the answer.\\n\\nWe will explore two strategies to do that, as promised, without any additional code or framework.\\n\\n### Strategy 1: API Calls\\n\\nWe can run two API calls: one to the `_search` endpoint to retrieve the chunk, and another one to the `inference` endpoint to do the LLM completion step.\\n\\n#### Retrieving chunks\\n\\nWe are going to try a sort of \\\"needle in a haystack\\\" query, to make sure the answer from the LLM is obtained from this article, and not from the LLM base knowledge. We are going to ask about the cat gif referring to the recursivity of this article.\\n\\nWe could run the nice and short default query for semantic-text:\\n\\n```json\\nGET semantic-text-bedrock/_search\\n{\\n  \\\"query\\\": {\\n    \\\"semantic\\\": {\\n      \\\"field\\\": \\\"super_body\\\",\\n      \\\"query\\\": \\\"what's the cat thing about?\\\"\\n    }\\n  }\\n}\\n```\\n\\nThe problem is this query will not sort the inner hits (chunks) by relevance, which is what we need if we don't want to send the entire document to the LLM as context. It will sort the document\\u00b4s relevance _per document_, and not _per chunk_.\\n\\nThis longer query will sort inner hits (chunks) by relevance, so we can grab the juicy ones.\\n\\n```json\\nGET semantic-text-bedrock/_search\\n{\\n  \\\"_source\\\": false,\\n  \\\"retriever\\\": {\\n    \\\"standard\\\": {\\n      \\\"query\\\": {\\n        \\\"nested\\\": {\\n          \\\"path\\\": \\\"super_body.inference.chunks\\\",\\n          \\\"query\\\": {\\n            \\\"knn\"\n","                                    }\n","                                },\n","                                {\n","                                    \"_index\": \"semantic-text-bedrock\",\n","                                    \"_id\": \"eNG-rZABYjkNAUNO6B0A\",\n","                                    \"_nested\": {\n","                                        \"field\": \"super_body.inference.chunks\",\n","                                        \"offset\": 6\n","                                    },\n","                                    \"_score\": 0.6043019,\n","                                    \"_source\": {\n","                                        \"text\": \" default query for semantic-text:\\n\\n```json\\nGET semantic-text-bedrock/_search\\n{\\n  \\\"query\\\": {\\n    \\\"semantic\\\": {\\n      \\\"field\\\": \\\"super_body\\\",\\n      \\\"query\\\": \\\"what's the cat thing about?\\\"\\n    }\\n  }\\n}\\n```\\n\\nThe problem is this query will not sort the inner hits (chunks) by relevance, which is what we need if we don't want to send the entire document to the LLM as context. It will sort the document\\u00b4s relevance _per document_, and not _per chunk_.\\n\\nThis longer query will sort inner hits (chunks) by relevance, so we can grab the juicy ones.\\n\\n```json\\nGET semantic-text-bedrock/_search\\n{\\n  \\\"_source\\\": false,\\n  \\\"retriever\\\": {\\n    \\\"standard\\\": {\\n      \\\"query\\\": {\\n        \\\"nested\\\": {\\n          \\\"path\\\": \\\"super_body.inference.chunks\\\",\\n          \\\"query\\\": {\\n            \\\"knn\\\": {\\n              \\\"field\\\": \\\"super_body.inference.chunks.embeddings\\\",\\n              \\\"query_vector_builder\\\": {\\n                \\\"text_embedding\\\": {\\n                  \\\"model_id\\\": \\\"bedrock-embeddings\\\",\\n                  \\\"model_text\\\": \\\"what's the cat thing about?\\\"\\n                }\\n              }\\n            }\\n          },\\n          \\\"inner_hits\\\": {\\n            \\\"size\\\": 1,\\n            \\\"name\\\": \\\"semantic-text-bedrock.super_body\\\",\\n            \\\"_source\\\": \\\"*.text\\\"\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n\\n_we set root level `_source` to false, because we are interested on the relevant chunks only_\\n\\nAs you can see, we are using [retrievers](https://www.elastic.co/search-labs/blog/elasticsearch-retrievers) for this query, and the response looks like this:\\n\\nNow from the response we can copy the top chunk and combine the text in one big string. What some frameworks do, is to add metadata to each of the chunks.\\n\\n#### Answering the question\\n\\nNow we can use the bedrock completion endpoint we created previously to send this question along with the relevant chunks and get the answer.\\n\\n```json\\nPOST _inference/completion/bedrock-completion\\n{\\n    \\\"input\\\": \\\"\\\"\\\"Answer the question:\\\\n\\n\\n    _what's the cat thing about?_ ,\\n    based on the following context \\\\n\\n\\n    <paste the relevant chunks here>\\\"\\\"\\\"\\n}\\n```\\n\\nLet's take a\"\n","                                    }\n","                                },\n","                                {\n","                                    \"_index\": \"semantic-text-bedrock\",\n","                                    \"_id\": \"eNG-rZABYjkNAUNO6B0A\",\n","                                    \"_nested\": {\n","                                        \"field\": \"super_body.inference.chunks\",\n","                                        \"offset\": 4\n","                                    },\n","                                    \"_score\": 0.53672576,\n","                                    \"_source\": {\n","                                        \"text\": \" care of inferring the embedding mappings and configurations, and doing the passage chunking for you! If you want to read more you can go to this great [article](https://www.elastic.co/search-labs/blog/semantic-search-simplified-semantic-text).\\n\\n```json\\nPUT semantic-text-bedrock\\n{\\n  \\\"mappings\\\": {\\n    \\\"properties\\\": {\\n      \\\"super_body\\\": {\\n        \\\"type\\\": \\\"semantic_text\\\",\\n        \\\"inference_id\\\": \\\"bedrock-embeddings\\\"\\n      }\\n    }\\n  }\\n}\\n```\\n\\nYES. That's it. `super_body` is ready to be searched with vectors, and to handle chunking.\\n\\n## Indexing data\\n\\nFor data indexing we have many [methods available](https://www.elastic.co/search-labs/blog/ES-data-ingestion), you can pick the one of your preference.\\n\\nFor simplicity, and _recursivity_, I will just copy this whole article as rich text and store it as a document.\\n\\n![a cat looking into a screen that displays a live feed of the same cat, creating an infinite loop effect.](/assets/images/semantic-text-with-amazon-bedrock/recursive_cat.gif)\\n\\n```json\\nPOST semantic-text-bedrock/_doc\\n{\\n  \\\"super_body\\\": \\\"<The content of this article>\\\"\\n}\\n```\\n\\nWe have it. Time to test.\\n\\n## Asking questions\\n\\nThe question and answer is a two steps process. First we must retrieve the text chunks relevant to the question, and then we must send the chunks to the LLM to generate the answer.\\n\\nWe will explore two strategies to do that, as promised, without any additional code or framework.\\n\\n### Strategy 1: API Calls\\n\\nWe can run two API calls: one to the `_search` endpoint to retrieve the chunk, and another one to the `inference` endpoint to do the LLM completion step.\\n\\n#### Retrieving chunks\\n\\nWe are going to try a\"\n","                                    }\n","                                }\n","                            ]\n","                        }\n","                    }\n","                }\n","            }\n","        ]\n","    }\n","}\n"]}]},{"cell_type":"markdown","source":["## Answering the question"],"metadata":{"id":"nAyDGfz8WV0L"}},{"cell_type":"code","source":["# Extracting chunks from response\n","chunks_arr = []\n","\n","for r in response.body['hits']['hits'][0]['inner_hits']['semantic-text-bedrock.super_body']['hits']['hits']:\n","    chunks_arr.append(r['_source']['text'])\n","\n","chunks_str = '\\n'.join(chunks_arr)\n","\n","input_content = {\n","    \"input\": chunks_str\n","}"],"metadata":{"id":"E8RhnRwAIDsN","executionInfo":{"status":"ok","timestamp":1720900961810,"user_tz":300,"elapsed":589,"user":{"displayName":"jeff rengifo","userId":"13415845823741737327"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["response = es_client.options(\n","    request_timeout=60, max_retries=3, retry_on_timeout=True\n",").inference.inference(\n","    task_type=\"completion\", inference_id=\"bedrock-completion\", body=input_content\n",")\n","\n","# Print results\n","formatted_json = json.dumps(response.body, indent=4)\n","\n","print(formatted_json)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"arBgj7ZfA8A1","executionInfo":{"status":"ok","timestamp":1720900971729,"user_tz":300,"elapsed":3263,"user":{"displayName":"jeff rengifo","userId":"13415845823741737327"}},"outputId":"62bb8d0f-3ecd-4bb8-8397-2d3e1818cebd"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","    \"completion\": [\n","        {\n","            \"result\": \"It seems the \\\"cat thing\\\" is referring to the recursive GIF shown in the article, which depicts a cat looking at a screen that displays the same cat, creating an infinite loop effect. This recursive cat GIF is used as an example to illustrate the concept of the article, which is about creating an end-to-end Retrieving Augmented Generation (RAG) application using Elastic's semantic_text mapping type and AWS Bedrock as the inference service. The article is guiding the reader through the process of setting up the necessary components, indexing the data, and then retrieving and answering questions based on the content.\"\n","        }\n","    ]\n","}\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-60-eaf8e9964268>:1: ElasticsearchWarning: [tracing.apm.agent.global_labels.deploymentId] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  response = es_client.options(\n","<ipython-input-60-eaf8e9964268>:1: ElasticsearchWarning: [tracing.apm.agent.global_labels.deploymentName] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  response = es_client.options(\n","<ipython-input-60-eaf8e9964268>:1: ElasticsearchWarning: [tracing.apm.agent.global_labels.isElasticUser] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  response = es_client.options(\n","<ipython-input-60-eaf8e9964268>:1: ElasticsearchWarning: [tracing.apm.secret_token] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  response = es_client.options(\n","<ipython-input-60-eaf8e9964268>:1: ElasticsearchWarning: [tracing.apm.agent.service_node_name] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  response = es_client.options(\n","<ipython-input-60-eaf8e9964268>:1: ElasticsearchWarning: [tracing.apm.agent.global_labels.subscriptionLevel] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  response = es_client.options(\n","<ipython-input-60-eaf8e9964268>:1: ElasticsearchWarning: [tracing.apm.agent.global_labels.organizationId] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  response = es_client.options(\n","<ipython-input-60-eaf8e9964268>:1: ElasticsearchWarning: [tracing.apm.agent.environment] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  response = es_client.options(\n","<ipython-input-60-eaf8e9964268>:1: ElasticsearchWarning: [tracing.apm.agent.server_url] setting was deprecated in Elasticsearch and will be removed in a future release.\n","  response = es_client.options(\n"]}]},{"cell_type":"markdown","source":["## Deleting\n","\n","Finally, we can delete the resources used to prevent them from consuming resources."],"metadata":{"id":"MV9Je8qQPRLo"}},{"cell_type":"code","source":["# Cleanup - Delete Index\n","es_client.indices.delete(index='semantic-text-bedrock', ignore=[400, 404])\n","\n","# Cleanup - Delete Completions\n","es_client.inference.delete_model(inference_id='bedrock-completion', ignore=[400, 404])\n","\n","# Cleanup - Delete Embeddings Endpoint\n","es_client.inference.delete_model(inference_id='bedrock-embeddings', ignore=[400, 404])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dZPkU4ivA8Du","executionInfo":{"status":"ok","timestamp":1720910279870,"user_tz":300,"elapsed":279,"user":{"displayName":"jeff rengifo","userId":"13415845823741737327"}},"outputId":"9667d876-485b-485b-b8d1-931eb0624632"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-72-c92b2fcea880>:2: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n","  es_client.indices.delete(index='semantic-text-bedrock', ignore=[400, 404])\n","<ipython-input-72-c92b2fcea880>:5: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n","  es_client.inference.delete_model(inference_id='bedrock-completion', ignore=[400, 404])\n","<ipython-input-72-c92b2fcea880>:9: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n","  es_client.inference.delete_model(inference_id='bedrock-embeddings', ignore=[400, 404])\n"]},{"output_type":"execute_result","data":{"text/plain":["ObjectApiResponse({'acknowledged': True, 'pipelines': [], 'indexes': []})"]},"metadata":{},"execution_count":72}]}]}