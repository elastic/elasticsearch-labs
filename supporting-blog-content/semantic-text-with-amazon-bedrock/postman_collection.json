{
	"info": {
		"_postman_id": "73d3076b-a958-4c5d-aeb2-0135acf5df3a",
		"name": "Elastic search labs",
		"description": "Postman collection for semantic_text with AWS Bedrock article.",
		"schema": "https://schema.getpostman.com/json/collection/v2.1.0/collection.json",
		"_exporter_id": "485305"
	},
	"item": [
		{
			"name": "semantic_text with AWS Bedrock",
			"item": [
				{
					"name": "Requests",
					"item": [
						{
							"name": "1. Create embeddings",
							"request": {
								"method": "PUT",
								"header": [],
								"body": {
									"mode": "raw",
									"raw": " {\n    \"service\": \"amazonbedrock\",\n    \"service_settings\": {\n        \"access_key\": \"{{AWS_ACCESS_KEY}}\",\n        \"secret_key\": \"{{AWS_SECRET_KEY}}\",\n        \"region\": \"us-east-1\",\n        \"provider\": \"amazontitan\",\n        \"model\": \"amazon.titan-embed-text-v1\"\n    }\n}",
									"options": {
										"raw": {
											"language": "json"
										}
									}
								},
								"url": {
									"raw": "{{ES_URL}}/_inference/text_embedding/bedrock-embeddings",
									"host": [
										"{{ES_URL}}"
									],
									"path": [
										"_inference",
										"text_embedding",
										"bedrock-embeddings"
									]
								}
							},
							"response": []
						},
						{
							"name": "2. Create completion",
							"request": {
								"method": "PUT",
								"header": [],
								"body": {
									"mode": "raw",
									"raw": "{\n    \"service\": \"amazonbedrock\",\n    \"service_settings\": {\n        \"access_key\": \"{{AWS_ACCESS_KEY}}\",\n        \"secret_key\": \"{{AWS_SECRET_KEY}}\",\n        \"region\": \"us-east-1\",\n        \"provider\": \"anthropic\",\n        \"model\": \"anthropic.claude-3-haiku-20240307-v1:0\"\n    }\n}",
									"options": {
										"raw": {
											"language": "json"
										}
									}
								},
								"url": {
									"raw": "{{ES_URL}}/_inference/completion/bedrock-completion",
									"host": [
										"{{ES_URL}}"
									],
									"path": [
										"_inference",
										"completion",
										"bedrock-completion"
									]
								}
							},
							"response": []
						},
						{
							"name": "3. Create Index",
							"request": {
								"method": "PUT",
								"header": [],
								"body": {
									"mode": "raw",
									"raw": "{\n  \"mappings\": {\n    \"properties\": {\n      \"super_body\": {\n        \"type\": \"semantic_text\",\n        \"inference_id\": \"bedrock-embeddings\"\n      }\n    }\n  }\n}",
									"options": {
										"raw": {
											"language": "json"
										}
									}
								},
								"url": {
									"raw": "{{ES_URL}}/semantic-text-bedrock",
									"host": [
										"{{ES_URL}}"
									],
									"path": [
										"semantic-text-bedrock"
									]
								}
							},
							"response": []
						},
						{
							"name": "4. Index document",
							"request": {
								"method": "POST",
								"header": [],
								"body": {
									"mode": "raw",
									"raw": "{\n  \"super_body\": \"---\\\\ntitle: \\\\\\\"semantic_text with Amazon Bedrock\\\\\\\"\\\\nslug: \\\\\\\"semantic-text-with-amazon-bedrock\\\\\\\"\\\\ndate: \\\\\\\"2024-07-03\\\\\\\"\\\\ndescription: \\\\\\\"Using semantic_text new feature, and Bedrock as inference endpoint service\\\\\\\"\\\\nauthor:\\\\n  - slug: gustavo-llermaly\\\\nimage: \\\\\\\"headshot-gustavo-llermaly.jpeg\\\\\\\"\\\\ncategory:\\\\n  - slug: integrations\\\\n  - slug: how-to\\\\ntags:\\\\n  - slug: stack\\\\n  - slug: search\\\\n---\\\\n\\\\n## Introduction\\\\n\\\\nOne of the biggest challenges on RAG systems is chunking data. \\\\nDeciding which settings to use, and how to actually generate the chunks required developing additional code or using frameworks like LangChain or Llamaindex.\\\\n\\\\nFew months ago, we provided a way to [chunk documents using ingest pipelines](https:\\/\\/www.elastic.co\\/search-labs\\/blog\\/chunking-via-ingest-pipelines) , leveraging the recent addition of [nested vector fields](https:\\/\\/www.elastic.co\\/search-labs\\/blog\\/multi-vector-relevance).\\\\n\\\\nWith the addition of the [semantic_text mapping type](https:\\/\\/www.elastic.co\\/search-labs\\/blog\\/semantic-search-simplified-semantic-text) the process es even simpler!.\\\\n\\\\nOn this article, we are going to create an end-to-end RAG application without leaving Elastic, and using Bedrock as our inference service.\\\\n\\\\n### Steps \\\\n\\\\n1. Creating Endpoints \\\\n2. Creating mappings\\\\n3. Indexing data\\\\n5. Asking questions\\\\n\\\\n## Creating Endpoints \\\\n\\\\nBefore creating our index, we must create the endpoints we are going to use for our inference tasks: \\\\n\\\\n1. Embeddings Task\\\\n2. Completions Task\\\\n\\\\nWe will use Bedrock as our provider for both of them. With this two endpoints we can create a full [RAG](https:\\/\\/www.elastic.co\\/search-labs\\/blog\\/retrieval-augmented-generation-rag) application only using Elastic tools!.\\\\n\\\\nIf you want to read more about how to configure Bedrock I recommend you reading [this article](https:\\/\\/www.elastic.co\\/search-labs\\/blog\\/use-amazon-bedrock-with-elasticsearch-and-langchain) first.\\\\n\\\\n### Embeddings Task \\\\n\\\\nThis task will help us creating [vector embeddings](https:\\/\\/www.elastic.co\\/search-labs\\/tutorials\\/search-tutorial\\/vector-search\\/embeddings-intro) for our documents content, and for the question the user will ask. \\\\n\\\\nWith this vectors we can find the most relevant chunks to the question and retrieve the documents that contain the answer.\\\\n\\\\nGo ahead and run in [Kibana DevTools Console](https:\\/\\/www.elastic.co\\/guide\\/en\\/kibana\\/current\\/console-kibana.html) to create the endpoint: \\\\n\\\\n```json\\\\nPUT _inference\\/text_embedding\\/bedrock-embeddings\\\\n {\\\\n    \\\\\\\"service\\\\\\\": \\\\\\\"amazonbedrock\\\\\\\",\\\\n    \\\\\\\"service_settings\\\\\\\": {\\\\n        \\\\\\\"access_key\\\\\\\": \\\\\\\"{AWS_ACCESS_KEY}\\\\\\\",\\\\n        \\\\\\\"secret_key\\\\\\\": \\\\\\\"{AWS_SECRET_KEY}\\\\\\\",\\\\n        \\\\\\\"region\\\\\\\": \\\\\\\"{AWS_REGION}\\\\\\\",\\\\n        \\\\\\\"provider\\\\\\\": \\\\\\\"cohere\\\\\\\",\\\\n        \\\\\\\"model\\\\\\\": \\\\\\\"cohere.embed-english-v3\\\\\\\"\\\\n    }\\\\n}\\\\n```\\\\n\\\\n- _`provider` must be one of `amazontitan, cohere`_\\\\n- _`model` must be one _model_id_ [you have access](https:\\/\\/docs.aws.amazon.com\\/bedrock\\/latest\\/userguide\\/model-ids.html) to in Bedrock_\\\\n\\\\nOptional additional settings\\\\n-   `dimensions`: The output dimensions to use for the inference\\\\n-   `max_input_tokens`: the maximum number of input tokens\\\\n-   `similarity`: the similarity measure to use\\\\n\\\\n### Completions Task\\\\n\\\\nAfter we find the best chunks, we must send them to the LLM model so it can generate an answer for us. \\\\n\\\\nRun the following to add the completions endpoint: \\\\n\\\\n```json\\\\nPUT _inference\\/completion\\/bedrock-completions\\\\n{\\\\n    \\\\\\\"service\\\\\\\": \\\\\\\"amazonbedrock\\\\\\\",\\\\n    \\\\\\\"service_settings\\\\\\\": {\\\\n        \\\\\\\"access_key\\\\\\\": \\\\\\\"{AWS_ACCESS_KEY}\\\\\\\",\\\\n        \\\\\\\"secret_key\\\\\\\": \\\\\\\"{AWS_SECRET_KEY}\\\\\\\",\\\\n        \\\\\\\"region\\\\\\\": \\\\\\\"{AWS_REGION}\\\\\\\",\\\\n        \\\\\\\"model\\\\\\\": \\\\\\\"anthropic.claude-3-haiku-20240307-v1:0\\\\\\\",\\\\n        \\\\\\\"provider\\\\\\\": \\\\\\\"anthropic\\\\\\\",\\\\n    }\\\\n}\\\\n```\\\\n- _`provider` must be one of `amazontitan, anthropic, ai21labs, cohere, meta, mistral`_\\\\n- _`model` must be one _model_id_ or ARN [you have access](https:\\/\\/docs.aws.amazon.com\\/bedrock\\/latest\\/userguide\\/model-ids.html) to in Bedrock_\\\\n\\\\n## Creating Mappings \\\\n\\\\nThe new [semantic_text](https:\\/\\/www.elastic.co\\/search-labs\\/blog\\/semantic-search-simplified-semantic-text)  mapping type will make things super easy. It will take care of infering the embedding mappings and configurations, and doing the passage chunking for you! If you want to read more you can go to this great [article](https:\\/\\/www.elastic.co\\/search-labs\\/blog\\/semantic-search-simplified-semantic-text). \\\\n\\\\n```json\\\\nPUT semantic-text-bedrock\\\\n{\\\\n  \\\\\\\"mappings\\\\\\\": {\\\\n    \\\\\\\"properties\\\\\\\": {\\\\n      \\\\\\\"super_body\\\\\\\": {\\\\n        \\\\\\\"type\\\\\\\": \\\\\\\"semantic_text\\\\\\\",\\\\n        \\\\\\\"inference_id\\\\\\\": \\\\\\\"bedrock-embeddings\\\\\\\"\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\n```\\\\nYES. That\\\\'s it. `super_body` is ready to be searched with vectors, and to handle chunking.\\\\n\\\\n## Indexing data\\\\n\\\\nFor data indexing we have many [methods available](https:\\/\\/www.elastic.co\\/search-labs\\/blog\\/ES-data-ingestion), you can pick any of your preference. \\\\n\\\\nFor simplicity, and *recursivity*, I will just copy this whole article and store it as a document with many chunks.\\\\n\\\\n![a cat looking into a screen that displays a live feed of the same cat, creating an infinite loop effect.](\\/assets\\/images\\/semantic-text-with-amazon-bedrock\\/recursive_cat.gif)\\\\n\\\\n```json\\\\nPOST semantic-text-bedrock\\/_doc\\\\n{\\\\n  \\\\\\\"super_body\\\\\\\": \\\\\\\"## Introduction \\\\n    One of the biggest challenges on RAG systems is chunking data. \\\\n    Deciding which settings to use, and how to actually generate the chunks required developing additional code or using frameworks like LangChain or Llamaindex.\\\\n  \\\\\\\"\\\\n}\\\\n```\\\\n*Triple quotes are very useful to add long paragraphs to Kibana Console because it handles <a href=\\\\\\\"https:\\/\\/en.wikipedia.org\\/wiki\\/Escape_sequence\\\\\\\" target=\\\\\\\"_blank\\\\\\\">text escaping<\\/a>.*\\\\n\\\\nWe have it. Time to test.\\\\n\\\\n## Asking questions\\\\n\\\\nThe question and answer is a two steps process. First we must retrieve the relevant text chunks to the question, and then we must send the chunks to the LLM to generate the answer. \\\\n\\\\nWe will explore two strategies to do that, as promised, without any additional code or framework.\\\\n\\\\n### Strategy 1: API Calls \\\\n\\\\nWe can run two API calls, one to the `_search` endpoint to retrieve the chunks, and another one to the `inference` endpoint to do the LLM completion step. \\\\n\\\\n#### Retrieving chunks \\\\n\\\\nWe are going to try a sort of \\\\\\\"needle in the haystack\\\\\\\" query, to make sure the answer from the LLM is obtained from this article, and not from the LLM base knowledge. We are going to ask about the cat gif referring to the recursivity of this article.\\\\n\\\\nWe could run the nice and short default query for semantic-text: \\\\n\\\\n```json\\\\nGET semantic-text-bedrock\\/_search\\\\n{\\\\n  \\\\\\\"query\\\\\\\": {\\\\n    \\\\\\\"semantic\\\\\\\": {\\\\n      \\\\\\\"field\\\\\\\": \\\\\\\"super_body\\\\\\\",\\\\n      \\\\\\\"query\\\\\\\": \\\\\\\"what\\\\'s the cat gif thing about?\\\\\\\"\\\\n    }\\\\n  }\\\\n}\\\\n```\\\\n\\\\nThe problem is this query will not sort the inner hits (chunks) by relevance, which is what we need for this use case. It will sort the documents relevance *per document*, and not *per chunk*. \\\\n\\\\nThis longer query will sort inner hits (chunks) by relevance, so we can grab the juicy ones. \\\\n\\\\n```json\\\\nGET semantic-text-bedrock\\/_search\\\\n{\\\\n  \\\\\\\"_source\\\\\\\": false,\\\\n  \\\\\\\"retriever\\\\\\\": {\\\\n    \\\\\\\"standard\\\\\\\": {\\\\n      \\\\\\\"query\\\\\\\": {\\\\n        \\\\\\\"nested\\\\\\\": {\\\\n          \\\\\\\"path\\\\\\\": \\\\\\\"super_body.inference.chunks\\\\\\\",\\\\n          \\\\\\\"query\\\\\\\": {\\\\n            \\\\\\\"knn\\\\\\\": {\\\\n              \\\\\\\"field\\\\\\\": \\\\\\\"super_body.inference.chunks.embeddings\\\\\\\",\\\\n              \\\\\\\"query_vector_builder\\\\\\\": {\\\\n                \\\\\\\"text_embedding\\\\\\\": {\\\\n                  \\\\\\\"model_id\\\\\\\": \\\\\\\"bedrock-embeddings\\\\\\\",\\\\n                  \\\\\\\"model_text\\\\\\\": \\\\\\\"what\\\\'s the cat gif thing about?\\\\\\\"\\\\n                }\\\\n              }\\\\n            }\\\\n          },\\\\n          \\\\\\\"inner_hits\\\\\\\": {\\\\n            \\\\\\\"size\\\\\\\": 1,\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"semantic-text-bedrock.super_body\\\\\\\",\\\\n            \\\\\\\"_source\\\\\\\": \\\\\\\"*.text\\\\\\\"\\\\n          }\\\\n        }\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\n```\\\\n_we set root level `_source` to false, because we are interested on the relevant chunks only_\\\\n\\\\nAs you can see, we are using [retrievers](https:\\/\\/www.elastic.co\\/search-labs\\/blog\\/elasticsearch-retrievers) for this query, and the response looks like this: \\\\n\\\\nRESPONSE PICTURE \\\\n\\\\nNow from the response we can copy the top chunk, and combine the text in one big string. What some frameworks do, is to add metadata to each of the chunks.\\\\n\\\\n#### Answering the question \\\\n\\\\nNow we can use the bedrock completions endpoint we created previously to send this questions along with the relevant chunks and get the answer. \\\\n\\\\n```json\\\\nPOST _inference\\/completion\\/bedrock-completions\\\\n{\\\\n    \\\\\\\"input\\\\\\\": \\\\\\\"Answer the question _what\\\\'s the cat gif thing about?_ , based on the following context \\\\\\\\n <paste the relevant chunks here>\\\\\\\"\\\\n}\\\\n```\\\\nLet\\\\'s take a look to the answer!\\\\n\\\\n### Strategy 2: Playground \\\\n\\\\nNow you learned how things work internally, let me show you how you can do this nice and easy, and with a nice UI on top. Using [Elastic Playground](https:\\/\\/www.elastic.co\\/search-labs\\/blog\\/rag-playground-introduction). \\\\n\\\\nGo to Playground, configure the Bedrock connector, and then select the index we just created and you are ready to go.\\\\n\\\\n![Writing an article about indexing the article](\\/assets\\/images\\/semantic-text-with-amazon-bedrock\\/playground_config_1.gif)\\\\n\\\\nFrom here you can start asking questions to your brand new index.\\\\n\\\\nPLAYGROUND IMAGE HERE \\\\n\\\\n## Conclusion \\\\n\\\\nThe new `semantic_text` mapping type makes creating a RAG setup extremely easy, without having to leave the Elastic ecosystem. Things like chunking and mapping settings are not a problem anymore, and there are various alternatives to ask questions to the data. \\\\n\\\\nAWS Bedrock is fully integrated by providing both embeddings and completions endpoints, and also being included as a Playground connector!.\"\n}",
									"options": {
										"raw": {
											"language": "json"
										}
									}
								},
								"url": {
									"raw": "{{ES_URL}}/semantic-text-bedrock/_doc",
									"host": [
										"{{ES_URL}}"
									],
									"path": [
										"semantic-text-bedrock",
										"_doc"
									]
								}
							},
							"response": []
						},
						{
							"name": "5. Search",
							"request": {
								"method": "POST",
								"header": [],
								"body": {
									"mode": "raw",
									"raw": "{\n\"_source\": false,\n  \"retriever\": {\n    \"standard\": {\n      \"query\": {\n        \"nested\": {\n          \"path\": \"super_body.inference.chunks\",\n          \"query\": {\n            \"knn\": {\n              \"field\": \"super_body.inference.chunks.embeddings\",\n              \"query_vector_builder\": {\n                \"text_embedding\": {\n                  \"model_id\": \"bedrock-embeddings\",\n                  \"model_text\": \"what's the cat thing about?\"\n                }\n              }\n            }\n          },\n          \"inner_hits\": {\n            \"size\": 5,\n            \"name\": \"semantic-text-bedrock.super_body\",\n            \"_source\": \"*.text\"\n          }\n        }\n      }\n    }\n  }\n}",
									"options": {
										"raw": {
											"language": "json"
										}
									}
								},
								"url": {
									"raw": "{{ES_URL}}/semantic-text-bedrock/_search",
									"host": [
										"{{ES_URL}}"
									],
									"path": [
										"semantic-text-bedrock",
										"_search"
									]
								}
							},
							"response": []
						},
						{
							"name": "6. Inference completion",
							"request": {
								"method": "POST",
								"header": [],
								"body": {
									"mode": "raw",
									"raw": "{\n    \"input\": \"Answer the question _what's the cat thing about?_ , based on the following context \\n ---\\\\ntitle: \\\\\\\"semantic_text with Amazon Bedrock\\\\\\\"\\\\nslug: \\\\\\\"semantic-text-with-amazon-bedrock\\\\\\\"\\\\ndate: \\\\\\\"2024-07-03\\\\\\\"\\\\ndescription: \\\\\\\"Using semantic_text new feature, and Bedrock as inference endpoint service\\\\\\\"\\\\nauthor:\\\\n  - slug: gustavo-llermaly\\\\nimage: \\\\\\\"headshot-gustavo-llermaly.jpeg\\\\\\\"\\\\ncategory:\\\\n  - slug: integrations\\\\n  - slug: how-to\\\\ntags:\\\\n  - slug: stack\\\\n  - slug: search\\\\n---\\\\n\\\\n## Introduction\\\\n\\\\nOne of the biggest challenges on RAG systems is chunking data. \\\\nDeciding which settings to use, and how to actually generate the chunks required developing additional code or using frameworks like LangChain or Llamaindex.\\\\n\\\\nFew months ago, we provided a way to [chunk documents using ingest pipelines](https:\\/\\/www.elastic.co\\/search-labs\\/blog\\/chunking-via-ingest-pipelines) , leveraging the recent addition of [nested vector fields](https:\\/\\/www.elastic.co\\/search-labs\\/blog\\/multi-vector-relevance).\\\\n\\\\nWith the addition of the [semantic_text mapping type](https:\\/\\/www.elastic.co\\/search-labs\\/blog\\/semantic-search-simplified-semantic-text) the process es even simpler!.\\\\n\\\\nOn this article, we are going to create an end-to-end RAG application without leaving Elastic, and using Bedrock as our inference service.\\\\n\\\\n### Steps \\\\n\\\\n1. Creating Endpoints \\\\n2. Creating mappings\\\\n3. Indexing data\\\\n5. Asking questions\\\\n\\\\n## Creating Endpoints \\\\n\\\\nBefore creating our index, we must create the endpoints we are going to use for our inference tasks: \\\\n\\\\n1. Embeddings Task\\\\n2. Completions Task\\\\n\\\\nWe will use Bedrock as our provider for both of them. With this two endpoints we can create a full [RAG](https:\\/\\/www.elastic.co\\/search-labs\\/blog\\/retrieval-augmented-generation-rag) application only using Elastic tools!.\\\\n\\\\nIf you want to read more about how to configure Bedrock I recommend you reading [this article](https:\\/\\/www.elastic.co\\/search-labs\\/blog\\/use-amazon-bedrock-with-elasticsearch-and-langchain) first.\\\\n\\\\n### Embeddings Task \\\\n\\\\nThis task will help us creating [vector embeddings](https:\\/\\/www.elastic.co\\/search-labs\\/tutorials\\/search-tutorial\\/vector-search\\/embeddings-intro) for our documents content, and for the question the user will ask. \\\\n\\\\nWith this vectors we can find the most relevant chunks to the question and retrieve the documents that contain the answer.\\\\n\\\\nGo ahead and run in [Kibana DevTools Console](https:\\/\\/www.elastic.co\\/guide\\/en\\/kibana\\/current\\/console-kibana.html) to create the endpoint: \\\\n\\\\n```json\\\\nPUT _inference\\/text_embedding\\/bedrock-embeddings\\\\n {\\\\n    \\\\\\\"service\\\\\\\": \\\\\\\"amazonbedrock\\\\\\\",\\\\n    \\\\\\\"service_settings\\\\\\\": {\\\\n        \\\\\\\"access_key\\\\\\\": \\\\\\\"{AWS_ACCESS_KEY}\\\\\\\",\\\\n        \\\\\\\"secret_key\\\\\\\": \\\\\\\"{AWS_SECRET_KEY}\\\\\\\",\\\\n        \\\\\\\"region\\\\\\\": \\\\\\\"{AWS_REGION}\\\\\\\",\\\\n        \\\\\\\"provider\\\\\\\": \\\\\\\"cohere\\\\\\\",\\\\n        \\\\\\\"model\\\\\\\": \\\\\\\"cohere.embed-english-v3\\\\\\\"\\\\n    }\\\\n}\\\\n```\\\\n\\\\n- _`provider` must be one of `amazontitan, cohere`_\\\\n- _`model` must be one _model_id_ [you have access](https:\\/\\/docs.aws.amazon.com\\/bedrock\\/latest\\/userguide\\/model-ids.html) to in Bedrock_\\\\n\\\\nOptional additional settings\\\\n-   `dimensions`: The output dimensions to use for the inference\\\\n-   `max_input_tokens`: the maximum number of input tokens\\\\n-   `similarity`: the similarity measure to use\\\\n\\\\n### Completions Task\\\\n\\\\nAfter we find the best chunks, we must send them to the LLM model so it can generate an answer for us. \\\\n\\\\nRun the following to add the completions endpoint: \\\\n\\\\n```json\\\\nPUT _inference\\/completion\\/bedrock-completions\\\\n{\\\\n    \\\\\\\"service\\\\\\\": \\\\\\\"amazonbedrock\\\\\\\",\\\\n    \\\\\\\"service_settings\\\\\\\": {\\\\n        \\\\\\\"access_key\\\\\\\": \\\\\\\"{AWS_ACCESS_KEY}\\\\\\\",\\\\n        \\\\\\\"secret_key\\\\\\\": \\\\\\\"{AWS_SECRET_KEY}\\\\\\\",\\\\n        \\\\\\\"region\\\\\\\": \\\\\\\"{AWS_REGION}\\\\\\\",\\\\n        \\\\\\\"model\\\\\\\": \\\\\\\"anthropic.claude-3-haiku-20240307-v1:0\\\\\\\",\\\\n        \\\\\\\"provider\\\\\\\": \\\\\\\"anthropic\\\\\\\",\\\\n    }\\\\n}\\\\n```\\\\n- _`provider` must be one of `amazontitan, anthropic, ai21labs, cohere, meta, mistral`_\\\\n- _`model` must be one _model_id_ or ARN [you have access](https:\\/\\/docs.aws.amazon.com\\/bedrock\\/latest\\/userguide\\/model-ids.html) to in Bedrock_\\\\n\\\\n## Creating Mappings \\\\n\\\\nThe new [semantic_text](https:\\/\\/www.elastic.co\\/search-labs\\/blog\\/semantic-search-simplified-semantic-text)  mapping type will make things super easy. It will take care of infering the embedding mappings and configurations, and doing the passage chunking for you! If you want to read more you can go to this great [article](https:\\/\\/www.elastic.co\\/search-labs\\/blog\\/semantic-search-simplified-semantic-text). \\\\n\\\\n```json\\\\nPUT semantic-text-bedrock\\\\n{\\\\n  \\\\\\\"mappings\\\\\\\": {\\\\n    \\\\\\\"properties\\\\\\\": {\\\\n      \\\\\\\"super_body\\\\\\\": {\\\\n        \\\\\\\"type\\\\\\\": \\\\\\\"semantic_text\\\\\\\",\\\\n        \\\\\\\"inference_id\\\\\\\": \\\\\\\"bedrock-embeddings\\\\\\\"\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\n```\\\\nYES. That\\\\'s it. `super_body` is ready to be searched with vectors, and to handle chunking.\\\\n\\\\n## Indexing data\\\\n\\\\nFor data indexing we have many [methods available](https:\\/\\/www.elastic.co\\/search-labs\\/blog\\/ES-data-ingestion), you can pick any of your preference. \\\\n\\\\nFor simplicity, and *recursivity*, I will just copy this whole article and store it as a document with many chunks.\\\\n\\\\n![a cat looking into a screen that displays a live feed of the same cat, creating an infinite loop effect.](\\/assets\\/images\\/semantic-text-with-amazon-bedrock\\/recursive_cat.gif)\\\\n\\\\n```json\\\\nPOST semantic-text-bedrock\\/_doc\\\\n{\\\\n  \\\\\\\"super_body\\\\\\\": \\\\\\\"## Introduction \\\\n    One of the biggest challenges on RAG systems is chunking data. \\\\n    Deciding which settings to use, and how to actually generate the chunks required developing additional code or using frameworks like LangChain or Llamaindex.\\\\n  \\\\\\\"\\\\n}\\\\n```\\\\n*Triple quotes are very useful to add long paragraphs to Kibana Console because it handles <a href=\\\\\\\"https:\\/\\/en.wikipedia.org\\/wiki\\/Escape_sequence\\\\\\\" target=\\\\\\\"_blank\\\\\\\">text escaping<\\/a>.*\\\\n\\\\nWe have it. Time to test.\\\\n\\\\n## Asking questions\\\\n\\\\nThe question and answer is a two steps process. First we must retrieve the relevant text chunks to the question, and then we must send the chunks to the LLM to generate the answer. \\\\n\\\\nWe will explore two strategies to do that, as promised, without any additional code or framework.\\\\n\\\\n### Strategy 1: API Calls \\\\n\\\\nWe can run two API calls, one to the `_search` endpoint to retrieve the chunks, and another one to the `inference` endpoint to do the LLM completion step. \\\\n\\\\n#### Retrieving chunks \\\\n\\\\nWe are going to try a sort of \\\\\\\"needle in the haystack\\\\\\\" query, to make sure the answer from the LLM is obtained from this article, and not from the LLM base knowledge. We are going to ask about the cat gif referring to the recursivity of this article.\\\\n\\\\nWe could run the nice and short default query for semantic-text: \\\\n\\\\n```json\\\\nGET semantic-text-bedrock\\/_search\\\\n{\\\\n  \\\\\\\"query\\\\\\\": {\\\\n    \\\\\\\"semantic\\\\\\\": {\\\\n      \\\\\\\"field\\\\\\\": \\\\\\\"super_body\\\\\\\",\\\\n      \\\\\\\"query\\\\\\\": \\\\\\\"what\\\\'s the cat gif thing about?\\\\\\\"\\\\n    }\\\\n  }\\\\n}\\\\n```\\\\n\\\\nThe problem is this query will not sort the inner hits (chunks) by relevance, which is what we need for this use case. It will sort the documents relevance *per document*, and not *per chunk*. \\\\n\\\\nThis longer query will sort inner hits (chunks) by relevance, so we can grab the juicy ones. \\\\n\\\\n```json\\\\nGET semantic-text-bedrock\\/_search\\\\n{\\\\n  \\\\\\\"_source\\\\\\\": false,\\\\n  \\\\\\\"retriever\\\\\\\": {\\\\n    \\\\\\\"standard\\\\\\\": {\\\\n      \\\\\\\"query\\\\\\\": {\\\\n        \\\\\\\"nested\\\\\\\": {\\\\n          \\\\\\\"path\\\\\\\": \\\\\\\"super_body.inference.chunks\\\\\\\",\\\\n          \\\\\\\"query\\\\\\\": {\\\\n            \\\\\\\"knn\\\\\\\": {\\\\n              \\\\\\\"field\\\\\\\": \\\\\\\"super_body.inference.chunks.embeddings\\\\\\\",\\\\n              \\\\\\\"query_vector_builder\\\\\\\": {\\\\n                \\\\\\\"text_embedding\\\\\\\": {\\\\n                  \\\\\\\"model_id\\\\\\\": \\\\\\\"bedrock-embeddings\\\\\\\",\\\\n                  \\\\\\\"model_text\\\\\\\": \\\\\\\"what\\\\'s the cat gif thing about?\\\\\\\"\\\\n                }\\\\n              }\\\\n            }\\\\n          },\\\\n          \\\\\\\"inner_hits\\\\\\\": {\\\\n            \\\\\\\"size\\\\\\\": 1,\\\\n            \\\\\\\"name\\\\\\\": \\\\\\\"semantic-text-bedrock.super_body\\\\\\\",\\\\n            \\\\\\\"_source\\\\\\\": \\\\\\\"*.text\\\\\\\"\\\\n          }\\\\n        }\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\n```\\\\n_we set root level `_source` to false, because we are interested on the relevant chunks only_\\\\n\\\\nAs you can see, we are using [retrievers](https:\\/\\/www.elastic.co\\/search-labs\\/blog\\/elasticsearch-retrievers) for this query, and the response looks like this: \\\\n\\\\nRESPONSE PICTURE \\\\n\\\\nNow from the response we can copy the top chunk, and combine the text in one big string. What some frameworks do, is to add metadata to each of the chunks.\\\\n\\\\n#### Answering the question \\\\n\\\\nNow we can use the bedrock completions endpoint we created previously to send this questions along with the relevant chunks and get the answer. \\\\n\\\\n```json\\\\nPOST _inference\\/completion\\/bedrock-completions\\\\n{\\\\n    \\\\\\\"input\\\\\\\": \\\\\\\"Answer the question _what\\\\'s the cat gif thing about?_ , based on the following context \\\\\\\\n <paste the relevant chunks here>\\\\\\\"\\\\n}\\\\n```\\\\nLet\\\\'s take a look to the answer!\\\\n\\\\n### Strategy 2: Playground \\\\n\\\\nNow you learned how things work internally, let me show you how you can do this nice and easy, and with a nice UI on top. Using [Elastic Playground](https:\\/\\/www.elastic.co\\/search-labs\\/blog\\/rag-playground-introduction). \\\\n\\\\nGo to Playground, configure the Bedrock connector, and then select the index we just created and you are ready to go.\\\\n\\\\n![Writing an article about indexing the article](\\/assets\\/images\\/semantic-text-with-amazon-bedrock\\/playground_config_1.gif)\\\\n\\\\nFrom here you can start asking questions to your brand new index.\\\\n\\\\nPLAYGROUND IMAGE HERE \\\\n\\\\n## Conclusion \\\\n\\\\nThe new `semantic_text` mapping type makes creating a RAG setup extremely easy, without having to leave the Elastic ecosystem. Things like chunking and mapping settings are not a problem anymore, and there are various alternatives to ask questions to the data. \\\\n\\\\nAWS Bedrock is fully integrated by providing both embeddings and completions endpoints, and also being included as a Playground connector!.\"\n}",
									"options": {
										"raw": {
											"language": "json"
										}
									}
								},
								"url": {
									"raw": "{{ES_URL}}/_inference/completion/bedrock-completion",
									"host": [
										"{{ES_URL}}"
									],
									"path": [
										"_inference",
										"completion",
										"bedrock-completion"
									]
								}
							},
							"response": []
						}
					]
				},
				{
					"name": "Cleanup",
					"item": [
						{
							"name": "Delete Index",
							"request": {
								"method": "DELETE",
								"header": [],
								"url": {
									"raw": "{{ES_URL}}/semantic-text-bedrock",
									"host": [
										"{{ES_URL}}"
									],
									"path": [
										"semantic-text-bedrock"
									]
								}
							},
							"response": []
						},
						{
							"name": "Delete Completions",
							"request": {
								"method": "DELETE",
								"header": [],
								"url": {
									"raw": "{{ES_URL}}/_inference/completion/bedrock-completion",
									"host": [
										"{{ES_URL}}"
									],
									"path": [
										"_inference",
										"completion",
										"bedrock-completion"
									]
								}
							},
							"response": []
						},
						{
							"name": "Delete Embeddings Endpoint",
							"request": {
								"method": "DELETE",
								"header": [],
								"url": {
									"raw": "{{ES_URL}}/_inference/text_embedding/bedrock-embeddings",
									"host": [
										"{{ES_URL}}"
									],
									"path": [
										"_inference",
										"text_embedding",
										"bedrock-embeddings"
									]
								}
							},
							"response": []
						}
					]
				}
			]
		},
		{
			"name": "Health Check",
			"request": {
				"method": "GET",
				"header": [],
				"url": {
					"raw": "{{ES_URL}}",
					"host": [
						"{{ES_URL}}"
					]
				}
			},
			"response": []
		}
	],
	"auth": {
		"type": "basic",
		"basic": [
			{
				"key": "username",
				"value": "{{ES_USER}}",
				"type": "string"
			},
			{
				"key": "password",
				"value": "{{ES_PASSWORD}}",
				"type": "string"
			}
		]
	},
	"event": [
		{
			"listen": "prerequest",
			"script": {
				"type": "text/javascript",
				"packages": {},
				"exec": [
					""
				]
			}
		},
		{
			"listen": "test",
			"script": {
				"type": "text/javascript",
				"packages": {},
				"exec": [
					""
				]
			}
		}
	],
	"variable": [
		{
			"key": "ES_URL",
			"value": "",
			"type": "string"
		},
		{
			"key": "ES_USER",
			"value": "",
			"type": "string"
		},
		{
			"key": "ES_PASSWORD",
			"value": "",
			"type": "string"
		},
		{
			"key": "AWS_ACCESS_KEY",
			"value": "",
			"type": "string"
		},
		{
			"key": "AWS_SECRET_KEY",
			"value": "",
			"type": "string"
		}
	]
}