{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "C3EFzwK9YbaP",
   "metadata": {
    "id": "C3EFzwK9YbaP"
   },
   "source": [
    "# Using Elasticsearch Inference API along Hugging Face models\n",
    "\n",
    "This notebook demonstrates how to use the Elasticsearch Inference API along with Hugging Face models to build a question and answer system. This notebook is based on the [Using Elasticsearch Inference API along Hugging Face models](https://www.elastic.co/search-labs/blog/elasticsearch-inference-api-and-hugging-face)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb30f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74bb30f2",
    "outputId": "185bf00e-67fb-4504-e56c-30f8c8872f83"
   },
   "outputs": [],
   "source": [
    "%pip install requests elasticsearch dotenv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TgEea48gYnp5",
   "metadata": {
    "id": "TgEea48gYnp5"
   },
   "source": [
    "## Installing dependencies and importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4668548",
   "metadata": {
    "id": "b4668548"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import textwrap\n",
    "import re\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a30c9",
   "metadata": {},
   "source": [
    "## Setting up environment variables\n",
    "\n",
    "Configure API keys and URLs for Elasticsearch and Hugging Face, along with the index name and inference endpoint identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aab023",
   "metadata": {
    "id": "01aab023"
   },
   "outputs": [],
   "source": [
    "ELASTICSEARCH_API_KEY = os.getenv(\"ELASTICSEARCH_API_KEY\")\n",
    "ELASTICSEARCH_URL = os.getenv(\"ELASTICSEARCH_URL\")\n",
    "HUGGING_FACE_API_KEY = os.getenv(\"HUGGING_FACE_API_KEY\")\n",
    "HUGGING_FACE_INFERENCE_ENDPOINT_URL = os.getenv(\"HUGGING_FACE_INFERENCE_ENDPOINT_URL\")\n",
    "\n",
    "\n",
    "INDEX_NAME = \"blog-posts\"\n",
    "INFERENCE_ENDPOINT_ID = \"hugging-face-smollm3-3b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tjbXck_gc9lY",
   "metadata": {
    "id": "tjbXck_gc9lY"
   },
   "source": [
    "## Elasticsearch Python client\n",
    "\n",
    "Initialize the Elasticsearch client using the configured URL and API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf922ab",
   "metadata": {
    "id": "8cf922ab"
   },
   "outputs": [],
   "source": [
    "es_client = Elasticsearch(ELASTICSEARCH_URL, api_key=ELASTICSEARCH_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832186f0",
   "metadata": {},
   "source": [
    "## Hugging Face completions inference endpoint setup\n",
    "\n",
    "Create an Elasticsearch inference endpoint that connects to the Hugging Face model for generating responses based on blog articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tldq3os7m8o",
   "metadata": {},
   "source": [
    "> **NOTE:** Check your Hugging Face endpoint for any resource unavailable issues. If you encounter a `resource_already_exists_exception`, run the [cleanup](#Cleanup) code snippet below and re-run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eba1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    resp = es_client.inference.put(\n",
    "        task_type=\"chat_completion\",\n",
    "        inference_id=INFERENCE_ENDPOINT_ID,\n",
    "        body={\n",
    "            \"service\": \"hugging_face\",\n",
    "            \"service_settings\": {\n",
    "                \"api_key\": HUGGING_FACE_API_KEY,\n",
    "                \"url\": HUGGING_FACE_INFERENCE_ENDPOINT_URL,\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"Chat completion inference endpoint created successfully:\",\n",
    "        resp[\"inference_id\"],\n",
    "    )\n",
    "except Exception as e:\n",
    "    if hasattr(e, \"body\") and \"error\" in e.body:\n",
    "        error_info = e.body[\"error\"]\n",
    "        print(f\"Error: {error_info.get('reason', str(e))}\")\n",
    "\n",
    "        if \"caused_by\" in error_info:\n",
    "            print(f\"Caused by: {error_info['caused_by'].get('reason', '')}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Error creating chat completion inference endpoint: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21438e7e",
   "metadata": {},
   "source": [
    "### Creating index mapping\n",
    "\n",
    "Define field types and properties for the blog articles index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbe4d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mapping = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"id\": {\"type\": \"keyword\"},\n",
    "                \"title\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"original\": {\n",
    "                            \"type\": \"text\",\n",
    "                            \"copy_to\": \"semantic_field\",\n",
    "                            \"fields\": {\"keyword\": {\"type\": \"keyword\"}},\n",
    "                        },\n",
    "                        \"translated_title\": {\n",
    "                            \"type\": \"text\",\n",
    "                            \"fields\": {\"keyword\": {\"type\": \"keyword\"}},\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "                \"author\": {\"type\": \"keyword\", \"copy_to\": \"semantic_field\"},\n",
    "                \"category\": {\"type\": \"keyword\", \"copy_to\": \"semantic_field\"},\n",
    "                \"content\": {\"type\": \"text\", \"copy_to\": \"semantic_field\"},\n",
    "                \"date\": {\"type\": \"date\"},\n",
    "                \"semantic_field\": {\"type\": \"semantic_text\"},\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    es_client.indices.create(index=INDEX_NAME, body=mapping)\n",
    "    print(f\"Index {INDEX_NAME} created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating index: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd56355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(json_file, index_name):\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for doc in data:\n",
    "        action = {\"_index\": index_name, \"_source\": doc}\n",
    "        yield action\n",
    "\n",
    "\n",
    "try:\n",
    "    success, failed = helpers.bulk(\n",
    "        es_client,\n",
    "        build_data(\"dataset.json\", INDEX_NAME),\n",
    "    )\n",
    "    print(f\"{success} documents indexed successfully\")\n",
    "\n",
    "    if failed:\n",
    "        print(f\"Errors: {failed}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afce36a4",
   "metadata": {},
   "source": [
    "## Semantic search function\n",
    "\n",
    "Function to search for relevant articles using Elasticsearch semantic search capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99600e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_semantic_search(query_text, index_name=INDEX_NAME, size=5):\n",
    "    try:\n",
    "        query = {\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"semantic_field\": {\n",
    "                        \"query\": query_text,\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"size\": size,\n",
    "        }\n",
    "\n",
    "        response = es_client.search(index=index_name, body=query)\n",
    "        hits = response[\"hits\"][\"hits\"]\n",
    "\n",
    "        return hits\n",
    "    except Exception as e:\n",
    "        print(f\"Semantic search error: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44ce435",
   "metadata": {},
   "source": [
    "### Streaming function for real-time responses\n",
    "\n",
    "Send messages to the Elasticsearch inference endpoint with streaming support, processing server-sent events to extract model responses in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530b687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_chat_completion(messages: list, inference_id: str = INFERENCE_ENDPOINT_ID):\n",
    "    url = f\"{ELASTICSEARCH_URL}/_inference/chat_completion/{inference_id}/_stream\"\n",
    "    payload = {\"messages\": messages}\n",
    "    headers = {\n",
    "        \"Authorization\": f\"ApiKey {ELASTICSEARCH_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, headers=headers, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        for line in response.iter_lines(decode_unicode=True):\n",
    "            if line:\n",
    "                line = line.strip()\n",
    "\n",
    "                if line.startswith(\"event:\"):\n",
    "                    continue\n",
    "\n",
    "                if line.startswith(\"data: \"):\n",
    "                    data_content = line[6:]\n",
    "\n",
    "                    if not data_content.strip() or data_content.strip() == \"[DONE]\":\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        chunk_data = json.loads(data_content)\n",
    "\n",
    "                        if \"choices\" in chunk_data and len(chunk_data[\"choices\"]) > 0:\n",
    "                            choice = chunk_data[\"choices\"][0]\n",
    "                            if \"delta\" in choice and \"content\" in choice[\"delta\"]:\n",
    "                                content = choice[\"delta\"][\"content\"]\n",
    "                                if content:\n",
    "                                    yield content\n",
    "\n",
    "                    except json.JSONDecodeError as json_err:\n",
    "                        print(f\"\\nJSON decode error: {json_err}\")\n",
    "                        print(f\"Problematic data: {data_content}\")\n",
    "                        continue\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        yield f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc966da8",
   "metadata": {},
   "source": [
    "## Article recommendation system\n",
    "\n",
    "Method that calls the semantic search and the real time chat_completions for generating personalized article recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2efa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_articles(search_query, index_name=INDEX_NAME, max_articles=5):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üîç Search Query: {search_query}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    articles = perform_semantic_search(search_query, index_name, size=max_articles)\n",
    "\n",
    "    if not articles:\n",
    "        print(\"‚ùå No relevant articles found.\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"‚úÖ Found {len(articles)} relevant articles\\n\")\n",
    "\n",
    "    # Build context with found articles\n",
    "    context = \"Available blog articles:\\n\\n\"\n",
    "    for i, article in enumerate(articles, 1):\n",
    "        source = article.get(\"_source\", article)\n",
    "        context += f\"Article {i}:\\n\"\n",
    "        context += f\"- Title: {source.get('title', 'N/A')}\\n\"\n",
    "        context += f\"- Author: {source.get('author', 'N/A')}\\n\"\n",
    "        context += f\"- Category: {source.get('category', 'N/A')}\\n\"\n",
    "        context += f\"- Date: {source.get('date', 'N/A')}\\n\"\n",
    "        context += f\"- Content: {source.get('content', 'N/A')}\\n\\n\"\n",
    "\n",
    "    system_prompt = \"\"\"You are an expert content curator that recommends blog articles.\n",
    "\n",
    "    Write recommendations in a conversational style starting with phrases like:\n",
    "    - \"If you're interested in [topic], this article...\"\n",
    "    - \"This post complements your search with...\"\n",
    "    - \"For those looking into [topic], this article provides...\"\n",
    "\n",
    "\n",
    "    FORMAT REQUIREMENTS:\n",
    "    - Return ONLY a JSON array\n",
    "    - Each element must have EXACTLY these three fields: \"article_number\", \"title\", \"recommendation\"\n",
    "    - If the original title is in spanish, use the \"translated_title\" subfield in the \"title\" field\n",
    "\n",
    "    Keep each recommendation concise (2-3 sentences max) and focused on VALUE to the reader.\n",
    "\n",
    "    EXAMPLE OF CORRECT FORMAT:\n",
    "    [\n",
    "        {\"article_number\": 1, \"title\": \"Article title in english\", \"recommendation\": \"If you are interested in [topic], this article provides...\"},\n",
    "        {\"article_number\": 2, \"title\": \"Article title in english\", \"recommendation\": \" for those looking into [topic], this article provides...\"}\n",
    "    ]\n",
    "\n",
    "    Return ONLY the JSON array following this exact structure.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Search query: \"{search_query}\"\n",
    "\n",
    "    Generate recommendations for the following articles: {context}\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"/no_think\"},\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    # LLM generation\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"ü§ñ Generating personalized recommendations...\\n\")\n",
    "\n",
    "    full_response = \"\"\n",
    "\n",
    "    for chunk in stream_chat_completion(messages):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        full_response += chunk\n",
    "\n",
    "    return context, articles, full_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bab62f",
   "metadata": {},
   "source": [
    "### Card visualization helper\n",
    "\n",
    "Function to display recommendations in a card-style format for better visual differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2f4981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_recommendation_cards(articles, recommendations_text):\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"üìá RECOMMENDED ARTICLES\".center(100))\n",
    "    print(\"=\" * 100 + \"\\n\")\n",
    "\n",
    "    # Parse JSON recommendations - clean tags and extract JSON\n",
    "    recommendations_list = []\n",
    "    try:\n",
    "\n",
    "        # Clean up <think> tags\n",
    "        cleaned_text = re.sub(\n",
    "            r\"<think>.*?</think>\", \"\", recommendations_text, flags=re.DOTALL\n",
    "        )\n",
    "        # Remove markdown code blocks ( ... ``` or ``` ... ```)\n",
    "        cleaned_text = re.sub(r\"```(?:json)?\", \"\", cleaned_text)\n",
    "        cleaned_text = cleaned_text.strip()\n",
    "\n",
    "        parsed = json.loads(cleaned_text)\n",
    "\n",
    "        # Extract recommendations from list format\n",
    "        for item in parsed:\n",
    "            article_number = item.get(\"article_number\")\n",
    "            title = item.get(\"title\", \"\")\n",
    "            rec_text = item.get(\"recommendation\", \"\")\n",
    "\n",
    "            if article_number and rec_text:\n",
    "                recommendations_list.append(\n",
    "                    {\n",
    "                        \"article_number\": article_number,\n",
    "                        \"title\": title,\n",
    "                        \"recommendation\": rec_text,\n",
    "                    }\n",
    "                )\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not parse recommendations as JSON: {e}\")\n",
    "        return\n",
    "\n",
    "    for i, article in enumerate(articles, 1):\n",
    "        source = article.get(\"_source\", article)\n",
    "\n",
    "        # Card border\n",
    "        print(\"‚îå\" + \"‚îÄ\" * 98 + \"‚îê\")\n",
    "\n",
    "        # Find recommendation and title for this article number\n",
    "        recommendation = None\n",
    "        title = None\n",
    "        for rec in recommendations_list:\n",
    "            if rec.get(\"article_number\") == i:\n",
    "                recommendation = rec.get(\"recommendation\")\n",
    "                title = rec.get(\"title\")\n",
    "                break\n",
    "\n",
    "        # Print title\n",
    "        title_lines = textwrap.wrap(f\"üìå {title}\", width=94)\n",
    "        for line in title_lines:\n",
    "            print(f\"‚îÇ  {line}\".ljust(99) + \"‚îÇ\")\n",
    "\n",
    "        # Card border\n",
    "        print(\"‚îú\" + \"‚îÄ\" * 98 + \"‚î§\")\n",
    "\n",
    "        # Print recommendation\n",
    "        if recommendation:\n",
    "            recommendation_lines = textwrap.wrap(recommendation, width=94)\n",
    "            for line in recommendation_lines:\n",
    "                print(f\"‚îÇ  {line}\".ljust(99) + \"‚îÇ\")\n",
    "\n",
    "        # Card bottom\n",
    "        print(\"‚îî\" + \"‚îÄ\" * 98 + \"‚îò\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d9ea51",
   "metadata": {},
   "source": [
    "## Testing the recommendation system\n",
    "\n",
    "Testing the recommendation system using a search query in Spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce48fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = \"Security and vulnerabilities\"\n",
    "\n",
    "context, articles, recommendations = recommend_articles(search_query)\n",
    "\n",
    "print(\"\\nElasticsearch context:\\n\", context)\n",
    "\n",
    "# Display visual cards\n",
    "display_recommendation_cards(articles, recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbeb64b",
   "metadata": {},
   "source": [
    "`ask_question_streaming` function to put together the semantic search and the real time chat_completions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C2WKnj8UZa7g",
   "metadata": {
    "id": "C2WKnj8UZa7g"
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Delete the index and inference endpoints to prevent consuming resources after completing the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TgqFHEhPZfAd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TgqFHEhPZfAd",
    "outputId": "675e161b-f7e3-43f8-d3cc-f9e7fa72f6aa"
   },
   "outputs": [],
   "source": [
    "# Cleanup - Delete Index\n",
    "es_client.indices.delete(index=INDEX_NAME)\n",
    "print(f\"Index {INDEX_NAME} deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc72db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup - Delete Inference Endpoint\n",
    "es_client.inference.delete(inference_id=INFERENCE_ENDPOINT_ID)\n",
    "print(f\"Inference endpoint {INFERENCE_ENDPOINT_ID} deleted\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
