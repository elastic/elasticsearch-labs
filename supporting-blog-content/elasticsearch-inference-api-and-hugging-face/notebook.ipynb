{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "C3EFzwK9YbaP",
      "metadata": {
        "id": "C3EFzwK9YbaP"
      },
      "source": [
        "# Using Elasticsearch Inference API along Hugging Face models\n",
        "\n",
        "This notebook demonstrates how to use the Elasticsearch Inference API along with Hugging Face models to build a question and answer system. This notebook is based on the [Using Elasticsearch Inference API along Hugging Face models](https://www.elastic.co/search-labs/blog/elasticsearch-inference-api-and-hugging-face)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74bb30f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74bb30f2",
        "outputId": "185bf00e-67fb-4504-e56c-30f8c8872f83"
      },
      "outputs": [],
      "source": [
        "%pip install requests elasticsearch -q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TgEea48gYnp5",
      "metadata": {
        "id": "TgEea48gYnp5"
      },
      "source": [
        "## Installing dependencies and importing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4668548",
      "metadata": {
        "id": "b4668548"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "import textwrap\n",
        "import re\n",
        "import time\n",
        "\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "from elasticsearch import Elasticsearch, helpers\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd7a30c9",
      "metadata": {},
      "source": [
        "## Setting up environment variables\n",
        "\n",
        "Configure API keys and URLs for Elasticsearch and Hugging Face, along with the index name and inference endpoint identifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "01aab023",
      "metadata": {
        "id": "01aab023"
      },
      "outputs": [],
      "source": [
        "ELASTICSEARCH_API_KEY = os.getenv(\"ELASTICSEARCH_API_KEY\")\n",
        "ELASTICSEARCH_URL = os.getenv(\"ELASTICSEARCH_URL\")\n",
        "HUGGING_FACE_API_KEY = os.getenv(\"HUGGING_FACE_API_KEY\")\n",
        "HUGGING_FACE_INFERENCE_ENDPOINT_URL = os.getenv(\"HUGGING_FACE_INFERENCE_ENDPOINT_URL\")\n",
        "\n",
        "\n",
        "INDEX_NAME = \"blog-posts\"\n",
        "INFERENCE_ENDPOINT_ID = \"hugging-face-smollm3-3b\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tjbXck_gc9lY",
      "metadata": {
        "id": "tjbXck_gc9lY"
      },
      "source": [
        "## Elasticsearch Python client\n",
        "\n",
        "Initialize the Elasticsearch client using the configured URL and API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "8cf922ab",
      "metadata": {
        "id": "8cf922ab"
      },
      "outputs": [],
      "source": [
        "es_client = Elasticsearch(ELASTICSEARCH_URL, api_key=ELASTICSEARCH_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "832186f0",
      "metadata": {},
      "source": [
        "## Hugging Face completions inference endpoint setup\n",
        "\n",
        "Create an Elasticsearch inference endpoint that connects to the Hugging Face model for generating responses based on blog articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40eba1a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    resp = es_client.inference.put(\n",
        "        task_type=\"chat_completion\",\n",
        "        inference_id=INFERENCE_ENDPOINT_ID,\n",
        "        body={\n",
        "            \"service\": \"hugging_face\",\n",
        "            \"service_settings\": {\n",
        "                \"api_key\": HUGGING_FACE_API_KEY,\n",
        "                \"url\": HUGGING_FACE_INFERENCE_ENDPOINT_URL,\n",
        "            },\n",
        "        },\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        \"Chat completion inference endpoint created successfully:\",\n",
        "        resp[\"inference_id\"],\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"Error creating chat completion inference endpoint:\", {e})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21438e7e",
      "metadata": {},
      "source": [
        "### Creating index mapping\n",
        "\n",
        "Define field types and properties for the blog articles index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "2fbe4d1c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index blog-posts created successfully\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    mapping = {\n",
        "        \"mappings\": {\n",
        "            \"properties\": {\n",
        "                \"id\": {\"type\": \"keyword\"},\n",
        "                \"title\": {\n",
        "                    \"type\": \"text\",\n",
        "                    \"copy_to\": \"semantic_field\",\n",
        "                    \"fields\": {\"keyword\": {\"type\": \"keyword\"}},\n",
        "                },\n",
        "                \"author\": {\"type\": \"keyword\", \"copy_to\": \"semantic_field\"},\n",
        "                \"category\": {\"type\": \"keyword\", \"copy_to\": \"semantic_field\"},\n",
        "                \"content\": {\"type\": \"text\", \"copy_to\": \"semantic_field\"},\n",
        "                \"date\": {\"type\": \"date\"},\n",
        "                \"semantic_field\": {\"type\": \"semantic_text\"},\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    es_client.indices.create(index=INDEX_NAME, body=mapping)\n",
        "    print(f\"Index {INDEX_NAME} created successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating index: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dd56355",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_data(json_file, index_name):\n",
        "    with open(json_file, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    for doc in data:\n",
        "        action = {\"_index\": index_name, \"_source\": doc}\n",
        "        yield action\n",
        "\n",
        "\n",
        "try:\n",
        "    success, failed = helpers.bulk(\n",
        "        es_client,\n",
        "        build_data(\"dataset.json\", INDEX_NAME),\n",
        "    )\n",
        "    print(f\"{success} documents indexed successfully\")\n",
        "\n",
        "    if failed:\n",
        "        print(f\"Errors: {failed}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afce36a4",
      "metadata": {},
      "source": [
        "## Semantic search function\n",
        "\n",
        "Function to search for relevant articles using Elasticsearch semantic search capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "e99600e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_semantic_search(query_text, index_name=INDEX_NAME, size=5):\n",
        "    try:\n",
        "        query = {\n",
        "            \"query\": {\n",
        "                \"semantic\": {\n",
        "                    \"field\": \"semantic_field\",\n",
        "                    \"query\": query_text,\n",
        "                }\n",
        "            },\n",
        "            \"size\": size,\n",
        "        }\n",
        "\n",
        "        response = es_client.search(index=index_name, body=query)\n",
        "        hits = response[\"hits\"][\"hits\"]\n",
        "\n",
        "        return hits\n",
        "    except Exception as e:\n",
        "        print(f\"Semantic search error: {str(e)}\")\n",
        "        return []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b44ce435",
      "metadata": {},
      "source": [
        "### Streaming function for real-time responses\n",
        "\n",
        "Send messages to the Elasticsearch inference endpoint with streaming support, processing server-sent events to extract model responses in real-time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "530b687e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def stream_chat_completion(messages: list, inference_id: str = INFERENCE_ENDPOINT_ID):\n",
        "    url = f\"{ELASTICSEARCH_URL}/_inference/chat_completion/{inference_id}/_stream\"\n",
        "    payload = {\"messages\": messages}\n",
        "    headers = {\n",
        "        \"Authorization\": f\"ApiKey {ELASTICSEARCH_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, json=payload, headers=headers, stream=True)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        for line in response.iter_lines(decode_unicode=True):\n",
        "            if line:\n",
        "                line = line.strip()\n",
        "\n",
        "                if line.startswith(\"event:\"):\n",
        "                    continue\n",
        "\n",
        "                if line.startswith(\"data: \"):\n",
        "                    data_content = line[6:]\n",
        "\n",
        "                    if not data_content.strip() or data_content.strip() == \"[DONE]\":\n",
        "                        continue\n",
        "\n",
        "                    try:\n",
        "                        chunk_data = json.loads(data_content)\n",
        "\n",
        "                        if \"choices\" in chunk_data and len(chunk_data[\"choices\"]) > 0:\n",
        "                            choice = chunk_data[\"choices\"][0]\n",
        "                            if \"delta\" in choice and \"content\" in choice[\"delta\"]:\n",
        "                                content = choice[\"delta\"][\"content\"]\n",
        "                                if content:\n",
        "                                    yield content\n",
        "\n",
        "                    except json.JSONDecodeError as json_err:\n",
        "                        print(f\"\\nJSON decode error: {json_err}\")\n",
        "                        print(f\"Problematic data: {data_content}\")\n",
        "                        continue\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        yield f\"Error: {str(e)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc966da8",
      "metadata": {},
      "source": [
        "## Article recommendation system\n",
        "\n",
        "Method that calls the semantic search and the real time chat_completions for generating personalized article recommendations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b2efa3d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def recommend_articles(search_query, index_name=INDEX_NAME, max_articles=5):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ğŸ” Search Query: {search_query}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    articles = perform_semantic_search(search_query, index_name, size=max_articles)\n",
        "\n",
        "    if not articles:\n",
        "        print(\"âŒ No relevant articles found.\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"âœ… Found {len(articles)} relevant articles\\n\")\n",
        "\n",
        "    # Build context with found articles\n",
        "    context = \"Available blog articles:\\n\\n\"\n",
        "    for i, article in enumerate(articles, 1):\n",
        "        source = article.get(\"_source\", article)\n",
        "        context += f\"Article {i}:\\n\"\n",
        "        context += f\"- Title: {source.get('title', 'N/A')}\\n\"\n",
        "        context += f\"- Author: {source.get('author', 'N/A')}\\n\"\n",
        "        context += f\"- Category: {source.get('category', 'N/A')}\\n\"\n",
        "        context += f\"- Date: {source.get('date', 'N/A')}\\n\"\n",
        "        context += f\"- Content: {source.get('content', 'N/A')}\\n\\n\"\n",
        "\n",
        "    # Simplified prompt that requests JSON in text\n",
        "    # Simplified prompt that requests JSON in text\n",
        "    system_prompt = \"\"\"You are an expert content curator that recommends blog articles.\n",
        "\n",
        "    Write recommendations in a conversational style starting with phrases like:\n",
        "    - \"If you're interested in [topic], this article...\"\n",
        "    - \"This post complements your search with...\"\n",
        "    - \"For those looking into [topic], this article provides...\"\n",
        "\n",
        "    Keep each recommendation concise (2-3 sentences max) and focused on VALUE to the reader.\n",
        "\n",
        "    Return ONLY a valid JSON object with this exact structure:\n",
        "    {\n",
        "        \"recommendations\": [\n",
        "            {\"article_number\": 1, \"recommendation\": \"your recommendation text\"},\n",
        "            {\"article_number\": 2, \"recommendation\": \"your recommendation text\"},\n",
        "            ... (continue for ALL articles)\n",
        "        ]\n",
        "    }\"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"Search query: \"{search_query}\"\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Generate a JSON object with one recommendation for EACH of the {len(articles)} articles above.\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"/no_think\"},\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "\n",
        "    # LLM generation\n",
        "    print(f\"{'='*80}\")\n",
        "    print(\"ğŸ¤– Generating personalized recommendations...\\n\")\n",
        "\n",
        "    full_response = \"\"\n",
        "\n",
        "    for chunk in stream_chat_completion(messages):\n",
        "        print(chunk, end=\"\", flush=True)\n",
        "        full_response += chunk\n",
        "\n",
        "    return articles, full_response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31bab62f",
      "metadata": {},
      "source": [
        "### Card visualization helper\n",
        "\n",
        "Function to display recommendations in a card-style format for better visual differentiation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "cb2f4981",
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_recommendation_cards(articles, recommendations_text):\n",
        "    print(\"\\n\" + \"=\" * 100)\n",
        "    print(\"ğŸ“‡ RECOMMENDED ARTICLES\".center(100))\n",
        "    print(\"=\" * 100 + \"\\n\")\n",
        "\n",
        "    # Parse JSON recommendations - clean tags and extract JSON\n",
        "    recommendations_list = []\n",
        "    if recommendations_text:\n",
        "        try:\n",
        "            # Clean up <think> tags\n",
        "            cleaned_text = re.sub(\n",
        "                r\"<think>.*?</think>\", \"\", recommendations_text, flags=re.DOTALL\n",
        "            )\n",
        "            # Remove markdown code blocks ( ... ``` or ``` ... ```)\n",
        "            cleaned_text = re.sub(r\"```(?:json)?\", \"\", cleaned_text)\n",
        "            cleaned_text = cleaned_text.strip()\n",
        "\n",
        "            parsed = json.loads(cleaned_text)\n",
        "            recommendations_list = parsed.get(\"recommendations\", [])\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"âš ï¸  Could not parse recommendations as JSON: {e}\")\n",
        "            return\n",
        "\n",
        "    for i, article in enumerate(articles, 1):\n",
        "        source = article.get(\"_source\", article)\n",
        "\n",
        "        # Card border\n",
        "        print(\"â”Œ\" + \"â”€\" * 98 + \"â”\")\n",
        "\n",
        "        # Title\n",
        "        title = source.get(\"title\", \"N/A\")\n",
        "        title_lines = textwrap.wrap(f\"ğŸ“Œ {title}\", width=94)\n",
        "        for line in title_lines:\n",
        "            print(f\"â”‚  {line}\".ljust(99) + \"â”‚\")\n",
        "\n",
        "        # Card border\n",
        "        print(\"â”œ\" + \"â”€\" * 98 + \"â”¤\")\n",
        "\n",
        "        # Find recommendation for this article number\n",
        "        recommendation = None\n",
        "        for rec in recommendations_list:\n",
        "            if rec.get(\"article_number\") == i:\n",
        "                recommendation = rec.get(\"recommendation\")\n",
        "                break\n",
        "\n",
        "        # Only show if recommendation exists\n",
        "        if recommendation:\n",
        "            recommendation_lines = textwrap.wrap(recommendation, width=94)\n",
        "            for line in recommendation_lines:\n",
        "                print(f\"â”‚  {line}\".ljust(99) + \"â”‚\")\n",
        "\n",
        "        # Card bottom\n",
        "        print(\"â””\" + \"â”€\" * 98 + \"â”˜\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27d9ea51",
      "metadata": {},
      "source": [
        "## Testing the recommendation system\n",
        "\n",
        "Testing the recommendation system using a search query in Spanish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "ce48fee9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸ” Search Query: Seguridad y vulnerabilidades\n",
            "================================================================================\n",
            "\n",
            "âœ… Found 5 relevant articles\n",
            "\n",
            "================================================================================\n",
            "ğŸ¤– Generating personalized recommendations...\n",
            "\n",
            "<think>\n",
            "\n",
            "</think>\n",
            "{\n",
            "    \"recommendations\": [\n",
            "        {\n",
            "            \"article_number\": 1,\n",
            "            \"recommendation\": \"If you're interested in security and vulnerabilities, this article about a critical vulnerability in authentication systems is highly relevant. It provides specific steps to update your SDK to prevent unauthorized access.\"\n",
            "        },\n",
            "        {\n",
            "            \"article_number\": 2,\n",
            "            \"recommendation\": \"This post complements your search with detailed guidance on migrating from version 1.x to 2.0, covering the significant changes and best practices for a smooth transition.\"\n",
            "        },\n",
            "        {\n",
            "            \"article_number\": 3,\n",
            "            \"recommendation\": \"For those looking into security updates, this article highlights important changes in the notification system, including new features like push notifications and customizable alert rules.\"\n",
            "        },\n",
            "        {\n",
            "            \"article_number\": 4,\n",
            "            \"recommendation\": \"This tutorial provides a checklist for updating to the new architecture, helping you assess risks and prepare for potential disruptions during migration.\"\n",
            "        },\n",
            "        {\n",
            "            \"article_number\": 5,\n",
            "            \"recommendation\": \"If you've recently processed large volumes of data, this article is crucial as it addresses a critical error that could lead to data loss. It recommends updating to version 2.1.3 immediately.\"\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "Elasticserch results:  [\n",
            "    {\n",
            "        \"_index\": \"blog-posts\",\n",
            "        \"_id\": \"yZpW_5oBAtFvugiaVf-I\",\n",
            "        \"_score\": 16.530277,\n",
            "        \"_source\": {\n",
            "            \"id\": \"2\",\n",
            "            \"title\": \"Advertencia de seguridad: Vulnerabilidad en el sistema de autenticaci\\u00f3n\",\n",
            "            \"author\": \"Equipo de Seguridad\",\n",
            "            \"date\": \"2025-11-02\",\n",
            "            \"category\": \"security\",\n",
            "            \"content\": \"Hemos identificado una vulnerabilidad cr\\u00edtica en el sistema de autenticaci\\u00f3n que podr\\u00eda permitir acceso no autorizado. La vulnerabilidad afecta a usuarios que utilizan tokens JWT emitidos antes del 15 de octubre. Recomendamos actualizar inmediatamente a la versi\\u00f3n 3.2.1 del SDK. Todos los tokens afectados han sido revocados autom\\u00e1ticamente. Por favor, regenere sus credenciales de acceso.\"\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"_index\": \"blog-posts\",\n",
            "        \"_id\": \"zJpW_5oBAtFvugiaVf-I\",\n",
            "        \"_score\": 2.7339423,\n",
            "        \"_source\": {\n",
            "            \"id\": \"5\",\n",
            "            \"title\": \"Riesgos conocidos al migrar de la versi\\u00f3n 1.x a 2.0\",\n",
            "            \"author\": \"Laura P\\u00e9rez\",\n",
            "            \"date\": \"2025-11-05\",\n",
            "            \"category\": \"tutorial\",\n",
            "            \"content\": \"Si est\\u00e1s planeando migrar de la versi\\u00f3n 1.x a 2.0, hay varios riesgos importantes a considerar. El cambio m\\u00e1s significativo es la nueva estructura de datos que requiere transformaci\\u00f3n de esquemas. Algunos endpoints han sido deprecados y otros tienen cambios en los par\\u00e1metros requeridos. Recomendamos hacer la migraci\\u00f3n primero en un ambiente de staging y validar todas las integraciones. Hemos creado una gu\\u00eda detallada de migraci\\u00f3n.\"\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"_index\": \"blog-posts\",\n",
            "        \"_id\": \"y5pW_5oBAtFvugiaVf-I\",\n",
            "        \"_score\": 2.587478,\n",
            "        \"_source\": {\n",
            "            \"id\": \"4\",\n",
            "            \"title\": \"Important changes to the notification system\",\n",
            "            \"author\": \"Carlos Rivera\",\n",
            "            \"date\": \"2025-11-04\",\n",
            "            \"category\": \"product\",\n",
            "            \"content\": \"We have made important changes to the notification system. We now support push, email, and SMS notifications. Users can configure custom rules to receive alerts based on metric thresholds. The new system includes a notification history and the ability to temporarily mute certain types of alerts. These changes require a mobile client update.\"\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"_index\": \"blog-posts\",\n",
            "        \"_id\": \"1ZpW_5oBAtFvugiaVf-I\",\n",
            "        \"_score\": 2.4722583,\n",
            "        \"_source\": {\n",
            "            \"id\": \"14\",\n",
            "            \"title\": \"Migration risk: Considerations for updating to the new architecture\",\n",
            "            \"author\": \"Jessica Martinez\",\n",
            "            \"date\": \"2025-11-14\",\n",
            "            \"category\": \"tutorial\",\n",
            "            \"content\": \"If you're considering updating to the new architecture, there are several risks to consider. The most important change is the database migration that requires planned downtime. There are also changes in the configuration structure that may break existing integrations. We recommend doing a complete impact assessment before proceeding. We have created a migration checklist available in our repository.\"\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"_index\": \"blog-posts\",\n",
            "        \"_id\": \"z5pW_5oBAtFvugiaVf-I\",\n",
            "        \"_score\": 2.2137363,\n",
            "        \"_source\": {\n",
            "            \"id\": \"8\",\n",
            "            \"title\": \"Actualizaci\\u00f3n cr\\u00edtica: Correcci\\u00f3n de error en procesamiento de datos\",\n",
            "            \"author\": \"Equipo de Desarrollo\",\n",
            "            \"date\": \"2025-11-08\",\n",
            "            \"category\": \"product\",\n",
            "            \"content\": \"Hemos corregido un error cr\\u00edtico que causaba p\\u00e9rdida de datos en ciertos escenarios de procesamiento. El error afectaba a usuarios que procesaban m\\u00e1s de 10,000 registros simult\\u00e1neamente. La correcci\\u00f3n est\\u00e1 disponible en la versi\\u00f3n 2.1.3. Si has procesado grandes vol\\u00famenes de datos recientemente, por favor verifica la integridad de tus datos y contacta a soporte si encuentras discrepancias.\"\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "====================================================================================================\n",
            "                                       ğŸ“‡ RECOMMENDED ARTICLES                                       \n",
            "====================================================================================================\n",
            "\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚  ğŸ“Œ Advertencia de seguridad: Vulnerabilidad en el sistema de autenticaciÃ³n                       â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚  If you're interested in security and vulnerabilities, this article about a critical             â”‚\n",
            "â”‚  vulnerability in authentication systems is highly relevant. It provides specific steps to       â”‚\n",
            "â”‚  update your SDK to prevent unauthorized access.                                                 â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚  ğŸ“Œ Riesgos conocidos al migrar de la versiÃ³n 1.x a 2.0                                           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚  This post complements your search with detailed guidance on migrating from version 1.x to 2.0,  â”‚\n",
            "â”‚  covering the significant changes and best practices for a smooth transition.                    â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚  ğŸ“Œ Important changes to the notification system                                                  â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚  For those looking into security updates, this article highlights important changes in the       â”‚\n",
            "â”‚  notification system, including new features like push notifications and customizable alert      â”‚\n",
            "â”‚  rules.                                                                                          â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚  ğŸ“Œ Migration risk: Considerations for updating to the new architecture                           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚  This tutorial provides a checklist for updating to the new architecture, helping you assess     â”‚\n",
            "â”‚  risks and prepare for potential disruptions during migration.                                   â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚  ğŸ“Œ ActualizaciÃ³n crÃ­tica: CorrecciÃ³n de error en procesamiento de datos                          â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚  If you've recently processed large volumes of data, this article is crucial as it addresses a   â”‚\n",
            "â”‚  critical error that could lead to data loss. It recommends updating to version 2.1.3            â”‚\n",
            "â”‚  immediately.                                                                                    â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
          ]
        }
      ],
      "source": [
        "search_query = \"Seguridad y vulnerabilidades\"\n",
        "\n",
        "articles, recommendations = recommend_articles(search_query)\n",
        "\n",
        "print(\"\\nElasticserch results: \", json.dumps(articles, indent=4))\n",
        "\n",
        "# Display visual cards\n",
        "display_recommendation_cards(articles, recommendations)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fbeb64b",
      "metadata": {},
      "source": [
        "`ask_question_streaming` function to put together the semantic search and the real time chat_completions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C2WKnj8UZa7g",
      "metadata": {
        "id": "C2WKnj8UZa7g"
      },
      "source": [
        "## Cleanup\n",
        "\n",
        "Delete the index and inference endpoints to prevent consuming resources after completing the workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TgqFHEhPZfAd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgqFHEhPZfAd",
        "outputId": "675e161b-f7e3-43f8-d3cc-f9e7fa72f6aa"
      },
      "outputs": [],
      "source": [
        "# Cleanup - Delete Index\n",
        "es_client.indices.delete(index=INDEX_NAME)\n",
        "print(f\"Index {INDEX_NAME} deleted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dcc72db",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup - Delete Inference Endpoint\n",
        "es_client.inference.delete(inference_id=INFERENCE_ENDPOINT_ID)\n",
        "print(f\"Inference endpoint {INFERENCE_ENDPOINT_ID} deleted\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
