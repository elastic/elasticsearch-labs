{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity search with ApproxRetrievalStrategy\n",
    "\n",
    "This workbook demonstrates how to perform a similarity search using [ElasticsearchStore](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.elasticsearch.ElasticsearchStore.html) and [ApproxRetrievalStrategy](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.elasticsearch.ElasticsearchStore.html#langchain.vectorstores.elasticsearch.ElasticsearchStore.ApproxRetrievalStrategy). We will first download sample dataset and  split documents into chunks using langchain and then index into elasticsearch through [ElasticsearchStore.from_documents](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.elasticsearch.ElasticsearchStore.html#langchain.vectorstores.elasticsearch.ElasticsearchStore.from_documents).\n",
    "\n",
    "The [ApproxRetrievalStrategy](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.elasticsearch.ElasticsearchStore.html#langchain.vectorstores.elasticsearch.ElasticsearchStore.ApproxRetrievalStrategy) uses HNSW algorithm to find [nearest neighbor](https://en.wikipedia.org/wiki/Nearest_neighbor_search), which is the fastest and memory efficient algorithm. During the indexing, dense vector fields are generated and is store within the index. We can either provide an embedding function or provide a `query_model_id` to embed the query. In this example we will provide a `query_model_id` during the query time. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages and import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install -qU langchain elasticsearch tiktoken  sentence-transformers eland  transformers\n",
    "\n",
    "from getpass import getpass\n",
    "from langchain.vectorstores import ElasticsearchStore\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from urllib.request import urlopen\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-22 17:32:51,016 INFO : Establishing connection to Elasticsearch\n",
      "2023-08-22 17:32:51,827 INFO : Connected to cluster named 'f5045883120d4bc483f8095d5bf21210' (version: 8.9.0)\n",
      "2023-08-22 17:32:51,830 INFO : Loading HuggingFace transformer tokenizer and model 'sentence-transformers/all-MiniLM-L6-v2'\n",
      "2023-08-22 17:32:54,943 ERROR : Trained model with id 'sentence-transformers__all-minilm-l6-v2' already exists\n",
      "2023-08-22 17:32:54,943 INFO : Run the script with the '--clear-previous' flag if you want to overwrite the existing model.\n"
     ]
    }
   ],
   "source": [
    "API_KEY = getpass(\"Elastic deployment API Key\")\n",
    "CLOUD_ID = getpass(\"Elastic deployment Cloud ID\")\n",
    "!eland_import_hub_model --cloud-id $CLOUD_ID --hub-model-id sentence-transformers/all-MiniLM-L6-v2 --task-type text_embedding --es-api-key $API_KEY --start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Elasticsearch\n",
    "\n",
    "ℹ️ We're using an Elastic Cloud deployment of Elasticsearch for this notebook. If you don't have an Elastic Cloud deployment, sign up here for a free trial.\n",
    "\n",
    "We'll use the Cloud ID to identify our deployment, because we are using Elastic Cloud deployment. To find the Cloud ID for your deployment, go to https://cloud.elastic.co/deployments and select your deployment.\n",
    "\n",
    "We will use ElasticsearchStore to connect to our elastic cloud deployment, This would help create and index data easily. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLOUD_ID = getpass(\"Elastic deployment Cloud ID\")\n",
    "CLOUD_USERNAME = \"elastic\"\n",
    "CLOUD_PASSWORD = getpass(\"Elastic deployment Password\")\n",
    "\n",
    "# we will explicity set ApproxRetrievalStrategy when instantiate ElasticsearchStore instance\n",
    "vector_store = ElasticsearchStore(\n",
    "            es_cloud_id=CLOUD_ID, \n",
    "            es_user=CLOUD_USERNAME, \n",
    "            query_field=\"content\",\n",
    "            vector_query_field=\"text_embedding.predicted_value\",\n",
    "            es_password=CLOUD_PASSWORD,\n",
    "            index_name= \"approx-search-demo\",\n",
    "            strategy=ElasticsearchStore.ApproxRetrievalStrategy(query_model_id=\"sentence-transformers__all-minilm-l6-v2\")\n",
    "            \n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the sample dataset\n",
    "\n",
    "Let's download the sample dataset and deserialize the document to make document chunking easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/elastic/elasticsearch-labs/main/example-apps/workplace-search/example-data/\"  \n",
    "# url = \"https://raw.githubusercontent.com/saarikabhasi/elasticsearch-labs/main/notebooks/integrations/hugging-face/blogs.json\"\n",
    "response = urlopen(url)\n",
    "\n",
    "workplace_docs = json.loads(response.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ingestion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ingest pipeline definition\n",
    "PIPELINE_ID=\"vectorize_workplace\"\n",
    "\n",
    "vector_store.client.ingest.put_pipeline(id=PIPELINE_ID, processors=[{\n",
    "        \"inference\": {\n",
    "          \"model_id\": \"sentence-transformers__all-minilm-l6-v2\",\n",
    "          \"target_field\": \"text_embedding\",\n",
    "          \"field_map\": {\n",
    "            \"content\": \"text_field\"\n",
    "          }\n",
    "        }\n",
    "      }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Documents into Passages\n",
    "\n",
    "Next, We will chunk these documents into 800 token passages with an overlap of 0 tokens using a text splitter, [CharacterTextSplitter](https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.CharacterTextSplitter.html) and then create documents from these texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xAkc1OVcOxy3",
    "outputId": "b2453634-89b8-48bc-ac65-a6a1c3b8170f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting existing approx-search-demo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s9/4v973pqs2g9bsdty8jhnyc_40000gn/T/ipykernel_87833/196954853.py:74: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  vector_store.client.indices.delete(index=INDEX_NAME, ignore=[400, 404])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index approx-search-demo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s9/4v973pqs2g9bsdty8jhnyc_40000gn/T/ipykernel_87833/196954853.py:77: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  vector_store.client.indices.create(index=INDEX_NAME, mappings=INDEX_MAPPING, settings=INDEX_SETTINGS,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'approx-search-demo'})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define index name\n",
    "INDEX_NAME=\"approx-search-demo\"\n",
    "\n",
    "# flag to check if index has to be deleted before creating\n",
    "SHOULD_DELETE_INDEX=True\n",
    "\n",
    "# define index mapping\n",
    "INDEX_MAPPING = {\n",
    "    \"properties\": {\n",
    "      \"content\": {\n",
    "        \"type\": \"text\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"summary\":{\n",
    "        \"type\": \"text\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"name\":{\n",
    "        \"type\": \"text\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"text_embedding\": {\n",
    "        \"properties\": {\n",
    "          \"is_truncated\": {\n",
    "            \"type\": \"boolean\"\n",
    "          },\n",
    "          \"model_id\": {\n",
    "            \"type\": \"text\",\n",
    "            \"fields\": {\n",
    "              \"keyword\": {\n",
    "                \"type\": \"keyword\",\n",
    "                \"ignore_above\": 256\n",
    "              }\n",
    "            }\n",
    "          },\n",
    "          \"predicted_value\": {\n",
    "            \"type\": \"dense_vector\",\n",
    "            \"dims\": 384,\n",
    "            \"index\": True,\n",
    "            \"similarity\": \"l2_norm\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "INDEX_SETTINGS = {\n",
    "    \"index\": {\n",
    "      \"number_of_replicas\": \"1\",\n",
    "      \"number_of_shards\": \"1\",\n",
    "      \"default_pipeline\": PIPELINE_ID\n",
    "    }\n",
    "}\n",
    "\n",
    "# check if we want to delete index before creating the index\n",
    "if(SHOULD_DELETE_INDEX):\n",
    "  if vector_store.client.indices.exists(index=INDEX_NAME):\n",
    "    print(\"Deleting existing %s\" % INDEX_NAME)\n",
    "    vector_store.client.indices.delete(index=INDEX_NAME, ignore=[400, 404])\n",
    "\n",
    "print(\"Creating index %s\" % INDEX_NAME)\n",
    "vector_store.client.indices.create(index=INDEX_NAME, mappings=INDEX_MAPPING, settings=INDEX_SETTINGS,\n",
    "                  ignore=[400, 404])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 866, which is longer than the specified 800\n",
      "Created a chunk of size 1120, which is longer than the specified 800\n"
     ]
    }
   ],
   "source": [
    "metadata = []\n",
    "content = []\n",
    "\n",
    "for doc in workplace_docs:\n",
    "  content.append(doc[\"content\"])\n",
    "  metadata.append({\n",
    "      \"name\": doc[\"name\"],\n",
    "      \"summary\": doc[\"summary\"],\n",
    "  })\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(content, metadatas=metadata)\n",
    "# actions = []\n",
    "# for doc in workplace_docs:\n",
    "#     actions.append({\"index\": {\"_index\": \"approx-search-demo\"}})\n",
    "#     actions.append(doc)\n",
    "# vector_store.client.bulk(index=\"approx-search-demo\", operations=actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index data into elasticsearch\n",
    "\n",
    "Now that we have our document ready, next we will index data to elasticsearch using [ElasticsearchStore.from_documents](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.elasticsearch.ElasticsearchStore.html#langchain.vectorstores.elasticsearch.ElasticsearchStore.from_documents). We will use Cloud ID and Passwords values set in the Create cloud deployment step.\n",
    "\n",
    "In the instance, we will set strategy to ElasticsearchStore.ApproxRetrievalStrategy()\n",
    "\n",
    "In this example, we will explicity set `strategy = ElasticsearchStore.ApproxRetrievalStrategy()` although ElasticsearchStore uses ApproxRetrievalStrategy strategy by default.\n",
    "\n",
    "Note: We are providing `query_model_id` in this example, so we are not providing an embedding function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error adding texts: 59 document(s) failed to index.\n"
     ]
    },
    {
     "ename": "BulkIndexError",
     "evalue": "59 document(s) failed to index.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBulkIndexError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mvector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mes_cloud_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCLOUD_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mes_user\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCLOUD_USERNAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mes_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCLOUD_PASSWORD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapprox-search-demo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mElasticsearchStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mApproxRetrievalStrategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_model_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence-transformers__all-minilm-l6-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/vectorstores/elasticsearch.py:1018\u001b[0m, in \u001b[0;36mElasticsearchStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m   1014\u001b[0m elasticsearchStore \u001b[39m=\u001b[39m ElasticsearchStore\u001b[39m.\u001b[39m_create_cls_from_kwargs(\n\u001b[1;32m   1015\u001b[0m     embedding\u001b[39m=\u001b[39membedding, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m   1016\u001b[0m )\n\u001b[1;32m   1017\u001b[0m \u001b[39m# Encode the provided texts and add them to the newly created index.\u001b[39;00m\n\u001b[0;32m-> 1018\u001b[0m elasticsearchStore\u001b[39m.\u001b[39;49madd_documents(documents)\n\u001b[1;32m   1020\u001b[0m \u001b[39mreturn\u001b[39;00m elasticsearchStore\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/vectorstores/base.py:103\u001b[0m, in \u001b[0;36mVectorStore.add_documents\u001b[0;34m(self, documents, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m texts \u001b[39m=\u001b[39m [doc\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m    102\u001b[0m metadatas \u001b[39m=\u001b[39m [doc\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_texts(texts, metadatas, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/vectorstores/elasticsearch.py:872\u001b[0m, in \u001b[0;36mElasticsearchStore.add_texts\u001b[0;34m(self, texts, metadatas, ids, refresh_indices, create_index_if_not_exists, **kwargs)\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    871\u001b[0m         logger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError adding texts: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 872\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    874\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    875\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mNo texts to add to index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/vectorstores/elasticsearch.py:861\u001b[0m, in \u001b[0;36mElasticsearchStore.add_texts\u001b[0;34m(self, texts, metadatas, ids, refresh_indices, create_index_if_not_exists, **kwargs)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(requests) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    860\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m         success, failed \u001b[39m=\u001b[39m bulk(\n\u001b[1;32m    862\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient, requests, stats_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, refresh\u001b[39m=\u001b[39;49mrefresh_indices\n\u001b[1;32m    863\u001b[0m         )\n\u001b[1;32m    864\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    865\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAdded \u001b[39m\u001b[39m{\u001b[39;00msuccess\u001b[39m}\u001b[39;00m\u001b[39m and failed to add \u001b[39m\u001b[39m{\u001b[39;00mfailed\u001b[39m}\u001b[39;00m\u001b[39m texts to index\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         )\n\u001b[1;32m    868\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39madded texts \u001b[39m\u001b[39m{\u001b[39;00mids\u001b[39m}\u001b[39;00m\u001b[39m to index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/elasticsearch/helpers/actions.py:521\u001b[0m, in \u001b[0;36mbulk\u001b[0;34m(client, actions, stats_only, ignore_status, *args, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[39m# make streaming_bulk yield successful results so we can count them\u001b[39;00m\n\u001b[1;32m    520\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39myield_ok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 521\u001b[0m \u001b[39mfor\u001b[39;00m ok, item \u001b[39min\u001b[39;00m streaming_bulk(\n\u001b[1;32m    522\u001b[0m     client, actions, ignore_status\u001b[39m=\u001b[39mignore_status, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    523\u001b[0m ):\n\u001b[1;32m    524\u001b[0m     \u001b[39m# go through request-response pairs and detect failures\u001b[39;00m\n\u001b[1;32m    525\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ok:\n\u001b[1;32m    526\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stats_only:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/elasticsearch/helpers/actions.py:436\u001b[0m, in \u001b[0;36mstreaming_bulk\u001b[0;34m(client, actions, chunk_size, max_chunk_bytes, raise_on_error, expand_action_callback, raise_on_exception, max_retries, initial_backoff, max_backoff, yield_ok, ignore_status, *args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39mmin\u001b[39m(max_backoff, initial_backoff \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m (attempt \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)))\n\u001b[1;32m    435\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 436\u001b[0m     \u001b[39mfor\u001b[39;00m data, (ok, info) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[1;32m    437\u001b[0m         bulk_data,\n\u001b[1;32m    438\u001b[0m         _process_bulk_chunk(\n\u001b[1;32m    439\u001b[0m             client,\n\u001b[1;32m    440\u001b[0m             bulk_actions,\n\u001b[1;32m    441\u001b[0m             bulk_data,\n\u001b[1;32m    442\u001b[0m             raise_on_exception,\n\u001b[1;32m    443\u001b[0m             raise_on_error,\n\u001b[1;32m    444\u001b[0m             ignore_status,\n\u001b[1;32m    445\u001b[0m             \u001b[39m*\u001b[39margs,\n\u001b[1;32m    446\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    447\u001b[0m         ),\n\u001b[1;32m    448\u001b[0m     ):\n\u001b[1;32m    449\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ok:\n\u001b[1;32m    450\u001b[0m             action, info \u001b[39m=\u001b[39m info\u001b[39m.\u001b[39mpopitem()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/elasticsearch/helpers/actions.py:355\u001b[0m, in \u001b[0;36m_process_bulk_chunk\u001b[0;34m(client, bulk_actions, bulk_data, raise_on_exception, raise_on_error, ignore_status, *args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     gen \u001b[39m=\u001b[39m _process_bulk_chunk_success(\n\u001b[1;32m    350\u001b[0m         resp\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mbody,\n\u001b[1;32m    351\u001b[0m         bulk_data\u001b[39m=\u001b[39mbulk_data,\n\u001b[1;32m    352\u001b[0m         ignore_status\u001b[39m=\u001b[39mignore_status,\n\u001b[1;32m    353\u001b[0m         raise_on_error\u001b[39m=\u001b[39mraise_on_error,\n\u001b[1;32m    354\u001b[0m     )\n\u001b[0;32m--> 355\u001b[0m \u001b[39myield from\u001b[39;00m gen\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/elasticsearch/helpers/actions.py:274\u001b[0m, in \u001b[0;36m_process_bulk_chunk_success\u001b[0;34m(resp, bulk_data, ignore_status, raise_on_error)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[39myield\u001b[39;00m ok, {op_type: item}\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m errors:\n\u001b[0;32m--> 274\u001b[0m     \u001b[39mraise\u001b[39;00m BulkIndexError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(errors)\u001b[39m}\u001b[39;00m\u001b[39m document(s) failed to index.\u001b[39m\u001b[39m\"\u001b[39m, errors)\n",
      "\u001b[0;31mBulkIndexError\u001b[0m: 59 document(s) failed to index."
     ]
    }
   ],
   "source": [
    "documents = vector_store.from_documents(\n",
    "    docs, es_cloud_id=CLOUD_ID, es_user=CLOUD_USERNAME, es_password=CLOUD_PASSWORD,   index_name= \"approx-search-demo\",\n",
    "    strategy=ElasticsearchStore.ApproxRetrievalStrategy(query_model_id=\"sentence-transformers__all-minilm-l6-v2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Result functions\n",
    "\n",
    "Next, we will create a small function to show the results of our query in human-readable format. This function would be used in our examples to display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showResults(output):\n",
    "  print(\"Total results: \", len(output))\n",
    "  for index in range(len(output)):\n",
    "    print(output[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results:  0\n"
     ]
    }
   ],
   "source": [
    "results = documents.similarity_search(\"How does the compensation work?\")\n",
    "showResults(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
