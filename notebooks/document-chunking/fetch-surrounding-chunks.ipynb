{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fetch surronding chucks (N-1, N+1)\n",
    "\n",
    "This notebook is designed to handle the ingestion of book text (Harry Potter and the Sorcerer's Stone) into an Elasticsearch Cloud instance. It includes partitioning the book text into chapters and chunking the chapter text, which are then ingested into Elasticsearch. The setup utilizes a nested structure, and for each chunk, it stores dense and sparse (ELSER) vector representations along with the text representation.\n",
    "\n",
    "Searches are performed using dense vector comparisons, sparse vector comparisons, and text search in parallel to demonstrate the power of hybrid search strategies. Additionally, the notebook is configured to retrieve adjacent chunks (n-1 and n+1), allowing for a more contextual understanding of the search results.\n",
    "\n"
   ],
   "metadata": {
    "id": "aAUkwshINwV7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install required python libraries\n"
   ],
   "metadata": {
    "id": "MUEpppV7SeLu"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXuL8wsQNq8G"
   },
   "outputs": [],
   "source": [
    "!pip install elasticsearch==8.13.2\n",
    "!pip install pandas\n",
    "!python -m pip install eland\n",
    "\n",
    "import json\n",
    "import time\n",
    "import urllib.request\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, BertTokenizer\n",
    "from elasticsearch import Elasticsearch, helpers, exceptions\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Elasticsearch and Tokenizer Configuration\n",
    "\n",
    "This section sets up the necessary configurations for connecting to Elasticsearch and initializing the tokenizers used for text processing.\n",
    "\n",
    "### Configuration Details:\n",
    "1. **Elasticsearch Credentials**:\n",
    "   - `ELASTIC_CLOUD_ID`: The Cloud ID for the Elasticsearch cluster, securely fetched using the `getpass` function.\n",
    "   - `ELASTIC_API_KEY`: The API key for Elasticsearch authentication, securely fetched using the `getpass` function.\n",
    "\n",
    "2. **Index Settings**:\n",
    "   - `raw_source_index`: The name of the index for the raw dataset (`harry_potter_dataset-raw`).\n",
    "   - `index_name`: The name of the enriched dataset index (`harry_potter_dataset_enriched`).\n",
    "\n",
    "3. **Embedding Models**:\n",
    "   - `dense_embedding_model`: Specifies the model used for generating dense embeddings (`sentence-transformers__all-minilm-l6-v2`).\n",
    "   - `dense_huggingface_model_id`: The Hugging Face model ID for the dense embeddings (`sentence-transformers/all-MiniLM-L6-v2`).\n",
    "   - `dense_model_number_of_allocators`: The number of allocators for the dense embedding model (2).\n",
    "\n",
    "   - `elser_model_id`: Specifies the ELSER model ID (`.elser_model_2_linux-x86_64`).\n",
    "   - `elser_model_number_of_allocators`: The number of allocators for the ELSER model (2).\n",
    "\n",
    "4. **Tokenizer Initialization**:\n",
    "   - `bert_tokenizer`: Initializes the BERT tokenizer (`bert-base-uncased`) for English text processing.\n",
    "\n",
    "5. **Chunking Parameters**:\n",
    "   - `SEMANTIC_SEARCH_TOKEN_LIMIT`: Sets the token limit for each chunk (500 tokens per chunk, considering space for special tokens).\n",
    "   - `ELSER_TOKEN_OVERLAP`: Defines the overlap ratio between chunks (default is 0%, customizable for context continuity).\n",
    "\n",
    "These configurations ensure that the necessary components are properly set up for efficient text processing, indexing, and search operations in Elasticsearch.\n"
   ],
   "metadata": {
    "id": "2w7uTCYdQ0m6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from getpass import getpass\n",
    "\n",
    "# https://www.elastic.co/search-labs/tutorials/install-elasticsearch/elastic-cloud#finding-your-cloud-id\n",
    "ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID: \")\n",
    "\n",
    "# https://www.elastic.co/search-labs/tutorials/install-elasticsearch/elastic-cloud#creating-an-api-key\n",
    "ELASTIC_API_KEY = getpass(\"Elastic Api Key: \")\n",
    "\n",
    "raw_source_index = \"harry_potter_dataset-raw\"\n",
    "index_name = \"harry_potter_dataset_enriched\"\n",
    "\n",
    "dense_embedding_model = \"sentence-transformers__all-minilm-l6-v2\"\n",
    "dense_huggingface_model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "dense_model_number_of_allocators = 2\n",
    "\n",
    "elser_model_id = \".elser_model_2_linux-x86_64\"\n",
    "elser_model_number_of_allocators = 2\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "SEMANTIC_SEARCH_TOKEN_LIMIT = 500\n",
    "ELSER_TOKEN_OVERLAP = 0.0\n",
    "\n",
    "\n",
    "# Create the client instance\n",
    "esclient = Elasticsearch(\n",
    "    cloud_id=ELASTIC_CLOUD_ID,\n",
    "    api_key=ELASTIC_API_KEY,\n",
    ")\n",
    "print(esclient.info())"
   ],
   "metadata": {
    "id": "LGQAjG6PERfx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Import model\n",
    "Using the eland_import_hub_model script, download and install all-MiniLM-L6-v2 transformer model. Setting the NLP --task-type as text_embedding.\n",
    "\n",
    "To get the cloud id, go to Elastic cloud and On the deployment overview page, copy down the Cloud ID.\n",
    "\n",
    "To authenticate your request, You could use API key. Alternatively, you can use your cloud deployment username and password."
   ],
   "metadata": {
    "id": "rOWheQ-uJE2C"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!eland_import_hub_model --cloud-id $ELASTIC_CLOUD_ID --hub-model-id {dense_huggingface_model_id} --task-type text_embedding --es-api-key $ELASTIC_API_KEY --start --clear-previous\n",
    "resp = esclient.ml.update_trained_model_deployment(\n",
    "    model_id=dense_embedding_model,\n",
    "    body={\"number_of_allocations\": dense_model_number_of_allocators},\n",
    ")\n",
    "print(resp)"
   ],
   "metadata": {
    "id": "4NH8JJkQJDit"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Download and Deploy ELSER Model\n",
    "\n",
    "In this example, we are going to download and deploy the ELSER model in our ML node. Make sure you have an ML node in order to run the ELSER model."
   ],
   "metadata": {
    "id": "f1SXd1uhhhhe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# delete model if already downloaded and deployed\n",
    "try:\n",
    "    esclient.ml.delete_trained_model(model_id=elser_model_id, force=True)\n",
    "    print(\"Model deleted successfully, We will proceed with creating one\")\n",
    "except exceptions.NotFoundError:\n",
    "    print(\"Model doesn't exist, but We will proceed with creating one\")\n",
    "\n",
    "# Creates the ELSER model configuration. Automatically downloads the model if it doesn't exist.\n",
    "esclient.ml.put_trained_model(\n",
    "    model_id=elser_model_id, input={\"field_names\": [\"text_field\"]}\n",
    ")"
   ],
   "metadata": {
    "id": "vL68fse9hhAN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above command will download the ELSER model. This will take a few minutes to complete. Use the following command to check the status of the model download."
   ],
   "metadata": {
    "id": "2R54LYIqwC-f"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "while True:\n",
    "    status = esclient.ml.get_trained_models(\n",
    "        model_id=elser_model_id, include=\"definition_status\"\n",
    "    )\n",
    "\n",
    "    if status[\"trained_model_configs\"][0][\"fully_defined\"]:\n",
    "        print(\"ELSER Model is downloaded and ready to be deployed.\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"ELSER Model is downloaded but not ready to be deployed.\")\n",
    "    time.sleep(5)"
   ],
   "metadata": {
    "id": "wE3KHB3BwCVk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once the model is downloaded, we can deploy the model in our ML node. Use the following command to deploy the model.  This also will take a few minutes to complete.\n"
   ],
   "metadata": {
    "id": "_8-mvOj5wanm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Start trained model deployment if not already deployed\n",
    "esclient.ml.start_trained_model_deployment(\n",
    "    model_id=elser_model_id,\n",
    "    number_of_allocations=elser_model_number_of_allocators,\n",
    "    wait_for=\"starting\",\n",
    ")\n",
    "\n",
    "while True:\n",
    "    status = esclient.ml.get_trained_models_stats(\n",
    "        model_id=elser_model_id,\n",
    "    )\n",
    "    if status[\"trained_model_stats\"][0][\"deployment_stats\"][\"state\"] == \"started\":\n",
    "        print(\"ELSER Model has been successfully deployed.\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"ELSER Model is currently being deployed.\")\n",
    "    time.sleep(5)"
   ],
   "metadata": {
    "id": "xzdANHzxwaSf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Helper Methods/Functions"
   ],
   "metadata": {
    "id": "3LlGP3aJP1ce"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def whitespace_tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def manage_index(es, index_name, settings, mappings, delete_index=False):\n",
    "    if es.indices.exists(index=index_name):\n",
    "        if delete_index:\n",
    "            print(f\"Index {index_name} exists. Deleting it...\")\n",
    "            es.indices.delete(index=index_name)\n",
    "            print(f\"Index {index_name} deleted!\")\n",
    "        else:\n",
    "            print(f\"Index {index_name} already exists. Skipping creation.\")\n",
    "            return\n",
    "    es.indices.create(index=index_name, settings=settings, mappings=mappings)\n",
    "    print(f\"Index {index_name} created successfully!\")\n",
    "\n",
    "\n",
    "def generate_actions(df, index_name):\n",
    "    for _, row in df.iterrows():\n",
    "        chunks = chunk(row[\"chapter_full_text\"])\n",
    "        passages = [\n",
    "            {\"text\": ch[\"text\"], \"chunk_number\": ch[\"chunk_number\"]} for ch in chunks\n",
    "        ]\n",
    "        doc = {\n",
    "            \"_index\": index_name,\n",
    "            \"_source\": {\n",
    "                \"book_title\": row[\"book_title\"],\n",
    "                \"chapter\": row[\"chapter\"],\n",
    "                \"chapter_full_text\": row[\"chapter_full_text\"],\n",
    "                \"passages\": passages,\n",
    "            },\n",
    "        }\n",
    "        yield doc\n",
    "\n",
    "\n",
    "def index_dataframe(es, index_name, df, thread_count=1, chunk_size=200):\n",
    "    print(f\"Indexing documents to {index_name}...\")\n",
    "    success_count = 0\n",
    "    failed_count = 0\n",
    "    try:\n",
    "        for success, _ in helpers.parallel_bulk(\n",
    "            es,\n",
    "            generate_actions(df, index_name),\n",
    "            thread_count=thread_count,\n",
    "            chunk_size=chunk_size,\n",
    "        ):\n",
    "            if success:\n",
    "                success_count += 1\n",
    "            else:\n",
    "                failed_count += 1\n",
    "    except helpers.BulkIndexError as e:\n",
    "        print(\"Bulk indexing error:\", e)\n",
    "        for error_detail in e.errors:\n",
    "            print(error_detail)\n",
    "    print(f\"Successfully indexed {success_count} documents.\")\n",
    "    print(f\"Failed to index {failed_count} documents.\")\n",
    "\n",
    "\n",
    "def build_vector(text):\n",
    "    docs = [{\"text_field\": text}]\n",
    "    response = esclient.ml.infer_trained_model(\n",
    "        model_id=dense_embedding_model, docs=docs\n",
    "    )\n",
    "    return response.get(\"inference_results\", [{}])[0].get(\"predicted_value\", [])\n",
    "\n",
    "\n",
    "def build_rrf_query(\n",
    "    embeddings, user_query, rrf_rank_constant, rrf_window_size, debug=False\n",
    "):\n",
    "    query = {\n",
    "        \"_source\": False,\n",
    "        \"sub_searches\": [\n",
    "            {\n",
    "                \"query\": {\n",
    "                    \"nested\": {\n",
    "                        \"path\": \"passages\",\n",
    "                        \"query\": {\"match\": {\"passages.text\": user_query}},\n",
    "                        \"inner_hits\": {\n",
    "                            \"name\": \"text_hits\",\n",
    "                            \"size\": 1,\n",
    "                            \"_source\": [\"passages.text\", \"passages.chunk_number\"],\n",
    "                        },\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"query\": {\n",
    "                    \"nested\": {\n",
    "                        \"path\": \"passages\",\n",
    "                        \"query\": {\n",
    "                            \"knn\": {\n",
    "                                \"query_vector\": embeddings,\n",
    "                                \"field\": \"passages.vector.predicted_value\",\n",
    "                                \"num_candidates\": 50,\n",
    "                            }\n",
    "                        },\n",
    "                        \"inner_hits\": {\n",
    "                            \"name\": \"dense_hit\",\n",
    "                            \"size\": 1,\n",
    "                            \"_source\": [\"passages.text\", \"passages.chunk_number\"],\n",
    "                        },\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"query\": {\n",
    "                    \"nested\": {\n",
    "                        \"path\": \"passages\",\n",
    "                        \"query\": {\n",
    "                            \"bool\": {\n",
    "                                \"should\": [\n",
    "                                    {\n",
    "                                        \"text_expansion\": {\n",
    "                                            \"passages.content_embedding.predicted_value\": {\n",
    "                                                \"model_id\": elser_model_id,\n",
    "                                                \"model_text\": user_query,\n",
    "                                            }\n",
    "                                        }\n",
    "                                    }\n",
    "                                ]\n",
    "                            }\n",
    "                        },\n",
    "                        \"inner_hits\": {\n",
    "                            \"name\": \"sparse_hits\",\n",
    "                            \"size\": 1,\n",
    "                            \"_source\": [\"passages.text\", \"passages.chunk_number\"],\n",
    "                        },\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        ],\n",
    "        \"rank\": {\n",
    "            \"rrf\": {\"window_size\": rrf_window_size, \"rank_constant\": rrf_rank_constant}\n",
    "        },\n",
    "    }\n",
    "    if debug:\n",
    "        print(json.dumps(query, indent=4))\n",
    "    return query\n",
    "\n",
    "\n",
    "def build_custom_query(\n",
    "    query_vector, user_query, knn_boost_factor, text_expansion_boost, debug=False\n",
    "):\n",
    "    query = {\n",
    "        \"_source\": False,\n",
    "        \"fields\": [\"chapter\"],\n",
    "        \"query\": {\n",
    "            \"function_score\": {\n",
    "                \"query\": {\n",
    "                    \"bool\": {\n",
    "                        \"should\": [\n",
    "                            {\n",
    "                                \"nested\": {\n",
    "                                    \"path\": \"passages\",\n",
    "                                    \"query\": {\"match\": {\"passages.text\": user_query}},\n",
    "                                    \"inner_hits\": {\n",
    "                                        \"name\": \"text_hits\",\n",
    "                                        \"size\": 1,\n",
    "                                        \"_source\": [\n",
    "                                            \"passages.text\",\n",
    "                                            \"passages.chunk_number\",\n",
    "                                        ],\n",
    "                                    },\n",
    "                                }\n",
    "                            },\n",
    "                            {\n",
    "                                \"nested\": {\n",
    "                                    \"path\": \"passages\",\n",
    "                                    \"query\": {\n",
    "                                        \"script_score\": {\n",
    "                                            \"query\": {\n",
    "                                                \"knn\": {\n",
    "                                                    \"field\": \"passages.vector.predicted_value\",\n",
    "                                                    \"query_vector\": query_vector,\n",
    "                                                    \"num_candidates\": 50,\n",
    "                                                }\n",
    "                                            },\n",
    "                                            \"script\": {\n",
    "                                                \"source\": \"Math.log(1 + _score * params.boost_factor)\",\n",
    "                                                \"params\": {\n",
    "                                                    \"boost_factor\": knn_boost_factor\n",
    "                                                },\n",
    "                                            },\n",
    "                                        }\n",
    "                                    },\n",
    "                                    \"inner_hits\": {\n",
    "                                        \"name\": \"dense_hit\",\n",
    "                                        \"size\": 1,\n",
    "                                        \"_source\": [\n",
    "                                            \"passages.text\",\n",
    "                                            \"passages.chunk_number\",\n",
    "                                        ],\n",
    "                                    },\n",
    "                                }\n",
    "                            },\n",
    "                            {\n",
    "                                \"nested\": {\n",
    "                                    \"path\": \"passages\",\n",
    "                                    \"query\": {\n",
    "                                        \"script_score\": {\n",
    "                                            \"query\": {\n",
    "                                                \"bool\": {\n",
    "                                                    \"should\": [\n",
    "                                                        {\n",
    "                                                            \"text_expansion\": {\n",
    "                                                                \"passages.content_embedding.predicted_value\": {\n",
    "                                                                    \"model_id\": \".elser_model_2_linux-x86_64\",\n",
    "                                                                    \"model_text\": user_query,\n",
    "                                                                }\n",
    "                                                            }\n",
    "                                                        }\n",
    "                                                    ]\n",
    "                                                }\n",
    "                                            },\n",
    "                                            \"script\": {\n",
    "                                                \"source\": \"_score * params.boost_factor\",\n",
    "                                                \"params\": {\n",
    "                                                    \"boost_factor\": text_expansion_boost\n",
    "                                                },\n",
    "                                            },\n",
    "                                        }\n",
    "                                    },\n",
    "                                    \"inner_hits\": {\n",
    "                                        \"name\": \"sparse_hits\",\n",
    "                                        \"size\": 1,\n",
    "                                        \"_source\": [\n",
    "                                            \"passages.text\",\n",
    "                                            \"passages.chunk_number\",\n",
    "                                        ],\n",
    "                                    },\n",
    "                                }\n",
    "                            },\n",
    "                        ]\n",
    "                    }\n",
    "                },\n",
    "                \"score_mode\": \"sum\",\n",
    "                \"boost_mode\": \"sum\",\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "    if debug:\n",
    "        print(json.dumps(query, indent=4))\n",
    "    return query\n",
    "\n",
    "\n",
    "def get_adjacent_chunks_query(doc_id, base_chunk_number, max_chunk_number, debug=False):\n",
    "    # Determine the chunk numbers to query based on the base_chunk_number\n",
    "    if base_chunk_number == 1:\n",
    "        chunk_numbers = [\n",
    "            base_chunk_number,\n",
    "            base_chunk_number + 1,\n",
    "            base_chunk_number + 2,\n",
    "        ]\n",
    "    elif base_chunk_number == max_chunk_number:\n",
    "        chunk_numbers = [\n",
    "            base_chunk_number,\n",
    "            base_chunk_number - 1,\n",
    "            base_chunk_number - 2,\n",
    "        ]\n",
    "    else:\n",
    "        chunk_numbers = [\n",
    "            base_chunk_number - 1,\n",
    "            base_chunk_number,\n",
    "            base_chunk_number + 1,\n",
    "        ]\n",
    "\n",
    "    # Construct the query\n",
    "    query = {\n",
    "        \"_source\": False,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\"term\": {\"_id\": doc_id}},\n",
    "                    {\n",
    "                        \"nested\": {\n",
    "                            \"path\": \"passages\",\n",
    "                            \"query\": {\n",
    "                                \"bool\": {\n",
    "                                    \"should\": [\n",
    "                                        {\"term\": {\"passages.chunk_number\": num}}\n",
    "                                        for num in chunk_numbers\n",
    "                                    ]\n",
    "                                }\n",
    "                            },\n",
    "                            \"inner_hits\": {\n",
    "                                \"_source\": [\"passages.text\", \"passages.chunk_number\"]\n",
    "                            },\n",
    "                        }\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if debug:\n",
    "        print(json.dumps(query, indent=4))\n",
    "\n",
    "    return query\n",
    "\n",
    "\n",
    "def get_max_chunk_number_query(chapter_number, debug=False):\n",
    "    # Construct the query\n",
    "    query = {\n",
    "        \"size\": 0,\n",
    "        \"query\": {\"term\": {\"chapter\": chapter_number}},\n",
    "        \"aggs\": {\n",
    "            \"max_chunk_number\": {\n",
    "                \"nested\": {\"path\": \"passages\"},\n",
    "                \"aggs\": {\"max_chunk\": {\"max\": {\"field\": \"passages.chunk_number\"}}},\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if debug:\n",
    "        print(json.dumps(query, indent=4))\n",
    "\n",
    "    return query\n",
    "\n",
    "\n",
    "def print_text_from_results(results):\n",
    "    if results[\"hits\"][\"hits\"]:\n",
    "        for hit in results[\"hits\"][\"hits\"]:\n",
    "            if \"inner_hits\" in hit and \"passages\" in hit[\"inner_hits\"]:\n",
    "                nested_hits = hit[\"inner_hits\"][\"passages\"][\"hits\"][\"hits\"]\n",
    "                for nested_hit in nested_hits:\n",
    "                    chunk_number = nested_hit[\"_source\"][\"chunk_number\"]\n",
    "                    text = nested_hit[\"_source\"][\"text\"]\n",
    "                    # print(f\"Text from Chunk {chunk_number}: {text}\")\n",
    "                    print(\n",
    "                        f\"\\n\\nText from Chunk {chunk_number}: {textwrap.fill(first_passage_text, width=200)}\"\n",
    "                    )\n",
    "    else:\n",
    "        print(\"No hits found.\")\n",
    "\n",
    "\n",
    "def chunk(\n",
    "    text, chunk_size=SEMANTIC_SEARCH_TOKEN_LIMIT, overlap_ratio=ELSER_TOKEN_OVERLAP\n",
    "):\n",
    "    step_size = round(chunk_size * (1 - overlap_ratio))\n",
    "    tokens = bert_tokenizer.encode(text)\n",
    "    tokens = tokens[1:-1]  # remove special beginning and end tokens\n",
    "    result = []\n",
    "    chunk_number = 1\n",
    "    for i in range(0, len(tokens), step_size):\n",
    "        end = i + chunk_size\n",
    "        chunk_text = bert_tokenizer.decode(tokens[i:end])\n",
    "        result.append({\"text\": chunk_text, \"chunk_number\": chunk_number})\n",
    "        chunk_number += 1\n",
    "        if end >= len(tokens):\n",
    "            break\n",
    "    return result\n",
    "\n",
    "\n",
    "def check_task_status(es, task_id):\n",
    "    while True:\n",
    "        task_response = es.tasks.get(task_id=task_id)\n",
    "        if task_response[\"completed\"]:\n",
    "            print(\"Reindexing complete.\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Indexing...\")\n",
    "            time.sleep(10)"
   ],
   "metadata": {
    "id": "xB2a9-qtONbQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Ingest Pipelines"
   ],
   "metadata": {
    "id": "izMU8HqqP7ld"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the ingest pipeline configuration\n",
    "pipeline_body = {\n",
    "    \"description\": \"Pipeline for processing book passages\",\n",
    "    \"processors\": [\n",
    "        {\n",
    "            \"foreach\": {\n",
    "                \"field\": \"passages\",\n",
    "                \"processor\": {\n",
    "                    \"inference\": {\n",
    "                        \"field_map\": {\"_ingest._value.text\": \"text_field\"},\n",
    "                        \"model_id\": \"sentence-transformers__all-minilm-l6-v2\",\n",
    "                        \"target_field\": \"_ingest._value.vector\",\n",
    "                        \"on_failure\": [\n",
    "                            {\n",
    "                                \"append\": {\n",
    "                                    \"field\": \"_source._ingest.inference_errors\",\n",
    "                                    \"value\": [\n",
    "                                        {\n",
    "                                            \"message\": \"Processor 'inference' in pipeline 'ml-inference-title-vector' failed with message '{{ _ingest.on_failure_message }}'\",\n",
    "                                            \"pipeline\": \"ml-inference-title-vector\",\n",
    "                                            \"timestamp\": \"{{{ _ingest.timestamp }}}\",\n",
    "                                        }\n",
    "                                    ],\n",
    "                                }\n",
    "                            }\n",
    "                        ],\n",
    "                    }\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"foreach\": {\n",
    "                \"field\": \"passages\",\n",
    "                \"processor\": {\n",
    "                    \"inference\": {\n",
    "                        \"field_map\": {\"_ingest._value.text\": \"text_field\"},\n",
    "                        \"model_id\": elser_model_id,\n",
    "                        \"target_field\": \"_ingest._value.content_embedding\",\n",
    "                        \"on_failure\": [\n",
    "                            {\n",
    "                                \"append\": {\n",
    "                                    \"field\": \"_source._ingest.inference_errors\",\n",
    "                                    \"value\": [\n",
    "                                        {\n",
    "                                            \"message\": \"Processor 'inference' in pipeline 'ml-inference-title-vector' failed with message '{{ _ingest.on_failure_message }}'\",\n",
    "                                            \"pipeline\": \"ml-inference-title-vector\",\n",
    "                                            \"timestamp\": \"{{{ _ingest.timestamp }}}\",\n",
    "                                        }\n",
    "                                    ],\n",
    "                                }\n",
    "                            }\n",
    "                        ],\n",
    "                    }\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Create or update the pipeline\n",
    "pipeline_id = \"books_dataset_chunker\"\n",
    "esclient.ingest.put_pipeline(id=pipeline_id, body=pipeline_body)\n",
    "print(f\"Ingest pipeline '{pipeline_id}' created/updated successfully.\")"
   ],
   "metadata": {
    "id": "iUOFJK48OamP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Index Settings"
   ],
   "metadata": {
    "id": "6ZkRwEGdQBRP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 2,\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"default_pipeline\": \"books_dataset_chunker\",\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"dynamic\": \"false\",\n",
    "        \"properties\": {\n",
    "            \"book_title\": {\"type\": \"keyword\"},\n",
    "            \"chapter\": {\"type\": \"keyword\"},\n",
    "            \"chapter_full_text\": {\"type\": \"text\", \"index\": False},\n",
    "            \"passages\": {\n",
    "                \"type\": \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"content_embedding\": {\n",
    "                        \"properties\": {\n",
    "                            \"is_truncated\": {\"type\": \"boolean\"},\n",
    "                            \"model_id\": {\n",
    "                                \"type\": \"text\",\n",
    "                                \"fields\": {\n",
    "                                    \"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}\n",
    "                                },\n",
    "                            },\n",
    "                            \"predicted_value\": {\"type\": \"sparse_vector\"},\n",
    "                        }\n",
    "                    },\n",
    "                    \"text\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}},\n",
    "                    },\n",
    "                    \"vector\": {\n",
    "                        \"properties\": {\n",
    "                            \"is_truncated\": {\"type\": \"boolean\"},\n",
    "                            \"model_id\": {\n",
    "                                \"type\": \"text\",\n",
    "                                \"fields\": {\n",
    "                                    \"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}\n",
    "                                },\n",
    "                            },\n",
    "                            \"predicted_value\": {\n",
    "                                \"type\": \"dense_vector\",\n",
    "                                \"dims\": 384,\n",
    "                                \"index\": True,\n",
    "                                \"similarity\": \"dot_product\",\n",
    "                            },\n",
    "                        }\n",
    "                    },\n",
    "                    \"chunk_number\": {\"type\": \"integer\"},\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "raw_source_index_settings = {\n",
    "    \"settings\": {\"number_of_shards\": 2, \"number_of_replicas\": 0},\n",
    "    \"mappings\": {\n",
    "        \"dynamic\": \"false\",\n",
    "        \"properties\": {\n",
    "            \"book_title\": {\"type\": \"keyword\"},\n",
    "            \"chapter\": {\"type\": \"keyword\"},\n",
    "            \"chapter_full_text\": {\"type\": \"text\", \"index\": False},\n",
    "            \"passages\": {\n",
    "                \"type\": \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"text\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}},\n",
    "                    },\n",
    "                    \"chunk_number\": {\"type\": \"integer\"},\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# Manage indices\n",
    "manage_index(\n",
    "    esclient,\n",
    "    index_name,\n",
    "    index_settings[\"settings\"],\n",
    "    index_settings[\"mappings\"],\n",
    "    delete_index=True,\n",
    ")\n",
    "manage_index(\n",
    "    esclient,\n",
    "    raw_source_index,\n",
    "    raw_source_index_settings[\"settings\"],\n",
    "    raw_source_index_settings[\"mappings\"],\n",
    "    delete_index=True,\n",
    ")"
   ],
   "metadata": {
    "id": "vZ3Z5gZbOgjF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fetch and Process the Book Text\n",
    "\n",
    "This section downloads the full text of \"Harry Potter and the Sorcerer's Stone\" from a specified URL and processes it to extract chapters and their titles. The text is then structured into a pandas DataFrame for further analysis and indexing.\n",
    "\n",
    "### Key Steps:\n",
    "1. **Download Text**: The book is fetched using `urllib.request` from the provided URL.\n",
    "2. **Extract Chapters**: The text is split into chapters based on predefined patterns, omitting the text before the first chapter.\n",
    "3. **Capture Chapter Titles**: Chapter titles are extracted and paired with their respective texts.\n",
    "4. **Data Structuring**:\n",
    "   - Convert the list of chapter titles and texts into a DataFrame.\n",
    "   - Assign sequential numbers to chapters.\n",
    "   - Add the book title as metadata.\n",
    "   - Apply a text chunking function to split each chapter into manageable passages.\n",
    "\n",
    "This prepares the text data for efficient indexing and advanced search operations in Elasticsearch.\n"
   ],
   "metadata": {
    "id": "NPtbLhVOQUF3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Fetch and process the book text\n",
    "potter_book_url = \"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt\"\n",
    "response = urllib.request.urlopen(potter_book_url)\n",
    "harry_potter_book_text = response.read().decode(\"utf-8\")\n",
    "chapter_pattern = re.compile(r\"CHAPTER [A-Z]+\", re.IGNORECASE)\n",
    "chapters = chapter_pattern.split(harry_potter_book_text)[1:]\n",
    "chapter_titles = re.findall(chapter_pattern, harry_potter_book_text)\n",
    "chapters_with_titles = list(zip(chapter_titles, chapters))\n",
    "\n",
    "print(\"Total chapters found:\", len(chapters))\n",
    "if chapters_with_titles:\n",
    "    print(\"First chapter title:\", chapters_with_titles[0][0])\n",
    "    print(\"Text sample from first chapter:\", chapters_with_titles[0][1][:500])\n",
    "\n",
    "\n",
    "# Structuring chapters into a DataFrame\n",
    "df = pd.DataFrame(chapters_with_titles, columns=[\"chapter_title\", \"chapter_full_text\"])\n",
    "df[\"chapter\"] = df.index + 1\n",
    "df[\"book_title\"] = \"Harry Potter and the Sorcererâ€™s Stone\"\n",
    "df[\"passages\"] = df[\"chapter_full_text\"].apply(lambda text: chunk(text))"
   ],
   "metadata": {
    "id": "0L4YI96xOuKn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Indexing DataFrame into Elasticsearch\n",
    "\n",
    "This section uploads the structured data from a pandas DataFrame into a specified Elasticsearch index. The DataFrame contains chapter information from \"Harry Potter and the Sorcerer's Stone\", including chapter titles, full texts, and additional metadata.\n",
    "\n",
    "### Key Operation:\n",
    "- **Index Data**: The `index_dataframe` function is called with the Elasticsearch client, the raw source index name, and the DataFrame as arguments. This operation effectively uploads the data into Elasticsearch, making it searchable and ready for further processing.\n"
   ],
   "metadata": {
    "id": "DKK4574EQaTl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "index_dataframe(esclient, raw_source_index, df)"
   ],
   "metadata": {
    "id": "7ReLAtz1O1HF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Asynchronous Reindexing in Elasticsearch\n",
    "\n",
    "This section initiates an asynchronous reindex operation to transfer data from the raw source index to the enriched index in Elasticsearch. This process runs in the background, allowing other operations to continue without waiting for completion.\n",
    "\n",
    "### Key Steps:\n",
    "1. **Start Reindex**: The reindex operation is triggered from the `raw_source_index` to the `index_name`, with `wait_for_completion` set to `False` to allow asynchronous execution.\n",
    "2. **Retrieve Task ID**: The task ID of the reindex operation is captured and printed for monitoring purposes.\n",
    "3. **Monitor Progress**: The `check_task_status` function continuously checks the status of the reindex task, providing updates every 10 seconds until the operation is complete.\n"
   ],
   "metadata": {
    "id": "pA5QroYdQgcM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Start the reindex operation asynchronously\n",
    "response = esclient.reindex(\n",
    "    body={\"source\": {\"index\": raw_source_index}, \"dest\": {\"index\": index_name}},\n",
    "    wait_for_completion=False,\n",
    ")\n",
    "task_id = response[\"task\"]\n",
    "print(\"Task ID:\", task_id)\n",
    "check_task_status(esclient, task_id)"
   ],
   "metadata": {
    "id": "HOCX_lbmO3zl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom Search Query Construction and Execution\n",
    "\n",
    "This section constructs and executes a custom search query in Elasticsearch, utilizing a hybrid approach combining vector and text-based search methods to enhance search accuracy and relevance. The specific example used is a user query about the \"Nimbus 2000\".\n",
    "\n",
    "### Key Steps:\n",
    "1. **Define User Query**: The user query is specified as \"what is a nimbus 2000\".\n",
    "2. **Set Boost Factors**:\n",
    "   - `knn_boost_factor`: A value to amplify the importance of the vector-based search component.\n",
    "   - `text_expansion_boost`: A value to modify the weight of the text-based search component.\n",
    "3. **Build Query**: The `build_custom_query` function constructs the search query, incorporating both dense vector and text expansion components.\n",
    "4. **Execute Search**: The query is executed against the specified Elasticsearch index.\n",
    "5. **Identify Relevant Passages**:\n",
    "   - The search results are analyzed to find the passage with the highest relevance score.\n",
    "   - The ID and chunk number of the best matching passage are captured and printed.\n",
    "6. **Fetch Surrounding Chunks**: Constructs and executes a query to retrieve chunks adjacent to the identified passage for broader context. If the matched chunk is the first chunk, fetches n, n+1, and n+2. If the chunk is the last chunk in the chapter, fetches n, n-1, and n-2. For other chunks, fetches n-1, n, and n+1.\n",
    "7. **Display Results**: Outputs text from the relevant and adjacent passages."
   ],
   "metadata": {
    "id": "xJBDwRmDQq4n"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Custom Search Query Construction\n",
    "user_query = \"what is a nimbus 2000\"\n",
    "\n",
    "\n",
    "knn_boost_factor = 20\n",
    "text_expansion_boost = 1\n",
    "query = build_custom_query(\n",
    "    build_vector(user_query),\n",
    "    user_query,\n",
    "    knn_boost_factor,\n",
    "    text_expansion_boost,\n",
    "    debug=False,\n",
    ")\n",
    "\n",
    "# Searching and identifying relevant passages\n",
    "results = esclient.search(index=index_name, body=query, _source=False)\n",
    "\n",
    "hit_id = None\n",
    "chunk_number = None\n",
    "\n",
    "if results and results.get(\"hits\") and results[\"hits\"].get(\"hits\"):\n",
    "    highest_score = -1\n",
    "    best_hit = None\n",
    "    hit_id = results[\"hits\"][\"hits\"][0][\"_id\"]\n",
    "    chapter_number = results[\"hits\"][\"hits\"][0][\"fields\"][\"chapter\"][0]\n",
    "    if \"inner_hits\" in results[\"hits\"][\"hits\"][0]:\n",
    "        for hit_type in [\"text_hits\", \"dense_hit\", \"sparse_hits\"]:\n",
    "            if hit_type in results[\"hits\"][\"hits\"][0][\"inner_hits\"]:\n",
    "                inner_hit = results[\"hits\"][\"hits\"][0][\"inner_hits\"][hit_type][\"hits\"]\n",
    "                if inner_hit[\"hits\"]:\n",
    "                    max_score = inner_hit[\"max_score\"]\n",
    "                    if max_score and max_score > highest_score:\n",
    "                        highest_score = max_score\n",
    "                        best_hit = inner_hit[\"hits\"][0]\n",
    "\n",
    "    if best_hit:\n",
    "        first_passage_text = best_hit[\"_source\"][\"text\"]\n",
    "        chunk_number = best_hit[\"_source\"][\"chunk_number\"]\n",
    "        # print(f\"Matched Chunk ID: {hit_id}, Chunk Number: {chunk_number}, Text: {first_passage_text}\")\n",
    "        print(\n",
    "            f\"Matched Chunk ID: {hit_id}, Chunk Number: {chunk_number}, Text:\\n{textwrap.fill(first_passage_text, width=200)}\"\n",
    "        )\n",
    "        print(f\"\\n\")\n",
    "    else:\n",
    "        print(f\"ID: {hit_id}, No relevant passages found.\")\n",
    "else:\n",
    "    print(\"No results found.\")\n",
    "\n",
    "print(f\"Fetch Surrounding Chunks\")\n",
    "print(f\"------------------------\")\n",
    "\n",
    "max_chapter_chunk_result = esclient.search(\n",
    "    index=index_name,\n",
    "    body=get_max_chunk_number_query(chapter_number, debug=False),\n",
    "    _source=False,\n",
    ")\n",
    "max_chunk_number = max_chapter_chunk_result[\"aggregations\"][\"max_chunk_number\"][\n",
    "    \"max_chunk\"\n",
    "][\"value\"]\n",
    "\n",
    "adjacent_chunks_query = get_adjacent_chunks_query(\n",
    "    hit_id, chunk_number, max_chunk_number, debug=False\n",
    ")\n",
    "results = esclient.search(index=index_name, body=adjacent_chunks_query, _source=False)\n",
    "print_text_from_results(results)"
   ],
   "metadata": {
    "id": "u7NFZBRJO3t7"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}