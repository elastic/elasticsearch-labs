{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot with LangChain conversational chain and OpenAI ü§ñüí¨\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/elastic/elasticsearch-labs/blob/main/notebooks/generative-ai/chatbot.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "In this notebook we'll build a chatbot that can respond to questions about custom data, such as policies of an employer.\n",
    "\n",
    "The chatbot uses LangChain's `ConversationalRetrievalChain` and has the following capabilities:\n",
    "- Answer questions asked in natural language\n",
    "- Run hybrid search in Elasticsearch to find documents that answer the question\n",
    "- Extract and summarize the answer using OpenAI LLM\n",
    "- Maintain conversational memory for follow-up questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements üß∞\n",
    "For this example, you will need:\n",
    "\n",
    "- Python 3.6 or later\n",
    "- An Elastic deployment\n",
    "  - We'll be using [Elastic Cloud](https://www.elastic.co/guide/en/cloud/current/ec-getting-started.html) for this example (available with a [free trial](https://cloud.elastic.co/registration))\n",
    "- OpenAI account\n",
    "\n",
    "### Create Elastic Cloud deployment\n",
    "\n",
    "If you don't have an Elastic Cloud deployment, follow these steps to create one.\n",
    "1. Go to https://cloud.elastic.co/registration and sign up for a free trial\n",
    "2. Select **Create Deployment** and follow the instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages üì¶\n",
    "\n",
    "First we `pip install` the packages we need for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U langchain==0.0.245 openai elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize clients üîå\n",
    "\n",
    "Next we input credentials with `getpass`. `getpass` is part of the Python standard library and is used to securely prompt for credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID: \")\n",
    "ELASTIC_USERNAME = getpass(\"Elastic username: \")\n",
    "ELASTIC_PASSWORD = getpass(\"Elastic password: \")\n",
    "OPENAI_API_KEY = getpass(\"OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these credentials we can now initialize the Elasticsearch Python client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch\n",
      " {'name': 'instance-0000000000', 'cluster_name': 'dacecbd336e6450d882c8adef9d131eb', 'cluster_uuid': 'PIWjifvgTcW7iV5zJmu0PA', 'version': {'number': '8.9.0', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '8aa461beb06aa0417a231c345a1b8c38fb498a0d', 'build_date': '2023-07-19T14:43:58.555259655Z', 'build_snapshot': False, 'lucene_version': '9.7.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "elasticsearch_client = Elasticsearch(\n",
    "    cloud_id=ELASTIC_CLOUD_ID,\n",
    "    basic_auth=(ELASTIC_USERNAME, ELASTIC_PASSWORD)\n",
    ")\n",
    "\n",
    "print(\"Connected to Elasticsearch\\n\", elasticsearch_client.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create index üóÑÔ∏è\n",
    "\n",
    "We'll create an Elasticsearch index to store documents along with the generated vector embeddings. This will allow us to execute vector search when retrieving documents for our query.\n",
    "\n",
    "Since we're using OpenAI's `text-embedding-ada-002` model, we need a 1536-dimensional [dense_vector](https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html) field to store the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {\n",
    "    \"properties\": {\n",
    "        \"text\": { \"type\": \"keyword\" },\n",
    "        \"vector\": {\n",
    "            \"type\": \"dense_vector\",\n",
    "            \"dims\": 1536,\n",
    "            \"index\": True,\n",
    "            \"similarity\": \"cosine\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "elasticsearch_client.indices.create(\n",
    "    index=\"workplace-docs\",\n",
    "    mappings=mappings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process documents üìÑ\n",
    "\n",
    "Time to load some data! We'll be using the workplace search example data, which is a list of employee documents and policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 15 documents\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/elastic/elasticsearch-labs/main/example-apps/workplace-search/example-data/data.json\"\n",
    "\n",
    "response = urlopen(url)\n",
    "\n",
    "workplace_docs = json.loads(response.read())\n",
    "\n",
    "print(f\"Successfully loaded {len(workplace_docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk documents into passages ü™ì\n",
    "\n",
    "As we're chatting with our bot, it will run semantic searches on the index to find the relevant documents. In order for this to be accurate, we need to split the full documents into small chunks (also called passages). This way the semantic search will find the passage within a document that most likely answers our question.\n",
    "\n",
    "We'll use LangChain's `CharacterTextSplitter` and split the documents' text at 800 characters with some overlap between chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 866, which is longer than the specified 800\n",
      "Created a chunk of size 1120, which is longer than the specified 800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 15 documents into 73 passages\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "metadata = []\n",
    "content = []\n",
    "\n",
    "for doc in workplace_docs:\n",
    "    content.append(doc[\"content\"])\n",
    "    metadata.append({\n",
    "        \"name\": doc[\"name\"],\n",
    "        \"summary\": doc[\"summary\"]\n",
    "    })\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=400\n",
    ")\n",
    "docs = text_splitter.create_documents(content, metadatas=metadata)\n",
    "\n",
    "print(f\"Split {len(workplace_docs)} documents into {len(docs)} passages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate the embeddings and index the documents with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 73 embeddings\n",
      "Indexed 73 documents\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Get the embeddings from openAI\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Extract page_content from the documents\n",
    "texts = list(map(lambda t: t.page_content, docs))\n",
    "\n",
    "text_embeddings = embeddings.embed_documents(texts)\n",
    "\n",
    "print(f\"Generated {len(text_embeddings)} embeddings\")\n",
    "\n",
    "# Persist the passage documents into Elasticsearch\n",
    "actions = []\n",
    "for i, passage in enumerate(docs):\n",
    "    actions.append({\"index\": {}})\n",
    "    actions.append({\n",
    "        \"text\": passage.page_content,\n",
    "        \"vector\": text_embeddings[i],\n",
    "        \"metadata\": passage.metadata\n",
    "    })\n",
    "\n",
    "bulk_response = elasticsearch_client.bulk(\n",
    "    operations=actions,\n",
    "    index=\"workplace-docs\"\n",
    ")\n",
    "\n",
    "print(f\"Indexed {len(bulk_response['items'])} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with the chatbot üí¨\n",
    "\n",
    "Let's initialize our chatbot. We'll define Elasticsearch as a store for retrieving documents, OpenAI as the LLM to interpret questions and summarize answers, then we'll pass these to the conversational chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.elastic_vector_search import ElasticKnnSearch\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "store = ElasticKnnSearch(\n",
    "    es_connection=elasticsearch_client,\n",
    "    index_name=\"workplace-docs\",\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "retriever = store.as_retriever()\n",
    "\n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "chat = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can ask questions from our chatbot!\n",
    "\n",
    "See how the chat history is passed as context for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:  What does NASA stand for? \n",
      "ANSWER:    NASA stands for North America South America. \n",
      "SUPPORTING DOCUMENTS:  ['Sales Organization Overview', 'Code Of Conduct', 'Code Of Conduct', 'Swe Career Matrix']\n",
      "QUESTION:  Which countries are part of it? \n",
      "ANSWER:    The North America South America region includes the United States, Canada, Mexico, as well as Central and South America. \n",
      "SUPPORTING DOCUMENTS:  ['Sales Organization Overview', 'Sales Organization Overview', 'Sales Organization Overview', 'Fy2024 Company Sales Strategy']\n",
      "QUESTION:  Who are the team's leads? \n",
      "ANSWER:    Laura Martinez is the Area Vice-President of North America, and Gary Johnson is the Area Vice-President of South America. \n",
      "SUPPORTING DOCUMENTS:  ['Sales Organization Overview', 'Sales Organization Overview', 'Swe Career Matrix', 'Swe Career Matrix']\n"
     ]
    }
   ],
   "source": [
    "# Define a convenience function for Q&A\n",
    "def ask(question, history):\n",
    "    result = chat({\"question\": question, \"chat_history\": chat_history})\n",
    "    print(\"QUESTION: \", question,\n",
    "          \"\\nANSWER:  \", result[\"answer\"],\n",
    "          \"\\nSUPPORTING DOCUMENTS: \", list(map(lambda d: d.metadata[\"name\"], list(result[\"source_documents\"])))\n",
    "    )\n",
    "    history.append((question, result[\"answer\"]))\n",
    "    \n",
    "chat_history = []\n",
    "\n",
    "ask(\"What does NASA stand for?\", chat_history)\n",
    "ask(\"Which countries are part of it?\", chat_history)\n",
    "ask(\"Who are the team's leads?\", chat_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° _Try experimenting with other questions or after clearing the workplace data, and observe how the responses change._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Clean up üßπ\n",
    "\n",
    "Once we're done, we can delete the Elasticsearch index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticsearch_client.indices.delete(index='workplace-docs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
