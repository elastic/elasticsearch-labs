{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot with LangChain conversational chain and OpenAI ðŸ¤–ðŸ’¬\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/elastic/elasticsearch-labs/blob/main/notebooks/generative-ai/chatbot.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "In this notebook we'll build a chatbot that can respond to questions about custom data, such as policies of an employer.\n",
    "\n",
    "The chatbot uses LangChain's `ConversationalRetrievalChain` and has the following capabilities:\n",
    "- Answer questions asked in natural language\n",
    "- Run hybrid search in Elasticsearch to find documents that answer the question\n",
    "- Extract and summarize the answer using OpenAI LLM\n",
    "- Maintain conversational memory for follow-up questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements ðŸ§°\n",
    "For this example, you will need:\n",
    "\n",
    "- Python 3.6 or later\n",
    "- An Elastic deployment\n",
    "  - We'll be using [Elastic Cloud](https://www.elastic.co/guide/en/cloud/current/ec-getting-started.html) for this example (available with a [free trial](https://cloud.elastic.co/registration))\n",
    "- OpenAI account\n",
    "\n",
    "### Create Elastic Cloud deployment\n",
    "\n",
    "If you don't have an Elastic Cloud deployment, follow these steps to create one.\n",
    "1. Go to https://cloud.elastic.co/registration and sign up for a free trial\n",
    "2. Select **Create Deployment** and follow the instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages ðŸ“¦\n",
    "\n",
    "First we `pip install` the packages we need for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (0.0.139)\n",
      "Collecting langchain\n",
      "  Obtaining dependency information for langchain from https://files.pythonhosted.org/packages/f2/42/01aa982897eb3829f04e9a8c1c761e2a61ae944a0cbc90bcc978fe3c02ae/langchain-0.0.271-py3-none-any.whl.metadata\n",
      "  Downloading langchain-0.0.271-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: openai in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (0.27.4)\n",
      "Collecting openai\n",
      "  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/a0/19/7e39855c63b68ef15b1c2c9e4d941026fb0c94940979e161bb5b470f8455/openai-0.27.9-py3-none-any.whl.metadata\n",
      "  Downloading openai-0.27.9-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: elasticsearch in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (8.9.0)\n",
      "Requirement already satisfied: tiktoken in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from langchain) (1.4.47)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from langchain) (0.5.7)\n",
      "Collecting langsmith<0.1.0,>=0.0.21 (from langchain)\n",
      "  Obtaining dependency information for langsmith<0.1.0,>=0.0.21 from https://files.pythonhosted.org/packages/94/af/c01937934a8d6e5e4f620a9c6013dc8b0d1a9f02a2a183761426f5ea67b6/langsmith-0.0.26-py3-none-any.whl.metadata\n",
      "  Downloading langsmith-0.0.26-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.4 (from langchain)\n",
      "  Obtaining dependency information for numexpr<3.0.0,>=2.8.4 from https://files.pythonhosted.org/packages/56/ed/ee046f0cd9d9a01002f4c2efa5f78dcb9e4e7a078d88ea182a79e8ef1ce3/numexpr-2.8.5-cp310-cp310-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached numexpr-2.8.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from langchain) (1.24.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from langchain) (1.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from langchain) (2.30.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: tqdm in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: elastic-transport<9,>=8 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from elasticsearch) (8.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from tiktoken) (2023.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: urllib3<2,>=1.26.2 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from elastic-transport<9,>=8->elasticsearch) (1.26.15)\n",
      "Requirement already satisfied: certifi in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from elastic-transport<9,>=8->elasticsearch) (2022.12.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/joe/.pyenv/versions/3.10.3/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Downloading langchain-0.0.271-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-0.27.9-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.0.26-py3-none-any.whl (34 kB)\n",
      "Using cached numexpr-2.8.5-cp310-cp310-macosx_11_0_arm64.whl (90 kB)\n",
      "Installing collected packages: numexpr, langsmith, openai, langchain\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.27.4\n",
      "    Uninstalling openai-0.27.4:\n",
      "      Successfully uninstalled openai-0.27.4\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.0.139\n",
      "    Uninstalling langchain-0.0.139:\n",
      "      Successfully uninstalled langchain-0.0.139\n",
      "Successfully installed langchain-0.0.271 langsmith-0.0.26 numexpr-2.8.5 openai-0.27.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain openai elasticsearch tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize clients ðŸ”Œ\n",
    "\n",
    "Next we input credentials with `getpass`. `getpass` is part of the Python standard library and is used to securely prompt for credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID: \")\n",
    "ELASTIC_USERNAME = getpass(\"Elastic username: \")\n",
    "ELASTIC_PASSWORD = getpass(\"Elastic password: \")\n",
    "OPENAI_API_KEY = getpass(\"OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process documents ðŸ“„\n",
    "\n",
    "Time to load some data! We'll be using the workplace search example data, which is a list of employee documents and policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 15 documents\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/elastic/elasticsearch-labs/main/notebooks/generative-ai/data/workplace-docs.json\"\n",
    "\n",
    "response = urlopen(url)\n",
    "\n",
    "workplace_docs = json.loads(response.read())\n",
    "\n",
    "print(f\"Successfully loaded {len(workplace_docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk documents into passages ðŸª“\n",
    "\n",
    "As we're chatting with our bot, it will run semantic searches on the index to find the relevant documents. In order for this to be accurate, we need to split the full documents into small chunks (also called passages). This way the semantic search will find the passage within a document that most likely answers our question.\n",
    "\n",
    "We'll use LangChain's `CharacterTextSplitter` and split the documents' text at 800 characters with some overlap between chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 866, which is longer than the specified 800\n",
      "Created a chunk of size 1120, which is longer than the specified 800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 15 documents into 73 passages\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "metadata = []\n",
    "content = []\n",
    "\n",
    "for doc in workplace_docs:\n",
    "    content.append(doc[\"content\"])\n",
    "    metadata.append({\n",
    "        \"name\": doc[\"name\"],\n",
    "        \"summary\": doc[\"summary\"]\n",
    "    })\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=400\n",
    ")\n",
    "docs = text_splitter.create_documents(content, metadatas=metadata)\n",
    "\n",
    "print(f\"Split {len(workplace_docs)} documents into {len(docs)} passages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate the embeddings and index the documents with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import ElasticsearchStore\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "vector_store = ElasticsearchStore.from_documents(\n",
    "    docs,\n",
    "    es_cloud_id=ELASTIC_CLOUD_ID, \n",
    "    es_user=ELASTIC_USERNAME, \n",
    "    es_password=ELASTIC_PASSWORD,\n",
    "    index_name=\"workplace_index\",\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with the chatbot ðŸ’¬\n",
    "\n",
    "Let's initialize our chatbot. We'll define Elasticsearch as a store for retrieving documents and for storing the chat session history, OpenAI as the LLM to interpret questions and summarize answers, then we'll pass these to the conversational chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from lib.elasticsearch_chat_message_history import ElasticsearchChatMessageHistory\n",
    "from uuid import uuid4\n",
    "\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "chat = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "session_id = str(uuid4())\n",
    "chat_history = ElasticsearchChatMessageHistory(\n",
    "    client=vector_store.client,\n",
    "    session_id=session_id,\n",
    "    index=\"workplace-docs-chat-history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can ask questions from our chatbot!\n",
    "\n",
    "See how the chat history is passed as context for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHAT SESSION ID] 75557767-2a26-4cc0-9860-1d4868e2449b\n",
      "[QUESTION] What does NASA stand for?\n",
      "[ANSWER]   NASA stands for North America South America.\n",
      "          [SUPPORTING DOCUMENTS] ['Sales Organization Overview', 'Sales Organization Overview', 'Code Of Conduct', 'Code Of Conduct']\n",
      "[QUESTION] Which countries are part of it?\n",
      "[ANSWER]   North America includes the United States, Canada, and Mexico. South America includes countries such as Brazil, Argentina, Peru, and Colombia.\n",
      "          [SUPPORTING DOCUMENTS] ['Sales Organization Overview', 'Sales Organization Overview', 'Sales Organization Overview', 'Sales Organization Overview']\n",
      "[QUESTION] Who are the team's leads?\n",
      "[ANSWER]   Laura Martinez is the Area Vice-President of North America, and Gary Johnson is the Area Vice-President of South America.\n",
      "          [SUPPORTING DOCUMENTS] ['Sales Organization Overview', 'Sales Organization Overview', 'Swe Career Matrix', 'Swe Career Matrix']\n"
     ]
    }
   ],
   "source": [
    "# Define a convenience function for Q&A\n",
    "def ask(question, chat_history):\n",
    "    result = chat({\"question\": question, \"chat_history\": chat_history.messages})\n",
    "    print(f\"\"\"[QUESTION] {question}\n",
    "[ANSWER]  {result[\"answer\"]}\n",
    "          [SUPPORTING DOCUMENTS] {list(map(lambda d: d.metadata[\"name\"], list(result[\"source_documents\"])))}\"\"\")\n",
    "    chat_history.add_user_message(result[\"question\"])\n",
    "    chat_history.add_ai_message(result[\"answer\"])\n",
    "\n",
    "# Chat away!\n",
    "print(f\"[CHAT SESSION ID] {session_id}\")\n",
    "ask(\"What does NASA stand for?\", chat_history)\n",
    "ask(\"Which countries are part of it?\", chat_history)\n",
    "ask(\"Who are the team's leads?\", chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ _Try experimenting with other questions or after clearing the workplace data, and observe how the responses change._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Clean up ðŸ§¹\n",
    "\n",
    "Once we're done, we can clean up the chat history for this session..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or delete the indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.client.indices.delete(index='workplace_index')\n",
    "vector_store.client.indices.delete(index='workplace-docs-chat-history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
