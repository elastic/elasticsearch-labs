{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# NLP text search\n",
    "The workbook implements NLP text search in Elasticsearch using the Elastic blogs dataset.\n",
    "\n",
    "You will index blogs documents, and using ingest pipeline generate text embeddings. By using NLP model you will query the documents using natural language over the the blogs documents.\n",
    "\n",
    "The dataset is a ~4000 blog posts from [elastic.co/blog](https://elastic.co/blog).\n",
    "\n",
    "### Prerequisities\n",
    "\n",
    "Before you start make sure you have Elasticsearch cluster running. The cluster must have at least one machine learning (ML) node with enough (4GB) memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdzl8tmZfr3y"
   },
   "source": [
    "### Install Python requirements\n",
    "Before you start you need to install all required Python dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9NM_6fGFURcz",
    "outputId": "53f1a78c-db7f-468e-c6af-bdf9be554473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting eland\n",
      "  Downloading eland-8.7.0-py3-none-any.whl (152 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting elasticsearch\n",
      "  Downloading elasticsearch-8.9.0-py3-none-any.whl (395 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.5/395.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.2+cu118)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
      "Collecting sentencepiece (from sentence-transformers)\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.10/dist-packages (from eland) (1.5.3)\n",
      "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.10/dist-packages (from eland) (3.7.1)\n",
      "Collecting elastic-transport<9,>=8 (from elasticsearch)\n",
      "  Downloading elastic_transport-8.4.0-py3-none-any.whl (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
      "Requirement already satisfied: urllib3<2,>=1.26.2 in /usr/local/lib/python3.10/dist-packages (from elastic-transport<9,>=8->elasticsearch) (1.26.16)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from elastic-transport<9,>=8->elasticsearch) (2023.7.22)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->eland) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->eland) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->eland) (4.41.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->eland) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->eland) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->eland) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->eland) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->eland) (2022.7.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.6)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->eland) (1.16.0)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=7d7961f303bed47921c998a34b4beda11c9cdefa2076fbf800d0c89c1073517e\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: tokenizers, sentencepiece, safetensors, elastic-transport, huggingface-hub, elasticsearch, transformers, eland, sentence-transformers\n",
      "Successfully installed eland-8.7.0 elastic-transport-8.4.0 elasticsearch-8.9.0 huggingface-hub-0.16.4 safetensors-0.3.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.31.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers eland elasticsearch transformers torch tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKU9L8o2FodV"
   },
   "source": [
    "### Upload NLP model for querying\n",
    "Using an `eland` tool you will import required NLP models. One model will transfer your search query into vector which will be used for the search over the set of documents stored in Elasticsearch.\n",
    "\n",
    "Model used:\n",
    "- [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) to transform the search text into the vector\n",
    "\n",
    "How to get cloud id? Go to [ESS cloud](https://cloud.elastic.co/logout?redirectTo=%2Fhome&reason=unauthorised) and `On the deployment overview page, copy down the Cloud ID.`\n",
    "\n",
    "The authentication is using api key (`--es-api-key`). Learn how to generate [API key](https://www.elastic.co/guide/en/kibana/current/api-keys.html#create-api-key). Another option is to use username/password.\n",
    "\n",
    "```\n",
    "$ eland_import_hub_model --cloud-id $CLOUD_ID \\\n",
    "  --hub-model-id sentence-transformers/all-MiniLM-L6-v2 \\\n",
    "  --task-type text_embedding --es-api-key $API_KEY --start\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vQkKn02j_FfJ",
    "outputId": "99b9ffd4-6780-4167-dbd7-e0264369e40e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-01 14:24:13,344 INFO : Establishing connection to Elasticsearch\n",
      "2023-08-01 14:24:13,923 INFO : Connected to cluster named 'a597bbe1e0d047c494e7d4015f67ef37' (version: 8.8.2)\n",
      "2023-08-01 14:24:13,924 INFO : Loading HuggingFace transformer tokenizer and model 'sentence-transformers/all-MiniLM-L6-v2'\n",
      "Downloading (…)okenizer_config.json: 100% 350/350 [00:00<00:00, 1.84MB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 3.98MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100% 112/112 [00:00<00:00, 714kB/s]\n",
      "Downloading (…)lve/main/config.json: 100% 612/612 [00:00<00:00, 3.15MB/s]\n",
      "Downloading pytorch_model.bin: 100% 90.9M/90.9M [00:00<00:00, 198MB/s]\n",
      "Downloading (…)e9125/.gitattributes: 100% 1.18k/1.18k [00:00<00:00, 6.64MB/s]\n",
      "Downloading (…)_Pooling/config.json: 100% 190/190 [00:00<00:00, 1.07MB/s]\n",
      "Downloading (…)7e55de9125/README.md: 100% 10.6k/10.6k [00:00<00:00, 37.5MB/s]\n",
      "Downloading (…)55de9125/config.json: 100% 612/612 [00:00<00:00, 2.35MB/s]\n",
      "Downloading (…)ce_transformers.json: 100% 116/116 [00:00<00:00, 734kB/s]\n",
      "Downloading (…)125/data_config.json: 100% 39.3k/39.3k [00:00<00:00, 87.4MB/s]\n",
      "Downloading pytorch_model.bin: 100% 90.9M/90.9M [00:00<00:00, 243MB/s]\n",
      "Downloading (…)nce_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 303kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100% 112/112 [00:00<00:00, 646kB/s]\n",
      "Downloading (…)e9125/tokenizer.json: 100% 466k/466k [00:00<00:00, 3.70MB/s]\n",
      "Downloading (…)okenizer_config.json: 100% 350/350 [00:00<00:00, 1.58MB/s]\n",
      "Downloading (…)9125/train_script.py: 100% 13.2k/13.2k [00:00<00:00, 56.0MB/s]\n",
      "Downloading (…)7e55de9125/vocab.txt: 100% 232k/232k [00:00<00:00, 92.5MB/s]\n",
      "Downloading (…)5de9125/modules.json: 100% 349/349 [00:00<00:00, 2.09MB/s]\n",
      "2023-08-01 14:24:25,325 INFO : Creating model with id 'sentence-transformers__all-minilm-l6-v2'\n",
      "2023-08-01 14:24:25,477 INFO : Uploading model definition\n",
      "100% 22/22 [00:24<00:00,  1.13s/ parts]\n",
      "2023-08-01 14:24:50,386 INFO : Uploading model vocabulary\n",
      "2023-08-01 14:24:50,870 INFO : Starting model deployment\n",
      "2023-08-01 14:24:53,639 INFO : Model successfully imported with id 'sentence-transformers__all-minilm-l6-v2'\n"
     ]
    }
   ],
   "source": [
    "API_KEY=''\n",
    "CLOUD_ID=''\n",
    "!eland_import_hub_model --cloud-id $CLOUD_ID --hub-model-id sentence-transformers/all-MiniLM-L6-v2 --task-type text_embedding --es-api-key $API_KEY --start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqNkx-WiJePa"
   },
   "source": [
    "### Import Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6YwgyXRJoyb"
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import zipfile\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGfkUDWDMkc4"
   },
   "source": [
    "### Connect to Elasticsearch cluster\n",
    "Use your own cluster details `ELASTIC_CLOUD_ID`, `API_KEY`.\n",
    "\n",
    "You also have another option to authentication (Username/Password) if you wish do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGi175RbJhVQ",
    "outputId": "14e7f403-06ba-4fa7-a3f4-4bcc271a1f5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Cloud ID: ··········\n",
      "Elastic API key: ··········\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': 'instance-0000000000', 'cluster_name': 'a597bbe1e0d047c494e7d4015f67ef37', 'cluster_uuid': 'EnT0vwwSSZeAahPw3Vhsuw', 'version': {'number': '8.8.2', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '98e1271edf932a480e4262a471281f1ee295ce6b', 'build_date': '2023-06-26T05:16:16.196344851Z', 'build_snapshot': False, 'lucene_version': '9.6.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ESS Cloud connection definition using an API_KEY\n",
    "ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID: \")\n",
    "API_KEY = getpass(\"Elastic API key: \")\n",
    "\n",
    "# ELASTIC_CLOUD_USER = \"elastic\"\n",
    "# CLOUD_PASSWORD = getpass(\"Elastic Password\")\n",
    "\n",
    "es = Elasticsearch(\n",
    "  cloud_id=ELASTIC_CLOUD_ID,\n",
    "  #basic_auth=(ELASTIC_CLOUD_USER, ELASTIC_CLOUD_PASSWORD),\n",
    "  api_key=API_KEY,\n",
    "  request_timeout=600\n",
    ")\n",
    "\n",
    "es.info() # should return cluster info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FoZ5TBrIqOT"
   },
   "source": [
    "### Upload Ingest pipeline\n",
    "You need to upload the ingest pipeline which will generate vector (text) embeddings for selected field.\n",
    "\n",
    "The pipeline below is defining a processor for the inference to the correct NLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "geY7WLh7Ky-k",
    "outputId": "97c17b36-94d6-454f-b976-4c20e8e49edc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ingest pipeline definition\n",
    "PIPELINE_ID=\"vectorize_blogs\"\n",
    "PIPELINE_DESC=\"This is the description for document procesing\"\n",
    "PIPELINE_VERSION=3\n",
    "\n",
    "es.ingest.put_pipeline(id=PIPELINE_ID, description=PIPELINE_DESC, version=PIPELINE_VERSION,\n",
    "                       processors=[{\n",
    "        \"inference\": {\n",
    "          \"model_id\": \"sentence-transformers__all-minilm-l6-v2\",\n",
    "          \"target_field\": \"text_embedding\",\n",
    "          \"field_map\": {\n",
    "            \"body_content_window\": \"text_field\"\n",
    "          }\n",
    "        }\n",
    "      }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IW-GIlH2OxB4"
   },
   "source": [
    "### Create Index and mappings for Blogs\n",
    "Befor you can index documents into Elasticsearch, you need to create an Index with correct mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xAkc1OVcOxy3",
    "outputId": "b2453634-89b8-48bc-ac65-a6a1c3b8170f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index blogs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-8366c643361b>:93: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  es.indices.create(index=INDEX_NAME, mappings=INDEX_MAPPING, settings=INDEX_SETTINGS,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'blogs'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDEX_NAME=\"blogs\"\n",
    "DELETE_INDEX=True\n",
    "\n",
    "INDEX_MAPPING = {\n",
    "    \"properties\": {\n",
    "      \"body_content_window\": {\n",
    "        \"type\": \"text\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"byline\": {\n",
    "        \"type\": \"text\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"name\": {\n",
    "        \"type\": \"text\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"publish_date\": {\n",
    "        \"type\": \"date\"\n",
    "      },\n",
    "      \"title\": {\n",
    "        \"type\": \"text\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"url\": {\n",
    "        \"type\": \"text\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"text_embedding\": {\n",
    "        \"properties\": {\n",
    "          \"is_truncated\": {\n",
    "            \"type\": \"boolean\"\n",
    "          },\n",
    "          \"model_id\": {\n",
    "            \"type\": \"text\",\n",
    "            \"fields\": {\n",
    "              \"keyword\": {\n",
    "                \"type\": \"keyword\",\n",
    "                \"ignore_above\": 256\n",
    "              }\n",
    "            }\n",
    "          },\n",
    "          \"predicted_value\": {\n",
    "            \"type\": \"dense_vector\",\n",
    "            \"dims\": 384,\n",
    "            \"index\": True,\n",
    "            \"similarity\": \"l2_norm\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "INDEX_SETTINGS = {\n",
    "    \"index\": {\n",
    "      \"number_of_replicas\": \"1\",\n",
    "      \"number_of_shards\": \"1\",\n",
    "      \"default_pipeline\": PIPELINE_ID\n",
    "    }\n",
    "}\n",
    "\n",
    "if(DELETE_INDEX):\n",
    "  if es.indices.exists(index=INDEX_NAME):\n",
    "    print(\"Deleting existing %s\" % INDEX_NAME)\n",
    "    es.indices.delete(index=INDEX_NAME, ignore=[400, 404])\n",
    "\n",
    "print(\"Creating index %s\" % INDEX_NAME)\n",
    "es.indices.create(index=INDEX_NAME, mappings=INDEX_MAPPING, settings=INDEX_SETTINGS,\n",
    "                  ignore=[400, 404])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKE-j0kPUMn_"
   },
   "source": [
    "### Get blogs dataset\n",
    "Download:\n",
    "- The example blogs dataset is located in [GitHub](https://raw.githubusercontent.com/elastic/elasticsearch-labs/tree/main/notebooks/integrations/hugging-face/2023-07-27-blogs-no-embeddings.json.zip)\n",
    "\n",
    "Then unzip the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QKrykunW35y"
   },
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!wget https://raw.githubusercontent.com/elastic/elasticsearch-labs/tree/main/notebooks/integrations/hugging-face/2023-07-27-blogs-no-embeddings.json.zip -P data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2YBi3TxJXgGP",
    "outputId": "0fa8136b-aeb6-49f1-81f6-41474b1d5d96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting file  2023-07-27-blogs-no-embeddings.json.zip .\n"
     ]
    }
   ],
   "source": [
    "# Unzip the file\n",
    "BLOGS_ZIP_FILE=\"data/2023-07-27-blogs-no-embeddings.json.zip\"\n",
    "with zipfile.ZipFile(BLOGS_ZIP_FILE, 'r') as zip_ref:\n",
    "  print('Extracting file ', BLOGS_ZIP_FILE, '.')\n",
    "  zip_ref.extractall('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOGsvnGveAoP"
   },
   "source": [
    "Let's index blogs using the ingest pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-qy8iE5eEy5",
    "outputId": "386ae640-26ae-48f1-ead5-d07f7bb329e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 200 documents\n",
      "Indexed 400 documents\n",
      "Indexed 600 documents\n",
      "Indexed 800 documents\n",
      "Indexed 1000 documents\n",
      "Indexed 1200 documents\n",
      "Indexed 1400 documents\n",
      "Indexed 1600 documents\n",
      "Indexed 1800 documents\n",
      "Indexed 2000 documents\n",
      "Indexed 2200 documents\n",
      "Indexed 2400 documents\n",
      "Indexed 2600 documents\n",
      "Indexed 2800 documents\n",
      "Indexed 3000 documents\n",
      "Indexed 3200 documents\n",
      "Indexed 3400 documents\n",
      "Indexed 3600 documents\n",
      "Indexed 3800 documents\n",
      "Indexed 4000 documents\n",
      "Indexed 4200 documents\n",
      "Indexed 4400 documents\n",
      "Indexed 4600 documents\n",
      "Indexed 4800 documents\n",
      "Indexed 5000 documents\n",
      "Indexed 5200 documents\n",
      "Indexed 5400 documents\n",
      "Indexed 5600 documents\n",
      "Indexed 5800 documents\n",
      "Indexed 6000 documents\n",
      "Indexed 6200 documents\n",
      "Indexed 6400 documents\n",
      "Indexed 6600 documents\n",
      "Indexed 6800 documents\n",
      "Indexed 7000 documents\n",
      "Indexed 7200 documents\n",
      "Indexed 7400 documents\n",
      "Indexed 7600 documents\n",
      "Indexed 7800 documents\n",
      "Indexed 8000 documents\n",
      "Indexed 8200 documents\n",
      "Indexed 8400 documents\n",
      "Indexed 8600 documents\n",
      "Indexed 8800 documents\n",
      "Indexed 9000 documents\n",
      "Indexed 9200 documents\n",
      "Indexed 9400 documents\n",
      "Indexed 9600 documents\n",
      "Indexed 9800 documents\n",
      "Indexed 10000 documents\n",
      "Indexed 10200 documents\n",
      "Indexed 10400 documents\n",
      "Indexed 10600 documents\n",
      "Indexed 10800 documents\n",
      "Indexed 11000 documents\n",
      "Indexed 11200 documents\n",
      "Indexed 11400 documents\n",
      "Indexed 11600 documents\n",
      "Indexed 11800 documents\n",
      "Indexed 12000 documents\n",
      "Indexed 12200 documents\n",
      "Indexed 12400 documents\n",
      "Indexed 12600 documents\n",
      "Indexed 12800 documents\n",
      "Indexed 13000 documents\n",
      "Indexed 13200 documents\n",
      "Indexed 13400 documents\n",
      "Indexed 13600 documents\n",
      "Indexed 13800 documents\n",
      "Indexed 14000 documents\n",
      "Indexed 14200 documents\n",
      "Indexed 14400 documents\n",
      "Indexed 14600 documents\n",
      "Indexed 14800 documents\n",
      "Indexed 15000 documents\n",
      "Indexed 15200 documents\n",
      "Indexed 15400 documents\n",
      "Indexed 15600 documents\n",
      "Indexed 15800 documents\n",
      "Indexed 16000 documents\n",
      "Indexed 16200 documents\n",
      "Indexed 16400 documents\n",
      "Indexed 16600 documents\n",
      "Indexed 16800 documents\n",
      "Indexed 17000 documents\n",
      "Indexed 17200 documents\n",
      "Indexed 17400 documents\n",
      "Indexed 17600 documents\n",
      "Indexed 17800 documents\n",
      "Indexed 18000 documents\n",
      "Indexed 18200 documents\n",
      "Indexed 18400 documents\n",
      "Indexed 18600 documents\n",
      "Indexed 18800 documents\n",
      "Indexed 19000 documents\n",
      "Indexed 19200 documents\n",
      "Indexed 19400 documents\n",
      "Indexed 19600 documents\n",
      "Indexed 19717 blogs embeddings documents\n"
     ]
    }
   ],
   "source": [
    "BLOGS_FILE=\"data/2023-07-27-blogs-no-embeddings.json\"\n",
    "df_blogs_embeddings = pd.read_json(BLOGS_FILE, lines=True)\n",
    "\n",
    "def gen_rows(df):\n",
    "  for doc in df.to_dict(orient='records'):\n",
    "    yield doc\n",
    "\n",
    "count = 0\n",
    "for success, info in parallel_bulk(\n",
    "        client=es,\n",
    "        actions=gen_rows(df_blogs_embeddings),\n",
    "        thread_count=5,\n",
    "        chunk_size=200,\n",
    "        timeout='%ss' % 600,\n",
    "        index=INDEX_NAME\n",
    "):\n",
    "  if success:\n",
    "    count += 1\n",
    "    if count % 200 == 0:\n",
    "      print('Indexed %s documents' % str(count), flush=True)\n",
    "      sys.stdout.flush()\n",
    "  else:\n",
    "    print(info)\n",
    "    print('Doc failed', info)\n",
    "\n",
    "print('Indexed %s blogs embeddings documents' % str(count), flush=True)\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPPHg8K8T3wY"
   },
   "source": [
    "### Query the dataset\n",
    "The next step is to run a query to search for relevant blogs. The example query searches for `\"model_text\": \"how to do image similarity search\"` using the model we uploaded to Elasticsearch `sentence-transformers__all-minilm-l6-v2`.\n",
    "\n",
    "The process is one query even it internally consists of two tasks. One is to transform your search text into a vector using the NLP model and the second task is to run the vector search over the dataset.\n",
    "\n",
    "```\n",
    "POST blogs/_search\n",
    "{\n",
    "  \"knn\": {\n",
    "  \"field\": \"text_embedding.predicted_value\",\n",
    "  \"k\": 10,\n",
    "  \"num_candidates\": 50,\n",
    "  \"query_vector_builder\": {\n",
    "    \"text_embedding\": {\n",
    "      \"model_id\": \"sentence-transformers__all-minilm-l6-v2\",\n",
    "      \"model_text\": \"how to do image similarity search\"\n",
    "    }\n",
    "  },\n",
    "  \"fields\": [\n",
    "    \"body_content_window\",\n",
    "    \"id\",\n",
    "    \"title\",\n",
    "    \"url\"\n",
    "  ],\n",
    "  \"_source\": false\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 566
    },
    "id": "c4G5V9wmU9C5",
    "outputId": "c8f0cc24-5713-4560-8a5d-c42da562a670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'took': 13, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 10, 'relation': 'eq'}, 'max_score': 0.5594575, 'hits': [{'_index': 'blogs', '_id': 'MbiqsYkBLg4Kd5ySdDxp', '_score': 0.5594575, '_ignored': ['body_content_window.keyword'], 'fields': {'body_content_window': [' GET my-image-embeddings/_search { \"knn\": { \"field\": \"image_embedding\", \"k\": 5, \"num_candidates\": 10, \"query_vector\": [ -0.19898493587970734, 0.1074572503566742, -0.05087625980377197, ... 0.08200495690107346, -0.07852292060852051 ] }, \"fields\": [ \"image_id\", \"image_name\", \"relative_path\" ], \"_source\": false } Read more The response from Elasticsearch will give you the best matching images based on our kNN search query, stored in Elastic as documents. The flow graph below summarizes the steps your interactive application moves through while processing a user query: Load the interactive application, its front end. The user selects an image they’re interested in. Your application vectorizes the image by applying the CLIP model, storing the resulting embedding as dense vector. The application initiates a kNN query in Elasticsearch, which takes the embedding and returns its nearest neighbors. Your application processes the response and renders one (or more) matching images. Now that you understand the main components and information flow required to implement an interactive image similarity search, you can walk through the final part of the series to learn how to make it happen. You’ll get a step-by-step guide on how to set up the application environment, import the NLP model, and finally complete the image embedding generation. Then you will be able to search through images with natural language — no keywords required. Start setting up image similarity search >> Share Share on Twitter Share on LinkedIn Share on Facebook Share by email Print Table of contents Sign up for Elastic Cloud free trial Spin up a fully loaded deployment on the cloud provider you choose. As the company behind Elasticsearch , we bring our features and support to your Elastic clusters in the cloud. Start free trial Products & Solutions Search Observability Security Elastic Stack Elasticsearch Kibana Integrations Subscriptions Pricing Resources Documentation What is the ELK Stack? What is Elasticsearch?'], 'id': ['64b9368de761e3623d493ea9'], 'title': ['5 Technical components of image similarity search | Elastic.co | Elastic Blog'], 'url': ['https://www.elastic.co/blog/5-technical-components-image-similarity-search']}}, {'_index': 'blogs', '_id': '3birsYkBLg4Kd5ySxT6o', '_score': 0.54852164, '_ignored': ['body_content_window.keyword'], 'fields': {'body_content_window': [' Compared to traditional text search (in Elastic, based on BM25 scoring ), vector search yields more relevant results and executes faster (without the need of extreme search engine optimizations). This approach works not only with text data but also images and other types of unstructured data for which generic embedding models are available. For text data, it is commonly referred to as semantic search , while similarity search is frequently used in the context of images and audio. How do you generate vector embeddings for images? Vector embeddings are the numeric representation of data and related context stored in high dimensional (dense) vectors. Models that generate embeddings are typically trained on millions of examples to deliver more relevant and accurate results. For text data, BERT-like transformers are popular to generate embeddings that work with many types of text, and they are available on public repositories like Hugging Face. Embedding models, which work well on any type of image, are a subject of ongoing research. The CLIP model — used by our teams to prototype the image similarity app — is distributed by OpenAI and provides a good starting point. For specialized use cases and advanced users, you may need to train a custom embedding model to achieve desired performance. Next, you need the ability to search efficiently . Elastic supports the widely adopted HNSW-based approximate nearest neighbor search . SignUpCTA A search toolkit for the AI era The Elasticsearch Relevance Engine (ESRE) gives developers the tools they need to build AI-powered search apps. Build Generative AI Search Engines and Applications How similarity search powers innovative applications How does similarity search power innovation? In our first example, users could take a screenshot and search to find a favorite celebrity’s outfit. You can also use similarity search to: Suggest products that are similar to what other shoppers purchased. Find related existing designs or relevant templates from a library of visual design elements. Find songs you may like from popular music streaming services based on what you listened to recently. Search through huge datasets of unstructured and untagged images using the natural description. Learn more about what powers image similarity search >> Architecture overview of image similarity app Making this kind of interactive application can seem complex. That’s especially true if you have been considering implementation within a traditional architecture as shown below.'], 'id': ['64b93618e761e30894486814'], 'title': ['Overview of image similarity search | Elastic.co | Elastic Blog'], 'url': ['https://www.elastic.co/blog/overview-image-similarity-search-in-elastic']}}, {'_index': 'blogs', '_id': '3LirsYkBLg4Kd5ySxT6o', '_score': 0.54476166, '_ignored': ['body_content_window.keyword'], 'fields': {'body_content_window': [\" Log in Blog Solutions Stack + Cloud Tech Topics News Customers Generative AI Culture Overview of image similarity search in Elasticsearch By Radovan Ondas, Bernhard Suhm 28 February 2023 Table of contents Share on Twitter Share on LinkedIn Share on Facebook Share by email Print Imagine being able to mimic a celebrity’s look with a screenshot. Users could use the image to quickly find clothing sold online that matches the style. But, this is not the search experience of today. Customers struggle to find what they need, and if they can’t, they'll leave. Some of them don't remember the name (keyword) of what they are looking for, but have an idea of what it looks like or the actual image. With vector search , an integrated capability in Elastic, organizations can implement similarity image search, also known as reverse image search. This helps organizations create a more intuitive search experience so customers can easily search for what they are looking for with just an image. To implement this functionality in Elastic, you don’t need to be a machine learning expert to get started. That’s because vector search is already integrated into our scalable, highly performant platform. You get integrations into application frameworks, making it easier to stand up interactive applications. In this multi-part blog series, you’ll walk through how to build a prototype similarity search application in Elastic using your own set of images. The front end of this prototype application is implemented using Flask. It can serve as a blueprint for your own custom application. Part 1: 5 technical components of image similarity search Part 2: How to implement image similarity search in Elastic In this overview blog, you’ll go behind the scenes to better understand the architecture required to apply vector search to image data with Elastic. If you’re actually more interested in semantic search on text rather than images, review the multi-blog series on natural language processing (NLP) to learn about text embeddings and vector search, named entity recognition (NER), sentiment analysis, and how to apply these techniques in Elastic. We’ll get started by stepping back a little and explaining how both similarity and semantic search are powered by vector search. Semantic search and similarity search — both powered by vector search Vector search leverages machine learning (ML) to capture the meaning and context of unstructured data. Vector search finds similar data using approximate nearing neighbor (ANN) algorithms.\"], 'id': ['64b93618e761e30894486814'], 'title': ['Overview of image similarity search | Elastic.co | Elastic Blog'], 'url': ['https://www.elastic.co/blog/overview-image-similarity-search-in-elastic']}}, {'_index': 'blogs', '_id': 'L7iqsYkBLg4Kd5ySdDxp', '_score': 0.5347501, '_ignored': ['body_content_window.keyword'], 'fields': {'body_content_window': [' ... img_model = SentenceTransformer(\\'clip-ViT-B-32\\') ... for filename in glob.glob(PATH_TO_IMAGES, recursive=True): doc = {} image = Image.open(filename) embedding = img_model.encode(image) doc[\\'image_name\\'] = os.path.basename(filename) doc[\\'image_embedding\\'] = embedding.tolist() lst.append(doc) ... Read more Or refer to the figure below as an illustration: The document after processing might look like the following. The critical part is the field \" image_embedding\" where the dense vector representation is stored. { \"_index\": \"my-image-embeddings\", \"_id\": \"_g9ACIUBMEjlQge4tztV\", \"_score\": 6.703597, \"_source\": { \"image_id\": \"IMG_4032\", \"image_name\": \"IMG_4032.jpeg\", \"image_embedding\": [ -0.3415695130825043, 0.1906963288784027, ..... -0.10289803147315979, -0.15871885418891907 ], \"relative_path\": \"phone/IMG_4032.jpeg\" } } Read more 5. The application logic Building on these basic components, you can finally put all pieces together and work through the logic to implement an interactive image similarity search. Let’s start conceptually, with what needs to happen when you want to interactively retrieve images that match a given description. For textual queries, the input can be as simple as a single word like roses or a more extended description like “a mountain covered in snow.” Or you can also provide an image and ask for similar images to the one you have. Even though you are using different modalities to formulate your query, both are executed using the same sequence of steps in the underlying vector search, namely using a query (kNN) over documents represented by their embeddings (as “dense” vectors).'], 'id': ['64b9368de761e3623d493ea9'], 'title': ['5 Technical components of image similarity search | Elastic.co | Elastic Blog'], 'url': ['https://www.elastic.co/blog/5-technical-components-image-similarity-search']}}, {'_index': 'blogs', '_id': 'LriqsYkBLg4Kd5ySdDxp', '_score': 0.527482, '_ignored': ['body_content_window.keyword'], 'fields': {'body_content_window': [' # Run kNN search against <query-embedding> obtained above POST <your-image-index>/_search { \"fields\": [...], \"knn\": { \"field\": \"image_embedding\", \"k\": 5, \"num_candidates\": 10, \"query_vector\": <query-embedding> } } Read more To learn more about kNN in Elastic, refer to our documentation: https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search.html . 4. Generate image embeddings The image embeddings mentioned above are critical for good performance of your image similarity search. They should be stored in a separate index that holds the image embeddings, which is referred to as you-image-index in the code above. The index consists of a document per image together with fields for the context and the dense vector (image embedding) interpretation of the image. Image embeddings represent an image in a lower-dimensional space. Similar images are mapped to nearby points in this space. The raw image can be several MB large, depending on its resolution. The specific details of how these embeddings are generated can vary. In general, this process involves extracting features from the images and then mapping them to a lower-dimensional space using a mathematical function. This function is typically trained on a large data set of images to learn the best way to represent the features in the lower-dimensional space. Generating embeddings is a one-time task. In this blog, we’ll employ the CLIP model for this purpose. It is distributed by OpenAI and provides a good starting point. You may need to train a custom embedding model for specialized use cases to achieve desired performance, depending how well the types of images you want to classify are represented in the publicly available data used to train the CLIP model. Embedding generation in Elastic needs to occur at ingest time, and therefore in a process external to the search, with the following steps: Load the CLIP model. For every image: Load the image. Evaluate the image using the model. Save the generated embeddings into a document. Save the document into the datastore/Elasticsearch. The pseudo code makes these steps more concrete, and you can access the full code in the example repository.'], 'id': ['64b9368de761e3623d493ea9'], 'title': ['5 Technical components of image similarity search | Elastic.co | Elastic Blog'], 'url': ['https://www.elastic.co/blog/5-technical-components-image-similarity-search']}}, {'_index': 'blogs', '_id': 'orivsYkBLg4Kd5ySC0bj', '_score': 0.5039488, '_ignored': ['body_content_window.keyword'], 'fields': {'body_content_window': [' In the example below, the text entered was “ endless route to the top .” The results are shown from our data set. If a user likes one particular image in the result set, simply click the button next to it, and similar images will display. Users can do this endless times and build their own path through the image data set. The search also works by simply uploading an image. The application will convert the image into a vector and search for a similar image in the data set. To do this, navigate to the third tab Similar Image , upload an image from the disk, and hit Search . Because the NLP ( sentence-transformers/clip-ViT-B-32-multilingual-v1 ) model we are using in Elasticsearch is multilingual and supports inference in many languages, try to search for the images in your own language. Then verify the results by using English text as well. It’s important to note that the models used are generic models, which are pretty accurate but the results you get will vary depending on the use case or other factors. If you need higher accuracy, you will have to adapt a generic model or develop your own model — the CLIP model is just intended as a starting point. Code summary You can find the complete code in the GitHub repository . You may be inspecting the code in routes.py , which implements the main logic of the application. Besides the obvious route definition, you should focus on methods that define the _infer and _search endpoints ( infer_trained_model and knn_search_images ). The code that generates image embeddings is located in create-image-embeddings.py file. Summary Now that you have the Flask app set up, you can search your own set of images with ease! Elastic provides native integration of vector search within the platform, which avoids communication with external processes. You get the flexibility to develop and employ custom embedding models that you may have developed using PyTorch. Semantic image search delivers the following benefits of other traditional approaches to image search: Higher accuracy: Vector similarity captures context and associations without relying on textual meta descriptions of the images. Enhanced user experience: Describe what you’re looking for, or provide a sample image, compared to guessing which keywords may be relevant.'], 'id': ['64b935e8e761e3fd514806f7'], 'title': ['How to implement similarity image search | Elastic.co | Elastic Blog'], 'url': ['https://www.elastic.co/blog/implement-image-similarity-search-elastic']}}, {'_index': 'blogs', '_id': 'JbiosYkBLg4Kd5ySgDcS', '_score': 0.4988286, '_ignored': ['body_content_window.keyword'], 'fields': {'body_content_window': [\" Log in Blog Solutions Stack + Cloud Tech Topics News Customers Generative AI Culture 27 August 2019 Tech Topics en de pt es kr cn jp fr Text similarity search with vector fields By Julie Tibshirani Share From its beginnings as a recipe search engine , Elasticsearch was designed to provide fast and powerful full-text search. Given these roots, improving text search has been an important motivation for our ongoing work with vectors. In Elasticsearch 7.0, we introduced experimental field types for high-dimensional vectors, and now the 7.3 release brings support for using these vectors in document scoring. This post focuses on a particular technique called text similarity search. In this type of search, a user enters a short free-text query, and documents are ranked based on their similarity to the query. Text similarity can be useful in a variety of use cases: Question-answering: Given a collection of frequently asked questions, find questions that are similar to the one the user has entered. Article search: In a collection of research articles, return articles with a title that’s closely related to the user’s query. Image search: In a dataset of captioned images, find images whose caption is similar to the user’s description. A straightforward approach to similarity search would be to rank documents based on how many words they share with the query. But a document may be similar to the query even if they have very few words in common — a more robust notion of similarity would take into account its syntactic and semantic content as well. The natural language processing (NLP) community has developed a technique called text embedding that encodes words and sentences as numeric vectors. These vector representations are designed to capture the linguistic content of the text, and can be used to assess similarity between a query and a document. This post explores how text embeddings and Elasticsearch’s dense_vector type could be used to support similarity search. We’ll first give an overview of embedding techniques, then step through a simple prototype of similarity search using Elasticsearch. Note: Using text embeddings in search is a complex and evolving area. This blog is not a recommendation for a particular architecture or implementation. Start here to learn how you can enhance your search experience with the power of vector search . What are text embeddings? Let's take a closer look at different types of text embeddings, and how they compare to traditional search approaches.\"], 'id': ['64b93621e761e3711c487974'], 'title': ['Text similarity search in Elasticsearch using vector fields | Elastic Blog'], 'url': ['https://www.elastic.co/blog/text-similarity-search-with-vectors-in-elasticsearch']}}, {'_index': 'blogs', '_id': 'LbiqsYkBLg4Kd5ySdDxp', '_score': 0.49111536, '_ignored': ['body_content_window.keyword'], 'fields': {'body_content_window': [' Another important aspect of CLIP is that it is a \"zero-shot\" model, which allows it to perform tasks it has not been specifically trained on. For example, it can translate between languages it has not seen during training or classify images into categories it has not seen before. This makes CLIP a very flexible and versatile model. You will use the CLIP model to vectorize your images, using the inference endpoint in Elastic as described next and executing inference on a large set of images as described in section 3 further below. SignUpCTA A search toolkit for the AI era The Elasticsearch Relevance Engine (ESRE) gives developers the tools they need to build AI-powered search apps. Build Generative AI Search Engines and Applications 2. The inference endpoint Once the NLP model is loaded into Elasticsearch, you can process an actual user query. First, you need to translate the text of the query into a vector using the Elasticsearch _infer endpoint. The endpoint provides a built-in method of using the NLP model natively in Elastic and does not require querying an external service, significantly simplifying the implementation. POST _ml/trained_models/sentence-transformers__clip-vit-b-32-multilingual-v1/deployment/_infer { \"docs\" : [ {\"text_field\": \"A mountain covered in snow\"} ] } 3. Vector (similarity) search After indexing both queries and documents with vector embeddings, similar documents are the nearest neighbors of your query in embedding space. One popular algorithm to achieve that is k-nearest neighbor (kNN), which finds the k nearest vectors to a query vector. However, on the large data sets you’d typically process in image search applications, kNN requires very high computational resources and can lead to excessive execution times. As a solution, approximate nearest neighbor (ANN) search sacrifices perfect accuracy in exchange for executing efficiently in high dimensional embedding spaces, at scale. In Elastic, the _search endpoint supports both exact and approximate nearest neighbor searches. Use the code below for the kNN search. It assumes the embeddings for all the images in your-image-index are available in the image_embedding field. The next section discusses how you can create the embeddings.'], 'id': ['64b9368de761e3623d493ea9'], 'title': ['5 Technical components of image similarity search | Elastic.co | Elastic Blog'], 'url': ['https://www.elastic.co/blog/5-technical-components-image-similarity-search']}}, {'_index': 'blogs', '_id': 'qLinsYkBLg4Kd5yS0jWv', '_score': 0.48471558, '_ignored': ['body_content_window.keyword'], 'fields': {'body_content_window': [' Thankfully there is a great open source project called ann-benchmarks which tests the leading algorithms against several datasets and publishes comparisons. Elasticsearch 8.0 uses an ANN algorithm called Hierarchical Navigable Small World graphs (HNSW), which organizes vectors into a graph based on their similarity to each other. HNSW shows strong search performance across a variety of ann-benchmarks datasets , and also did well in our own testing. Another benefit of HNSW is that it’s widely used in industry, having been implemented in several different systems. In addition to the original academic paper , there are many helpful resources for learning about the algorithm\\'s details. Although Elasticsearch ANN is currently based on HNSW, the feature is designed in a flexible way to let us incorporate different approaches in the future. SignUpCTA A search toolkit for the AI era The Elasticsearch Relevance Engine (ESRE) gives developers the tools they need to build AI-powered search apps. Build Generative AI Search Engines and Applications Show me the code! To index vectors for ANN search, we need to set index: true and specify the similarity metric we’re using to compare them: PUT index { \"mappings\": { \"properties\": { \"image-vector\": { \"type\": \"dense_vector\", \"dims\": 128, \"index\": true, \"similarity\": \"l2_norm\" } } } } PUT index/_doc { \"image-vector\": [0.12, 1.34, ...] } Read more Then, after adding vectors, we can search for the k nearest neighbors to a query vector: GET index/_knn_search { \"knn\": { \"field\": \"image-vector\", \"query_vector\": [-0.5, 9.4, ...], \"k\": 10, \"num_candidates\": 100 } } The new _knn_search endpoint uses HNSW graphs to efficiently retrieve similar vectors. Unlike exact kNN, which performs a full scan of the data, it scales well to large datasets.'], 'id': ['64b9361ce761e3e7d9487036'], 'title': ['Introducing approximate nearest neighbor search in Elasticsearch 8.0 | Elastic Blog'], 'url': ['https://www.elastic.co/blog/introducing-approximate-nearest-neighbor-search-in-elasticsearch-8-0']}}, {'_index': 'blogs', '_id': 'p7insYkBLg4Kd5yS0jWv', '_score': 0.48257306, '_ignored': ['body_content_window.keyword'], 'fields': {'body_content_window': [' Log in Blog Solutions Stack + Cloud Tech Topics News Customers Generative AI Culture Introducing approximate nearest neighbor search in Elasticsearch 8.0 By Julie Tibshirani 07 February 2022 Share on Twitter Share on LinkedIn Share on Facebook Share by email Print There has been a surge of interest in vector search , thanks to a new generation of machine learning models that can represent all sorts of content as vectors, including text, images, events, and more. Often called “embedding models”, these powerful representations can capture similarity between two pieces of content in a way that goes beyond their surface level characteristics. k-nearest neighbor (kNN) search algorithms find the vectors in a dataset that are most similar to a query vector. Paired with these vector representations, kNN search opens up exciting possibilities for retrieval: Finding passages likely to contain the answer to a question Detecting near-duplicate images in a large dataset Finding songs that sound similar to a given song Vector search is poised to become an important component of the search toolbox, alongside traditional techniques like term-based scoring. Elasticsearch currently supports storing vectors through the dense_vector field type and using them to calculate document scores. This allows users to perform an exact kNN search by scanning all documents. Elasticsearch 8.0 builds on this functionality to support fast, approximate nearest neighbor search (ANN). This represents a much more scalable approach, allowing vector search to run efficiently on large datasets. ANN in Elasticsearch What is approximate nearest neighbor search? There are well-established data structures for kNN on low-dimensional vectors, like KD-trees. In fact, Elasticsearch incorporates KD-trees to support searches on geospatial and numeric data. But modern embedding models for text and images typically produce high-dimensional vectors of 100 - 1000 elements, or even more. These vector representations present a unique challenge, as it’s very difficult to efficiently find nearest neighbors in high dimensions. Faced with this difficulty, nearest neighbor algorithms usually sacrifice perfect accuracy to improve their speed. These approximate nearest neighbor (ANN) algorithms may not always return the true k nearest vectors. But they run efficiently, scaling to large datasets while maintaining good performance. Choosing an ANN algorithm Designing ANN algorithms is an active area of academic research, and there are many promising algorithms to choose from. They often present different trade-offs in terms of their search speed, implementation complexity, and indexing cost.'], 'id': ['64b9361ce761e3e7d9487036'], 'title': ['Introducing approximate nearest neighbor search in Elasticsearch 8.0 | Elastic Blog'], 'url': ['https://www.elastic.co/blog/introducing-approximate-nearest-neighbor-search-in-elasticsearch-8-0']}}]}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "  <div id=\"df-5ffc7696-6743-4942-8f9e-bf694b85ae23\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>_score</th>\n",
       "      <th>fields.title</th>\n",
       "      <th>fields.body_content_window</th>\n",
       "      <th>fields.url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MbiqsYkBLg4Kd5ySdDxp</td>\n",
       "      <td>0.559458</td>\n",
       "      <td>[5 Technical components of image similarity se...</td>\n",
       "      <td>[ GET my-image-embeddings/_search { \"knn\": { \"...</td>\n",
       "      <td>[https://www.elastic.co/blog/5-technical-compo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3birsYkBLg4Kd5ySxT6o</td>\n",
       "      <td>0.548522</td>\n",
       "      <td>[Overview of image similarity search | Elastic...</td>\n",
       "      <td>[ Compared to traditional text search (in Elas...</td>\n",
       "      <td>[https://www.elastic.co/blog/overview-image-si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3LirsYkBLg4Kd5ySxT6o</td>\n",
       "      <td>0.544762</td>\n",
       "      <td>[Overview of image similarity search | Elastic...</td>\n",
       "      <td>[ Log in Blog Solutions Stack + Cloud Tech Top...</td>\n",
       "      <td>[https://www.elastic.co/blog/overview-image-si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L7iqsYkBLg4Kd5ySdDxp</td>\n",
       "      <td>0.534750</td>\n",
       "      <td>[5 Technical components of image similarity se...</td>\n",
       "      <td>[ ... img_model = SentenceTransformer('clip-Vi...</td>\n",
       "      <td>[https://www.elastic.co/blog/5-technical-compo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LriqsYkBLg4Kd5ySdDxp</td>\n",
       "      <td>0.527482</td>\n",
       "      <td>[5 Technical components of image similarity se...</td>\n",
       "      <td>[ # Run kNN search against &lt;query-embedding&gt; o...</td>\n",
       "      <td>[https://www.elastic.co/blog/5-technical-compo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>orivsYkBLg4Kd5ySC0bj</td>\n",
       "      <td>0.503949</td>\n",
       "      <td>[How to implement similarity image search | El...</td>\n",
       "      <td>[ In the example below, the text entered was “...</td>\n",
       "      <td>[https://www.elastic.co/blog/implement-image-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JbiosYkBLg4Kd5ySgDcS</td>\n",
       "      <td>0.498829</td>\n",
       "      <td>[Text similarity search in Elasticsearch using...</td>\n",
       "      <td>[ Log in Blog Solutions Stack + Cloud Tech Top...</td>\n",
       "      <td>[https://www.elastic.co/blog/text-similarity-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LbiqsYkBLg4Kd5ySdDxp</td>\n",
       "      <td>0.491115</td>\n",
       "      <td>[5 Technical components of image similarity se...</td>\n",
       "      <td>[ Another important aspect of CLIP is that it ...</td>\n",
       "      <td>[https://www.elastic.co/blog/5-technical-compo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>qLinsYkBLg4Kd5yS0jWv</td>\n",
       "      <td>0.484716</td>\n",
       "      <td>[Introducing approximate nearest neighbor sear...</td>\n",
       "      <td>[ Thankfully there is a great open source proj...</td>\n",
       "      <td>[https://www.elastic.co/blog/introducing-appro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>p7insYkBLg4Kd5yS0jWv</td>\n",
       "      <td>0.482573</td>\n",
       "      <td>[Introducing approximate nearest neighbor sear...</td>\n",
       "      <td>[ Log in Blog Solutions Stack + Cloud Tech Top...</td>\n",
       "      <td>[https://www.elastic.co/blog/introducing-appro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5ffc7696-6743-4942-8f9e-bf694b85ae23')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "\n",
       "\n",
       "\n",
       "    <div id=\"df-e733c552-5309-4b99-9ad1-edfdefe9d3c3\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e733c552-5309-4b99-9ad1-edfdefe9d3c3')\"\n",
       "              title=\"Suggest charts.\"\n",
       "              style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "    </div>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "    background-color: #E8F0FE;\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: #1967D2;\n",
       "    height: 32px;\n",
       "    padding: 0 0 0 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: #E2EBFA;\n",
       "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: #174EA6;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "    background-color: #3B4455;\n",
       "    fill: #D2E3FC;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart:hover {\n",
       "    background-color: #434B5C;\n",
       "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "    fill: #FFFFFF;\n",
       "  }\n",
       "</style>\n",
       "\n",
       "    <script>\n",
       "      async function quickchart(key) {\n",
       "        const containerElement = document.querySelector('#' + key);\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      }\n",
       "    </script>\n",
       "\n",
       "      <script>\n",
       "\n",
       "function displayQuickchartButton(domScope) {\n",
       "  let quickchartButtonEl =\n",
       "    domScope.querySelector('#df-e733c552-5309-4b99-9ad1-edfdefe9d3c3 button.colab-df-quickchart');\n",
       "  quickchartButtonEl.style.display =\n",
       "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "}\n",
       "\n",
       "        displayQuickchartButton(document);\n",
       "      </script>\n",
       "      <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-5ffc7696-6743-4942-8f9e-bf694b85ae23 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-5ffc7696-6743-4942-8f9e-bf694b85ae23');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                    _id    _score  \\\n",
       "0  MbiqsYkBLg4Kd5ySdDxp  0.559458   \n",
       "1  3birsYkBLg4Kd5ySxT6o  0.548522   \n",
       "2  3LirsYkBLg4Kd5ySxT6o  0.544762   \n",
       "3  L7iqsYkBLg4Kd5ySdDxp  0.534750   \n",
       "4  LriqsYkBLg4Kd5ySdDxp  0.527482   \n",
       "5  orivsYkBLg4Kd5ySC0bj  0.503949   \n",
       "6  JbiosYkBLg4Kd5ySgDcS  0.498829   \n",
       "7  LbiqsYkBLg4Kd5ySdDxp  0.491115   \n",
       "8  qLinsYkBLg4Kd5yS0jWv  0.484716   \n",
       "9  p7insYkBLg4Kd5yS0jWv  0.482573   \n",
       "\n",
       "                                        fields.title  \\\n",
       "0  [5 Technical components of image similarity se...   \n",
       "1  [Overview of image similarity search | Elastic...   \n",
       "2  [Overview of image similarity search | Elastic...   \n",
       "3  [5 Technical components of image similarity se...   \n",
       "4  [5 Technical components of image similarity se...   \n",
       "5  [How to implement similarity image search | El...   \n",
       "6  [Text similarity search in Elasticsearch using...   \n",
       "7  [5 Technical components of image similarity se...   \n",
       "8  [Introducing approximate nearest neighbor sear...   \n",
       "9  [Introducing approximate nearest neighbor sear...   \n",
       "\n",
       "                          fields.body_content_window  \\\n",
       "0  [ GET my-image-embeddings/_search { \"knn\": { \"...   \n",
       "1  [ Compared to traditional text search (in Elas...   \n",
       "2  [ Log in Blog Solutions Stack + Cloud Tech Top...   \n",
       "3  [ ... img_model = SentenceTransformer('clip-Vi...   \n",
       "4  [ # Run kNN search against <query-embedding> o...   \n",
       "5  [ In the example below, the text entered was “...   \n",
       "6  [ Log in Blog Solutions Stack + Cloud Tech Top...   \n",
       "7  [ Another important aspect of CLIP is that it ...   \n",
       "8  [ Thankfully there is a great open source proj...   \n",
       "9  [ Log in Blog Solutions Stack + Cloud Tech Top...   \n",
       "\n",
       "                                          fields.url  \n",
       "0  [https://www.elastic.co/blog/5-technical-compo...  \n",
       "1  [https://www.elastic.co/blog/overview-image-si...  \n",
       "2  [https://www.elastic.co/blog/overview-image-si...  \n",
       "3  [https://www.elastic.co/blog/5-technical-compo...  \n",
       "4  [https://www.elastic.co/blog/5-technical-compo...  \n",
       "5  [https://www.elastic.co/blog/implement-image-s...  \n",
       "6  [https://www.elastic.co/blog/text-similarity-s...  \n",
       "7  [https://www.elastic.co/blog/5-technical-compo...  \n",
       "8  [https://www.elastic.co/blog/introducing-appro...  \n",
       "9  [https://www.elastic.co/blog/introducing-appro...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDEX_NAME=\"blogs\"\n",
    "\n",
    "source_fields = [\"body_content_window\", \"id\", \"title\", \"url\"]\n",
    "\n",
    "query = {\n",
    "  \"field\": \"text_embedding.predicted_value\",\n",
    "  \"k\": 10,\n",
    "  \"num_candidates\": 50,\n",
    "  \"query_vector_builder\": {\n",
    "    \"text_embedding\": {\"model_id\": \"sentence-transformers__all-minilm-l6-v2\",\n",
    "      \"model_text\": \"how to do image similarity search\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "response = es.search(\n",
    "    index=INDEX_NAME,\n",
    "    fields=source_fields,\n",
    "    knn=query,\n",
    "    source=False)\n",
    "\n",
    "print(response.body)\n",
    "\n",
    "results = pd.json_normalize(json.loads(json.dumps(response.body['hits']['hits'])))\n",
    "results[['_id', '_score', 'fields.title', 'fields.body_content_window', 'fields.url']]\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
