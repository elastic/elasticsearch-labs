{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# NLP text search\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/elastic/elasticsearch-labs/blob/main/notebooks/integrations/hugging-face/loading-model-from-hugging-face.ipynb)\n",
    "\n",
    "\n",
    "The workbook implements NLP text search in Elasticsearch using the Elastic blogs dataset.\n",
    "\n",
    "You will index blogs documents, and using ingest pipeline generate text embeddings. By using NLP model you will query the documents using natural language over the the blogs documents.\n",
    "\n",
    "The dataset is a ~4000 blog posts from [elastic.co/blog](https://elastic.co/blog).\n",
    "\n",
    "### Prerequisities\n",
    "\n",
    "Before you start make sure you have Elasticsearch cluster running. The cluster must have at least one machine learning (ML) node with enough (4GB) memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdzl8tmZfr3y"
   },
   "source": [
    "### Install Python requirements\n",
    "Before you start you need to install all required Python dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9NM_6fGFURcz",
    "outputId": "53f1a78c-db7f-468e-c6af-bdf9be554473"
   },
   "outputs": [],
   "source": [
    "!pip install sentence-transformers eland elasticsearch transformers torch tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKU9L8o2FodV"
   },
   "source": [
    "### Upload NLP model for querying\n",
    "Using an `eland` tool you will import required NLP models. One model will transfer your search query into vector which will be used for the search over the set of documents stored in Elasticsearch.\n",
    "\n",
    "Model used:\n",
    "- [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) to transform the search text into the vector\n",
    "\n",
    "How to get cloud id? Go to [ESS cloud](https://cloud.elastic.co/logout?redirectTo=%2Fhome&reason=unauthorised) and `On the deployment overview page, copy down the Cloud ID.`\n",
    "\n",
    "The authentication is using api key (`--es-api-key`). Learn how to generate [API key](https://www.elastic.co/guide/en/kibana/current/api-keys.html#create-api-key). Another option is to use username/password.\n",
    "\n",
    "```\n",
    "$ eland_import_hub_model --cloud-id $CLOUD_ID \\\n",
    "  --hub-model-id sentence-transformers/all-MiniLM-L6-v2 \\\n",
    "  --task-type text_embedding --es-api-key $API_KEY --start\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vQkKn02j_FfJ",
    "outputId": "99b9ffd4-6780-4167-dbd7-e0264369e40e"
   },
   "outputs": [],
   "source": [
    "API_KEY=''\n",
    "CLOUD_ID=''\n",
    "!eland_import_hub_model --cloud-id $CLOUD_ID --hub-model-id sentence-transformers/all-MiniLM-L6-v2 --task-type text_embedding --es-api-key $API_KEY --start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqNkx-WiJePa"
   },
   "source": [
    "### Import Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6YwgyXRJoyb"
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import zipfile\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGfkUDWDMkc4"
   },
   "source": [
    "### Connect to Elasticsearch cluster\n",
    "Use your own cluster details `ELASTIC_CLOUD_ID`, `API_KEY`.\n",
    "\n",
    "You also have another option to authentication (Username/Password) if you wish do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGi175RbJhVQ",
    "outputId": "14e7f403-06ba-4fa7-a3f4-4bcc271a1f5d"
   },
   "outputs": [],
   "source": [
    "# ESS Cloud connection definition using an API_KEY\n",
    "ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID: \")\n",
    "API_KEY = getpass(\"Elastic API key: \")\n",
    "\n",
    "# ELASTIC_CLOUD_USER = \"elastic\"\n",
    "# CLOUD_PASSWORD = getpass(\"Elastic Password\")\n",
    "\n",
    "es = Elasticsearch(\n",
    "  cloud_id=ELASTIC_CLOUD_ID,\n",
    "  #basic_auth=(ELASTIC_CLOUD_USER, ELASTIC_CLOUD_PASSWORD),\n",
    "  api_key=API_KEY,\n",
    "  request_timeout=600\n",
    ")\n",
    "\n",
    "es.info() # should return cluster info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FoZ5TBrIqOT"
   },
   "source": [
    "### Upload Ingest pipeline\n",
    "You need to upload the ingest pipeline which will generate vector (text) embeddings for selected field.\n",
    "\n",
    "The pipeline below is defining a processor for the inference to the correct NLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "geY7WLh7Ky-k",
    "outputId": "97c17b36-94d6-454f-b976-4c20e8e49edc"
   },
   "outputs": [],
   "source": [
    "# ingest pipeline definition\n",
    "PIPELINE_ID=\"vectorize_blogs\"\n",
    "PIPELINE_DESC=\"This is the description for document procesing\"\n",
    "PIPELINE_VERSION=3\n",
    "\n",
    "es.ingest.put_pipeline(id=PIPELINE_ID, description=PIPELINE_DESC, version=PIPELINE_VERSION,\n",
    "                       processors=[{\n",
    "        \"inference\": {\n",
    "          \"model_id\": \"sentence-transformers__all-minilm-l6-v2\",\n",
    "          \"target_field\": \"text_embedding\",\n",
    "          \"field_map\": {\n",
    "            \"body_content_window\": \"text_field\"\n",
    "          }\n",
    "        }\n",
    "      }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IW-GIlH2OxB4"
   },
   "source": [
    "### Create Index and mappings for Blogs\n",
    "Befor you can index documents into Elasticsearch, you need to create an Index with correct mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xAkc1OVcOxy3",
    "outputId": "b2453634-89b8-48bc-ac65-a6a1c3b8170f"
   },
   "outputs": [],
   "source": [
    "INDEX_NAME=\"blogs\"\n",
    "DELETE_INDEX=True\n",
    "\n",
    "INDEX_MAPPING = {\n",
    "    \"properties\": {\n",
    "      \"body_content_window\": {\n",
    "        \"type\": \"text\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"byline\": {\n",
    "        \"type\": \"text\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"name\": {\n",
    "        \"type\": \"text\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"publish_date\": {\n",
    "        \"type\": \"date\"\n",
    "      },\n",
    "      \"title\": {\n",
    "        \"type\": \"text\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"url\": {\n",
    "        \"type\": \"text\",\n",
    "        \"fields\": {\n",
    "          \"keyword\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"text_embedding\": {\n",
    "        \"properties\": {\n",
    "          \"is_truncated\": {\n",
    "            \"type\": \"boolean\"\n",
    "          },\n",
    "          \"model_id\": {\n",
    "            \"type\": \"text\",\n",
    "            \"fields\": {\n",
    "              \"keyword\": {\n",
    "                \"type\": \"keyword\",\n",
    "                \"ignore_above\": 256\n",
    "              }\n",
    "            }\n",
    "          },\n",
    "          \"predicted_value\": {\n",
    "            \"type\": \"dense_vector\",\n",
    "            \"dims\": 384,\n",
    "            \"index\": True,\n",
    "            \"similarity\": \"l2_norm\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "INDEX_SETTINGS = {\n",
    "    \"index\": {\n",
    "      \"number_of_replicas\": \"1\",\n",
    "      \"number_of_shards\": \"1\",\n",
    "      \"default_pipeline\": PIPELINE_ID\n",
    "    }\n",
    "}\n",
    "\n",
    "if(DELETE_INDEX):\n",
    "  if es.indices.exists(index=INDEX_NAME):\n",
    "    print(\"Deleting existing %s\" % INDEX_NAME)\n",
    "    es.indices.delete(index=INDEX_NAME, ignore=[400, 404])\n",
    "\n",
    "print(\"Creating index %s\" % INDEX_NAME)\n",
    "es.indices.create(index=INDEX_NAME, mappings=INDEX_MAPPING, settings=INDEX_SETTINGS,\n",
    "                  ignore=[400, 404])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKE-j0kPUMn_"
   },
   "source": [
    "### Get blogs dataset\n",
    "Download:\n",
    "- The example blogs dataset is located in [GitHub](https://raw.githubusercontent.com/elastic/elasticsearch-labs/tree/main/notebooks/integrations/hugging-face/2023-07-27-blogs-no-embeddings.json.zip)\n",
    "\n",
    "Then unzip the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QKrykunW35y"
   },
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!wget https://raw.githubusercontent.com/elastic/elasticsearch-labs/tree/main/notebooks/integrations/hugging-face/2023-07-27-blogs-no-embeddings.json.zip -P data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2YBi3TxJXgGP",
    "outputId": "0fa8136b-aeb6-49f1-81f6-41474b1d5d96"
   },
   "outputs": [],
   "source": [
    "# Unzip the file\n",
    "BLOGS_ZIP_FILE=\"data/2023-07-27-blogs-no-embeddings.json.zip\"\n",
    "with zipfile.ZipFile(BLOGS_ZIP_FILE, 'r') as zip_ref:\n",
    "  print('Extracting file ', BLOGS_ZIP_FILE, '.')\n",
    "  zip_ref.extractall('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOGsvnGveAoP"
   },
   "source": [
    "Let's index blogs using the ingest pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-qy8iE5eEy5",
    "outputId": "386ae640-26ae-48f1-ead5-d07f7bb329e0"
   },
   "outputs": [],
   "source": [
    "BLOGS_FILE=\"data/2023-07-27-blogs-no-embeddings.json\"\n",
    "df_blogs_embeddings = pd.read_json(BLOGS_FILE, lines=True)\n",
    "\n",
    "def gen_rows(df):\n",
    "  for doc in df.to_dict(orient='records'):\n",
    "    yield doc\n",
    "\n",
    "count = 0\n",
    "for success, info in parallel_bulk(\n",
    "        client=es,\n",
    "        actions=gen_rows(df_blogs_embeddings),\n",
    "        thread_count=5,\n",
    "        chunk_size=200,\n",
    "        timeout='%ss' % 600,\n",
    "        index=INDEX_NAME\n",
    "):\n",
    "  if success:\n",
    "    count += 1\n",
    "    if count % 200 == 0:\n",
    "      print('Indexed %s documents' % str(count), flush=True)\n",
    "      sys.stdout.flush()\n",
    "  else:\n",
    "    print(info)\n",
    "    print('Doc failed', info)\n",
    "\n",
    "print('Indexed %s blogs embeddings documents' % str(count), flush=True)\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPPHg8K8T3wY"
   },
   "source": [
    "### Query the dataset\n",
    "The next step is to run a query to search for relevant blogs. The example query searches for `\"model_text\": \"how to do image similarity search\"` using the model we uploaded to Elasticsearch `sentence-transformers__all-minilm-l6-v2`.\n",
    "\n",
    "The process is one query even it internally consists of two tasks. One is to transform your search text into a vector using the NLP model and the second task is to run the vector search over the dataset.\n",
    "\n",
    "```\n",
    "POST blogs/_search\n",
    "{\n",
    "  \"knn\": {\n",
    "  \"field\": \"text_embedding.predicted_value\",\n",
    "  \"k\": 10,\n",
    "  \"num_candidates\": 50,\n",
    "  \"query_vector_builder\": {\n",
    "    \"text_embedding\": {\n",
    "      \"model_id\": \"sentence-transformers__all-minilm-l6-v2\",\n",
    "      \"model_text\": \"how to do image similarity search\"\n",
    "    }\n",
    "  },\n",
    "  \"fields\": [\n",
    "    \"body_content_window\",\n",
    "    \"id\",\n",
    "    \"title\",\n",
    "    \"url\"\n",
    "  ],\n",
    "  \"_source\": false\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 566
    },
    "id": "c4G5V9wmU9C5",
    "outputId": "c8f0cc24-5713-4560-8a5d-c42da562a670"
   },
   "outputs": [],
   "source": [
    "INDEX_NAME=\"blogs\"\n",
    "\n",
    "source_fields = [\"body_content_window\", \"id\", \"title\", \"url\"]\n",
    "\n",
    "query = {\n",
    "  \"field\": \"text_embedding.predicted_value\",\n",
    "  \"k\": 10,\n",
    "  \"num_candidates\": 50,\n",
    "  \"query_vector_builder\": {\n",
    "    \"text_embedding\": {\"model_id\": \"sentence-transformers__all-minilm-l6-v2\",\n",
    "      \"model_text\": \"how to do image similarity search\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "response = es.search(\n",
    "    index=INDEX_NAME,\n",
    "    fields=source_fields,\n",
    "    knn=query,\n",
    "    source=False)\n",
    "\n",
    "print(response.body)\n",
    "\n",
    "results = pd.json_normalize(json.loads(json.dumps(response.body['hits']['hits'])))\n",
    "results[['_id', '_score', 'fields.title', 'fields.body_content_window', 'fields.url']]\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
