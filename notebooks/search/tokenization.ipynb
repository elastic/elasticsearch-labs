{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "s49gpkvZ7q53"
   },
   "source": [
    "# Tokenization for Semantic Search (ELSER and E5)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/elastic/elasticsearch-labs/blob/main/notebooks/search/tokenization.ipynb)\n",
    "\n",
    "Elasticsearch offers some [semantic search](https://www.elastic.co/what-is/semantic-search) models, most notably [ELSER](https://www.elastic.co/guide/en/machine-learning/current/ml-nlp-elser.html) and [E5](https://www.elastic.co/search-labs/blog/articles/multilingual-vector-search-e5-embedding-model), to search through documents in a _menaningful_ way. Part of the process is breaking up texts (both for indexing documents and for queries) into tokens. Tokens are commonly thought of as words, but this is not accurate. Other substrings in the text also carry meaning to the semantic models and therefore have to be split out separately. For ELSER, our English-only model, this is done with the [`bert-base-uncased`](https://huggingface.co/bert-base-uncased) tokenizer.\n",
    "\n",
    "For Elasticsearch users it is important to know how texts are broken up into tokens because currently only the [first 512 tokens per field](https://www.elastic.co/guide/en/machine-learning/8.12/ml-nlp-limitations.html#ml-nlp-elser-v1-limit-512) are considered. This means that when you index longer texts, all tokens after the 512 will not be represented in your semantic search. Hence it is valuable to know the number of tokens for your input texts.\n",
    "\n",
    "Currently it is not possible to get the token count information via the API, so we share the code for calculating token counts here. This notebook also shows how to break longer text up into chunks of the right size so that no information is lost during indexing, which has to be done by the user (as of version 8.12, future version will remove the necessity and auto-chunk behind the scenes).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gaTFHLJC-Mgi"
   },
   "source": [
    "# Install packages\n",
    "\n",
    "As stated above, ELSER uses [BERT](https://huggingface.co/blog/bert-101)'s tokenizer internally. Here we install the `transformers` package that gives us an interface to this tokenizer. (We install the `tabulate` packge to be able to print a nice table for comparison later on.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K9Q1p2C9-wce",
    "outputId": "204d5aee-571e-4363-be6e-f87d058f2d29"
   },
   "outputs": [],
   "source": [
    "!pip install -qU tabulate transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import everything we need. You can ignore a potential warning on models not being available because we only need the tokenizer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxjakob/.pyenv/versions/3.11.7/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from tabulate import tabulate\n",
    "from transformers import AutoTokenizer, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define tokenizers\n",
    "\n",
    "Now we are ready to initialize the BERT tokenizer that ELSER uses and the E5 tokenizer for the multilingual semantic search. We also define a whitespace tokenizer in order to compare the naive version on creating tokens to the two tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "e5_tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-base')\n",
    "\n",
    "def whitespace_tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load example data\n",
    "\n",
    "Download the movies example data that is also used in the other search examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/elastic/elasticsearch-labs/main/notebooks/search/movies.json\"\n",
    "response = urlopen(url)\n",
    "movies = json.load(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare token counts\n",
    "\n",
    "Compare the token counts of the different tokenization methods for the descriptions of the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  whitespace    BERT    E5  text\n",
      "------------  ------  ----  -----------------------------------------------------------------------------------\n",
      "          16      21    30  An organized crime dynasty's aging patriarch transfers control of his clandestin...\n",
      "          19      25    32  Two imprisoned men bond over a number of years, finding solace and eventual rede...\n",
      "          20      25    34  Two detectives, a rookie and a veteran, hunt a serial killer who uses the seven ...\n",
      "          20      33    36  An insomniac office worker and a devil-may-care soapmaker form an underground fi...\n",
      "          22      28    27  An undercover cop and a mole in the police attempt to identify each other while ...\n",
      "          23      26    31  A computer hacker learns from mysterious rebels about the true nature of his rea...\n",
      "          26      36    42  A thief who steals corporate secrets through the use of dream-sharing technology...\n",
      "          27      36    42  The lives of two mob hitmen, a boxer, a gangster and his wife, and a pair of din...\n",
      "          27      40    48  A young F.B.I. cadet must receive the help of an incarcerated and manipulative c...\n",
      "          30      35    43  A sole survivor tells of the twisty events leading up to a horrific gun battle o...\n",
      "          33      39    44  When the menace known as the Joker wreaks havoc and chaos on the people of Gotha...\n",
      "          33      40    44  The story of Henry Hill and his life in the mob, covering his relationship with ...\n"
     ]
    }
   ],
   "source": [
    "def count_tokens(text):\n",
    "    whitespace_tokens = len(whitespace_tokenize(text))\n",
    "    bert_tokens = len(bert_tokenizer.encode(text))\n",
    "    e5_tokens = len(e5_tokenizer.encode(text))\n",
    "    return [whitespace_tokens, bert_tokens, e5_tokens, f\"{text[:80]}...\"]\n",
    "\n",
    "counts = [count_tokens(movie[\"plot\"]) for movie in movies]\n",
    "\n",
    "print(tabulate(sorted(counts), [\"whitespace\", \"BERT\", \"E5\", \"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that both the BERT the E5 tokenizers yields more tokens in every example, in some cases even twice as many. Why is that? Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lives of two mob hitmen, a boxer, a gangster and his wife, and a pair of diner bandits intertwine in four tales of violence and redemption.\n",
      "\n",
      "['[CLS]', 'the', 'lives', 'of', 'two', 'mob', 'hit', '##men', ',', 'a', 'boxer', ',', 'a', 'gangster', 'and', 'his', 'wife', ',', 'and', 'a', 'pair', 'of', 'diner', 'bandits', 'inter', '##t', '##wine', 'in', 'four', 'tales', 'of', 'violence', 'and', 'redemption', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "example_movie = movies[0][\"plot\"]\n",
    "print(example_movie)\n",
    "print()\n",
    "\n",
    "movie_tokens = bert_tokenizer.encode(example_movie)\n",
    "print(str([bert_tokenizer.decode([t]) for t in movie_tokens]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe:\n",
    "- There are special tokens `[CLS]` and `[SEP]` to model the the beginning and end of the text. These two extra tokens will become relevant below.\n",
    "- Punctuations are they own tokens.\n",
    "- Compounds words are split into two tokens, for example `hitmen` becomes `hit` and `##men`.\n",
    "\n",
    "Given this behavior, it is easy to see how longer tests yield lots of tokens and can quickly get beyond the 512 tokens limitation mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling long texts\n",
    "\n",
    "We saw how to count the number of tokens using the tokenizers from different models. ELSER uses the BERT tokenizer, so when using `.elser_model_2` it internally splits the text with this method.\n",
    "\n",
    "Currently there is a limitation that [only the first 512 tokens are considered](https://www.elastic.co/guide/en/machine-learning/8.12/ml-nlp-limitations.html#ml-nlp-elser-v1-limit-512). To work around this, we can first split the input text into chunks of 512 tokens and feed the chunks to Elasticsearch. Actually, we need to use a limit of 510 to leave space for the two special tokens (`[CLS]` and `[SEP]`) that we saw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEMANTIC_SEARCH_TOKEN_LIMIT = 510  # 512 minus space for the 2 special tokens\n",
    "\n",
    "def chunk(tokens, chunk_size=SEMANTIC_SEARCH_TOKEN_LIMIT):\n",
    "    for i in range(0, len(tokens), chunk_size):\n",
    "        yield tokens[i:i+chunk_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a longer example text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://raw.githubusercontent.com/elastic/elasticsearch-labs/main/notebooks/search/lorem-ipsum.txt\"\n",
    "# response = urlopen(url)\n",
    "response = open(\"./lorem-ipsum.txt\")  # TODO remove in favor of download\n",
    "long_text = response.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we tokenize the long text, exclude the special tokens, create chunks of size 510 tokens and map the tokens back to text. Notice that on the first run the BERT tokenizer itself is warning us about the 512 tokens limitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. est pellentesque elit ullamcorper dignissim. sit amet cursus sit amet dictum sit amet. enim neque volutpat ac tincidunt vitae semper quis lectus. nulla facilisi etiam dignissim diam quis enim lobortis. id velit ut tortor pretium. ut tortor pretium viverra suspendisse potenti nullam ac tortor. senectus et netus et malesuada fames ac. sed faucibus turpis in eu. maecenas ultricies mi eget mauris pharetra. in iaculis nunc sed augue. sit amet cursus sit amet dictum. sit amet luctus venenatis lectus magna. adipiscing tristique risus nec feugiat. nisi quis eleifend quam adipiscing vitae proin sagittis nisl rhoncus. scelerisque varius morbi enim nunc faucibus a. purus semper eget duis at tellus at. cursus metus aliquam eleifend mi. tristique senectus et netus et malesuada fames. netus et malesuada fames ac. viverra aliquet eget sit amet tellus cras. hac habitasse platea dictumst vestibulum rhoncus est pellentesque elit. molestie ac feugiat sed lectus vestibulum mattis. etiam erat velit scelerisque in dictum non. dolor sit amet consectetur adipiscing elit duis tristique sollicitudin nibh. diam vulputate ut pharetra sit amet aliquam id. arcu non sodales neque sodales ut etiam sit. neque vitae tempus quam pellentesque nec nam. amet porttitor eget dolor morbi non arcu risus quis. vitae semper quis',\n",
       " 'lectus nulla at volutpat diam ut. blandit volutpat maecenas volutpat blandit aliquam. lobortis elementum nibh tellus molestie nunc. lectus arcu bibendum at varius vel pharetra vel turpis nunc. in hac habitasse platea dictumst. vitae suscipit tellus mauris a diam maecenas. mi eget mauris pharetra et. habitant morbi tristique senectus et netus. eu lobortis elementum nibh tellus molestie nunc non. scelerisque varius morbi enim nunc faucibus a. tincidunt arcu non sodales neque sodales ut etiam sit amet. tellus integer feugiat scelerisque varius. magna fermentum iaculis eu non diam phasellus vestibulum lorem. eget nunc lobortis mattis aliquam faucibus. dignissim sodales ut eu sem integer vitae justo eget. urna id volutpat lacus laoreet. mauris nunc congue nisi vitae suscipit tellus mauris a diam. scelerisque in dictum non consectetur a erat nam at lectus. neque sodales ut etiam sit amet nisl. blandit cursus risus at ultrices. scelerisque mauris pellentesque pulvinar pellentesque habitant morbi tristique senectus et. cursus vitae congue mauris rhoncus aenean vel elit scelerisque. lobortis feugiat vivamus at augue eget arcu dictum. sagittis orci a scelerisque purus semper eget duis at. ornare suspendisse sed nisi lacus sed viverra tellus in hac. massa sapien faucibus et molestie. vulputate odio ut enim blandit volutpat maecenas volutpat. mauris rhoncus aenean vel elit scelerisque mauris pellentesque pu',\n",
       " '##lvinar pellentesque. massa sapien faucibus et molestie ac. orci porta non pulvinar neque laoreet suspendisse interdum consectetur. mauris commodo quis imperdiet massa. volutpat consequat mauris nunc congue nisi vitae suscipit. malesuada fames ac turpis egestas maecenas pharetra convallis. cursus risus at ultrices mi tempus imperdiet. non enim praesent elementum facilisis leo vel fringilla est. felis bibendum ut tristique et. felis donec et odio pellentesque diam volutpat commodo sed egestas. ut porttitor leo a diam sollicitudin tempor id eu. dolor purus non enim praesent. tortor aliquam nulla facilisi cras. rhoncus dolor purus non enim. sed vulputate odio ut enim blandit volutpat maecenas. consequat semper viverra nam libero justo laoreet. eget nunc scelerisque viverra mauris. id cursus metus aliquam eleifend mi in nulla. mattis molestie a iaculis at erat pellentesque adipiscing. enim nec dui nunc mattis. hendrerit gravida rutrum quisque non tellus orci ac. fermentum iaculis eu non diam phasellus vestibulum lorem sed. adipiscing diam donec adipiscing tristique risus. sit amet commodo nulla facilisi nullam vehicula ipsum. amet consectetur adipiscing elit ut aliquam purus sit. id diam vel quam elementum pulvinar etiam non quam. nulla pharetra diam sit amet nisl suscipit adipiscing bibendum. massa tempor nec feugiat nisl pretium fusce id.']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = bert_tokenizer.encode(long_text)[1:-1]  # exclude special tokens at beginning and end\n",
    "chunked = [\n",
    "    bert_tokenizer.decode(tokens_chunk)\n",
    "    for tokens_chunk in chunk(tokens)\n",
    "]\n",
    "chunked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now these chunks can be indexed and we can be sure the semantic search model consideres our whole text."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
