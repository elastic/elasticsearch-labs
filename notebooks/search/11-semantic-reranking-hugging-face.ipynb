{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tn-NTlKI2EYR"
   },
   "source": [
    "# Semantic reranking with a Hugging Face model\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/elastic/elasticsearch-labs/blob/main/notebooks/search/11-semantic-reranking-hugging-face.ipynb)\n",
    "\n",
    "In this notebook you'll learn how to implement semantic reranking in Elasticsearch by uploading a model from Hugging Face into your cluster. You'll also learn about the `retriever` abstraction, a simpler syntax for crafting queries and combining different search operations.\n",
    "\n",
    "You will:\n",
    "\n",
    "- Choose a cross-encoder model from Hugging Face to perform semantic reranking\n",
    "- Upload the model to your Elasticsearch deployment using [Eland](https://www.elastic.co/guide/en/elasticsearch/client/eland/current/machine-learning.html)\n",
    "- Create an inference endpoint to manage your `rerank` task\n",
    "- Query your data using the `text_similarity_rerank` retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tir9w4Sz80v"
   },
   "source": [
    "## 🧰 Requirements\n",
    "\n",
    "For this example, you will need:\n",
    "\n",
    "- An Elastic deployment:\n",
    "    \n",
    "    - We'll be using [Elastic Cloud](https://www.elastic.co/guide/en/cloud/current/ec-getting-started.html) for this example (available with a [free trial](https://cloud.elastic.co/registration?onboarding_token=vectorsearch&utm_source=github&utm_content=elasticsearch-labs-notebook))\n",
    "      - You can also use Elastic Cloud [Serverless](https://cloud.elastic.co/serverless-registration)\n",
    "- Elasticsearch 8.15.0 or above (for non-serverless deployments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ut-7TfSB2EYS"
   },
   "source": [
    "## Install packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AzuZInsj2EYS",
    "outputId": "8a699f03-0133-43db-ed4a-caae5941ee68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/480.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m471.0/480.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.2/480.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting eland[pytorch]\n",
      "  Downloading eland-8.14.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: elasticsearch<9,>=8.3 in /usr/local/lib/python3.10/dist-packages (from eland[pytorch]) (8.14.0)\n",
      "Collecting pandas<2,>=1.5 (from eland[pytorch])\n",
      "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.10/dist-packages (from eland[pytorch]) (3.7.1)\n",
      "Requirement already satisfied: numpy<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from eland[pytorch]) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from eland[pytorch]) (24.1)\n",
      "Requirement already satisfied: requests<3 in /usr/local/lib/python3.10/dist-packages (from eland[pytorch]) (2.32.3)\n",
      "Collecting torch==2.1.2 (from eland[pytorch])\n",
      "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from eland[pytorch]) (4.66.5)\n",
      "Collecting sentence-transformers<=2.3.1,>=2.1.0 (from eland[pytorch])\n",
      "  Downloading sentence_transformers-2.3.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting transformers<4.36.0,>=4.31.0 (from transformers[torch]<4.36.0,>=4.31.0; extra == \"pytorch\"->eland[pytorch])\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->eland[pytorch]) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->eland[pytorch]) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->eland[pytorch]) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->eland[pytorch]) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->eland[pytorch]) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->eland[pytorch]) (2024.6.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.2->eland[pytorch])\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.2->eland[pytorch])\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.2->eland[pytorch])\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.2->eland[pytorch])\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.2->eland[pytorch])\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.2->eland[pytorch])\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.2->eland[pytorch])\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.2->eland[pytorch])\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.2->eland[pytorch])\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.2->eland[pytorch])\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.2->eland[pytorch])\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.1.0 (from torch==2.1.2->eland[pytorch])\n",
      "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->eland[pytorch])\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.13 in /usr/local/lib/python3.10/dist-packages (from elasticsearch<9,>=8.3->eland[pytorch]) (8.15.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->eland[pytorch]) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->eland[pytorch]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->eland[pytorch]) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->eland[pytorch]) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->eland[pytorch]) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->eland[pytorch]) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->eland[pytorch]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2,>=1.5->eland[pytorch]) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3->eland[pytorch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3->eland[pytorch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3->eland[pytorch]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3->eland[pytorch]) (2024.7.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<=2.3.1,>=2.1.0->eland[pytorch]) (1.3.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<=2.3.1,>=2.1.0->eland[pytorch]) (1.13.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<=2.3.1,>=2.1.0->eland[pytorch]) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<=2.3.1,>=2.1.0->eland[pytorch]) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<=2.3.1,>=2.1.0->eland[pytorch]) (0.23.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.36.0,>=4.31.0->transformers[torch]<4.36.0,>=4.31.0; extra == \"pytorch\"->eland[pytorch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.36.0,>=4.31.0->transformers[torch]<4.36.0,>=4.31.0; extra == \"pytorch\"->eland[pytorch]) (2024.5.15)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers<4.36.0,>=4.31.0->transformers[torch]<4.36.0,>=4.31.0; extra == \"pytorch\"->eland[pytorch])\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.36.0,>=4.31.0->transformers[torch]<4.36.0,>=4.31.0; extra == \"pytorch\"->eland[pytorch]) (0.4.4)\n",
      "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<4.36.0,>=4.31.0; extra == \"pytorch\"->eland[pytorch]) (0.32.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]<4.36.0,>=4.31.0; extra == \"pytorch\"->eland[pytorch]) (5.9.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->eland[pytorch]) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.2->eland[pytorch]) (2.1.5)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers<=2.3.1,>=2.1.0->eland[pytorch]) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers<=2.3.1,>=2.1.0->eland[pytorch]) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers<=2.3.1,>=2.1.0->eland[pytorch]) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.2->eland[pytorch]) (1.3.0)\n",
      "Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m865.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-2.3.1-py3-none-any.whl (132 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading eland-8.14.0-py3-none-any.whl (165 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.6/165.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, eland, transformers, torch, sentence-transformers\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.3.1\n",
      "    Uninstalling triton-2.3.1:\n",
      "      Successfully uninstalled triton-2.3.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.1.4\n",
      "    Uninstalling pandas-2.1.4:\n",
      "      Successfully uninstalled pandas-2.1.4\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.42.4\n",
      "    Uninstalling transformers-4.42.4:\n",
      "      Successfully uninstalled transformers-4.42.4\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.3.1+cu121\n",
      "    Uninstalling torch-2.3.1+cu121:\n",
      "      Successfully uninstalled torch-2.3.1+cu121\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.1.4, but you have pandas 1.5.3 which is incompatible.\n",
      "torchaudio 2.3.1+cu121 requires torch==2.3.1, but you have torch 2.1.2 which is incompatible.\n",
      "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.1.2 which is incompatible.\n",
      "torchvision 0.18.1+cu121 requires torch==2.3.1, but you have torch 2.1.2 which is incompatible.\n",
      "xarray 2024.6.0 requires pandas>=2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed eland-8.14.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 pandas-1.5.3 sentence-transformers-2.3.1 tokenizers-0.15.2 torch-2.1.2 transformers-4.35.2 triton-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU elasticsearch\n",
    "!pip install eland[pytorch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnEQCDrd2EYT"
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bj8HebgN2EYT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from elasticsearch import Elasticsearch, helpers, exceptions\n",
    "from urllib.request import urlopen\n",
    "from getpass import getpass\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlJXqZkJ2EYT"
   },
   "source": [
    "## Initialize Elasticsearch Python client\n",
    "\n",
    "You need to connect to a running Elasticsearch instance. In this example we're using an Elastic Cloud deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "frMDUtzt2EYT",
    "outputId": "3c8804bc-f76d-4d3f-dfad-d2b7a8fc9fb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Cloud ID: ··········\n",
      "Elastic Api Key: ··········\n"
     ]
    }
   ],
   "source": [
    "# https://www.elastic.co/search-labs/tutorials/install-elasticsearch/elastic-cloud#finding-your-cloud-id\n",
    "ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID: \")\n",
    "\n",
    "# https://www.elastic.co/search-labs/tutorials/install-elasticsearch/elastic-cloud#creating-an-api-key\n",
    "ELASTIC_API_KEY = getpass(\"Elastic Api Key: \")\n",
    "\n",
    "# Create the client instance\n",
    "client = Elasticsearch(\n",
    "    # For local development\n",
    "    # hosts=[\"http://localhost:9200\"]\n",
    "    cloud_id=ELASTIC_CLOUD_ID,\n",
    "    api_key=ELASTIC_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rL5AEaDrzj9t"
   },
   "source": [
    "## Test connection\n",
    "\n",
    "Confirm that the Python client has connected to your Elasticsearch instance with this test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t_lhez7Tznhp",
    "outputId": "6ab9d02e-5cd1-4d6c-8bb2-958f78ab43e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'serverless', 'cluster_name': 'dbed854f18874014ad9ca1b1169e9e3d', 'cluster_uuid': 't3JzgD0aSeesEYPWch7KUg', 'version': {'number': '8.11.0', 'build_flavor': 'serverless', 'build_type': 'docker', 'build_hash': '00000000', 'build_date': '2023-10-31', 'build_snapshot': False, 'lucene_version': '9.7.0', 'minimum_wire_compatibility_version': '8.11.0', 'minimum_index_compatibility_version': '8.11.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "print(client.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Lt2KpOizHLe"
   },
   "source": [
    "## Enable Telemetry\n",
    "\n",
    "Knowing that you are using this notebook helps us decide where to invest our efforts to improve our products. We would like to ask you that you run the following code to let us gather anonymous usage statistics. See telemetry.py for details. Thank you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08qJdCH3zNSC"
   },
   "outputs": [],
   "source": [
    "!curl -O -s https://raw.githubusercontent.com/elastic/elasticsearch-labs/main/telemetry/telemetry.py\n",
    "from telemetry import enable_telemetry\n",
    "\n",
    "es_client = enable_telemetry(es_client, \"11-semantic-reranking-hugging-face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yR-zE2ii2EYU"
   },
   "source": [
    "## Upload sample data\n",
    "\n",
    "This examples uses a small dataset of movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bWEpw_X52EYU",
    "outputId": "f52c493e-0afa-417d-8ffb-3aae39e20cd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done indexing documents into `movies` index!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/elastic/elasticsearch-labs/main/notebooks/search/movies.json\"\n",
    "response = urlopen(url)\n",
    "\n",
    "# Load the response data into a JSON object\n",
    "data_json = json.loads(response.read())\n",
    "\n",
    "# Prepare the documents to be indexed\n",
    "documents = []\n",
    "for doc in data_json:\n",
    "    documents.append(\n",
    "        {\n",
    "            \"_index\": \"movies\",\n",
    "            \"_source\": doc,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Use helpers.bulk to index\n",
    "helpers.bulk(client, documents)\n",
    "\n",
    "print(\"Done indexing documents into `movies` index!\")\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWd_YOZh2EYU"
   },
   "source": [
    "## Upload Hugging Face model using Eland\n",
    "\n",
    "Now we'll use Eland's `eland_import_hub_model` command to upload the model to Elasticsearch. For this example we've chosen the [`cross-encoder/ms-marco-MiniLM-L-6-v2`](https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2) text similarity model.\n",
    "\n",
    "**Refer to [the Elastic NLP model reference](https://www.elastic.co/guide/en/machine-learning/8.15/ml-nlp-model-ref.html#ml-nlp-model-ref-text-similarity) for a list of third-party text similarity models supported by Elasticsearch.**\n",
    "\n",
    "ℹ️\n",
    "This example uses an Elastic Cloud deployment with an API key, but there are more deployment and [authentication options](https://www.elastic.co/guide/en/elasticsearch/client/eland/current/machine-learning.html#ml-nlp-pytorch-auth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sra93pWm2EYV",
    "outputId": "a025bc58-ba43-499a-8548-aeef7ade677b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-13 06:47:43,002 INFO : Establishing connection to Elasticsearch\n",
      "2024-08-13 06:47:43,550 INFO : Connected to serverless cluster 'dbed854f18874014ad9ca1b1169e9e3d'\n",
      "2024-08-13 06:47:43,551 INFO : Loading HuggingFace transformer tokenizer and model 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "tokenizer_config.json: 100% 316/316 [00:00<00:00, 1.76MB/s]\n",
      "config.json: 100% 794/794 [00:00<00:00, 4.32MB/s]\n",
      "vocab.txt: 100% 232k/232k [00:00<00:00, 15.2MB/s]\n",
      "special_tokens_map.json: 100% 112/112 [00:00<00:00, 610kB/s]\n",
      "pytorch_model.bin: 100% 90.9M/90.9M [00:05<00:00, 16.2MB/s]\n",
      "STAGE:2024-08-13 06:47:54 1405:1405 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2024-08-13 06:47:54 1405:1405 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2024-08-13 06:47:54 1405:1405 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
      "2024-08-13 06:47:57,749 INFO : Stopping deployment for model with id 'cross-encoder__ms-marco-minilm-l-6-v2'\n",
      "2024-08-13 06:47:57,884 INFO : Deleting model with id 'cross-encoder__ms-marco-minilm-l-6-v2'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/eland_import_hub_model\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/eland/cli/eland_import_hub_model.py\", line 321, in main\n",
      "    ptm.delete()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/eland/ml/pytorch/_pytorch_model.py\", line 155, in delete\n",
      "    self._client.options(ignore_status=404).ml.delete_trained_model(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/elasticsearch/_sync/client/utils.py\", line 446, in wrapped\n",
      "    return api(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/elasticsearch/_sync/client/ml.py\", line 682, in delete_trained_model\n",
      "    return self.perform_request(  # type: ignore[return-value]\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/elasticsearch/_sync/client/_base.py\", line 423, in perform_request\n",
      "    return self._client.perform_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/elasticsearch/_sync/client/_base.py\", line 271, in perform_request\n",
      "    response = self._perform_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/elasticsearch/_sync/client/_base.py\", line 352, in _perform_request\n",
      "    raise HTTP_EXCEPTIONS.get(meta.status, ApiError)(\n",
      "elasticsearch.ConflictError: ConflictError(409, 'status_exception', 'Cannot delete model [cross-encoder__ms-marco-minilm-l-6-v2] as it is currently deployed; use force to delete the model')\n"
     ]
    }
   ],
   "source": [
    "!eland_import_hub_model \\\n",
    "  --cloud-id $CLOUD_ID \\\n",
    "  --es-api-key $ES_API_KEY \\\n",
    "  --hub-model-id cross-encoder/ms-marco-MiniLM-L-6-v2 \\\n",
    "  --task-type text_similarity \\\n",
    "  --clear-previous \\\n",
    "  --start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDAB9pX3VGKE"
   },
   "source": [
    "## Lexical queries\n",
    "\n",
    "First let's use a `standard` retriever to test out some lexical (or full-text) searchs and then we'll compare the improvements when we layer in semantic reranking.\n",
    "\n",
    "### Lexical match with `query_string` query\n",
    "\n",
    "Let's say we vaguely remember that there is a famous movie about a killer who eats his victims. For the sake of argument, pretend we've momentarily forgotten the word \"cannibal\".\n",
    "\n",
    "Let's perform a [`query_string` query](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html) to find the phrase \"flesh-eating bad guy\" in the `plot` fields of our Elasticsearch documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EAG0yZ_bVX-j",
    "outputId": "61c61ecf-c731-4ace-b52d-5fbdc357a63b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No search results found\n"
     ]
    }
   ],
   "source": [
    "resp = client.search(\n",
    "    index=\"movies\",\n",
    "    retriever={\n",
    "        \"standard\": {\n",
    "            \"query\": {\n",
    "                \"query_string\": {\n",
    "                    \"query\": \"flesh-eating bad guy\",\n",
    "                    \"default_field\": \"plot\",\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "if resp[\"hits\"][\"hits\"]:\n",
    "    for hit in resp[\"hits\"][\"hits\"]:\n",
    "        title = hit[\"_source\"][\"title\"]\n",
    "        plot = hit[\"_source\"][\"plot\"]\n",
    "        print(f\"Title: {title}\\nPlot: {plot}\\n\")\n",
    "else:\n",
    "    print(\"No search results found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48_nf56d2EYV"
   },
   "source": [
    "No results! Unfortunately we don't have any near exact matches for \"flesh-eating bad guy\". Because we don't have any more specific information about the exact phrasing in the Elasticsearch data, we'll need to cast our search net wider.\n",
    "\n",
    "### Simple `match` query\n",
    "\n",
    "This lexical query performs a standard keyword search for the term \"crime\" within the \"plot\" and \"genre\" fields of our Elasticsearch documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XkAlzHW82EYV",
    "outputId": "b47b0cee-9f56-4efa-bf63-2d7da0a21287"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Godfather\n",
      "Plot: An organized crime dynasty's aging patriarch transfers control of his clandestine empire to his reluctant son.\n",
      "\n",
      "Title: Goodfellas\n",
      "Plot: The story of Henry Hill and his life in the mob, covering his relationship with his wife Karen Hill and his mob partners Jimmy Conway and Tommy DeVito in the Italian-American crime syndicate.\n",
      "\n",
      "Title: The Dark Knight\n",
      "Plot: When the menace known as the Joker wreaks havoc and chaos on the people of Gotham, Batman must accept one of the greatest psychological and physical tests of his ability to fight injustice.\n",
      "\n",
      "Title: Pulp Fiction\n",
      "Plot: The lives of two mob hitmen, a boxer, a gangster and his wife, and a pair of diner bandits intertwine in four tales of violence and redemption.\n",
      "\n",
      "Title: The Silence of the Lambs\n",
      "Plot: A young F.B.I. cadet must receive the help of an incarcerated and manipulative cannibal killer to help catch another serial killer, a madman who skins his victims.\n",
      "\n",
      "Title: The Usual Suspects\n",
      "Plot: A sole survivor tells of the twisty events leading up to a horrific gun battle on a boat, which began when five criminals met at a seemingly random police lineup.\n",
      "\n",
      "Title: Se7en\n",
      "Plot: Two detectives, a rookie and a veteran, hunt a serial killer who uses the seven deadly sins as his motives.\n",
      "\n",
      "Title: The Departed\n",
      "Plot: An undercover cop and a mole in the police attempt to identify each other while infiltrating an Irish gang in South Boston.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resp = client.search(\n",
    "    index=\"movies\",\n",
    "    retriever={\n",
    "        \"standard\": {\n",
    "            \"query\": {\"multi_match\": {\"query\": \"crime\", \"fields\": [\"plot\", \"genre\"]}}\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "for hit in resp[\"hits\"][\"hits\"]:\n",
    "    title = hit[\"_source\"][\"title\"]\n",
    "    plot = hit[\"_source\"][\"plot\"]\n",
    "    print(f\"Title: {title}\\nPlot: {plot}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2K1C_Pmzmrxw"
   },
   "source": [
    "That's better! At least we've got some results now. We broadened our search criteria to increase the chances of finding relevant results.\n",
    "\n",
    "But these results aren't very precise in the context of our original query \"flesh-eating bad guy\". We can see that \"The Silence of the Lambs\" is returned in the middle of the results set with this generic `match` query. Let's see if we can use our semantic reranking model to get closer to the searcher's original intent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nd8gAE6H2EYV"
   },
   "source": [
    "## Semantic reranker\n",
    "\n",
    "In the following `retriever` syntax, we wrap our standard `match` query retriever in a `text_similarity_reranker`. This allows us to leverage the NLP model we deployed to Elasticsearch to rerank the results based on the phrase \"flesh-eating bad guy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z9DegKqb2EYV",
    "outputId": "cd84f1a5-6e07-4f61-eab2-aa4383d00b8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Silence of the Lambs\n",
      "Plot: A young F.B.I. cadet must receive the help of an incarcerated and manipulative cannibal killer to help catch another serial killer, a madman who skins his victims.\n",
      "\n",
      "Title: Pulp Fiction\n",
      "Plot: The lives of two mob hitmen, a boxer, a gangster and his wife, and a pair of diner bandits intertwine in four tales of violence and redemption.\n",
      "\n",
      "Title: Se7en\n",
      "Plot: Two detectives, a rookie and a veteran, hunt a serial killer who uses the seven deadly sins as his motives.\n",
      "\n",
      "Title: Goodfellas\n",
      "Plot: The story of Henry Hill and his life in the mob, covering his relationship with his wife Karen Hill and his mob partners Jimmy Conway and Tommy DeVito in the Italian-American crime syndicate.\n",
      "\n",
      "Title: The Dark Knight\n",
      "Plot: When the menace known as the Joker wreaks havoc and chaos on the people of Gotham, Batman must accept one of the greatest psychological and physical tests of his ability to fight injustice.\n",
      "\n",
      "Title: The Godfather\n",
      "Plot: An organized crime dynasty's aging patriarch transfers control of his clandestine empire to his reluctant son.\n",
      "\n",
      "Title: The Departed\n",
      "Plot: An undercover cop and a mole in the police attempt to identify each other while infiltrating an Irish gang in South Boston.\n",
      "\n",
      "Title: The Usual Suspects\n",
      "Plot: A sole survivor tells of the twisty events leading up to a horrific gun battle on a boat, which began when five criminals met at a seemingly random police lineup.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resp = client.search(\n",
    "    index=\"movies\",\n",
    "    retriever={\n",
    "        \"text_similarity_reranker\": {\n",
    "            \"retriever\": {\n",
    "                \"standard\": {\n",
    "                    \"query\": {\n",
    "                        \"multi_match\": {\"query\": \"crime\", \"fields\": [\"plot\", \"genre\"]}\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"field\": \"plot\",\n",
    "            \"inference_id\": \"my-msmarco-minilm-model\",\n",
    "            \"inference_text\": \"flesh-eating bad guy\",\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "for hit in resp[\"hits\"][\"hits\"]:\n",
    "    title = hit[\"_source\"][\"title\"]\n",
    "    plot = hit[\"_source\"][\"plot\"]\n",
    "    print(f\"Title: {title}\\nPlot: {plot}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqVh3qt82EYW"
   },
   "source": [
    "Success! \"The Silence of the Lambs\" is our top result. Semantic reranking helped us find the most relevant result by parsing a natural language query, overcoming the limitations of lexical search that relies on keyword matching.\n",
    "\n",
    "Semantic reranking enables semantic search in a few steps, without the need for generating and storing embeddings. This a great tool for testing and building hybrid search systems in Elasticsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yYfJjjstxwK"
   },
   "source": [
    "## Learn more\n",
    "\n",
    "- [Semantic Reranking overview](https://www.elastic.co/guide/en/elasticsearch/reference/current/semantic-reranking.html). A high level overview of semantic reranking in Elasticsearch.\n",
    "- [`text_similarity_reranker` retriever ](https://www.elastic.co/guide/en/elasticsearch/reference/current/retriever.html#text-similarity-reranker-retriever). Detailed API syntax reference with examples.\n",
    "- [Semantic reranking with Cohere notebook](https://github.com/elastic/elasticsearch-labs/blob/main/notebooks/search/10-semantic-reranking-retriever-cohere.ipynb). You can also use the Cohere Rerank API instead of loading your own model into Elasticsearch.\n",
    "- [`elasticsearch-labs` notebooks](https://github.com/elastic/elasticsearch-labs/tree/main/notebooks). Check our full catalogue of Python notebooks."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
